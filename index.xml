<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jonathan K. Kummerfeld</title>
    <link>http://www.jkk.name/</link>
    <description>Jonathan K. Kummerfeld</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jonathan K. Kummerfeld</copyright>
    <lastBuildDate>Wed, 27 Sep 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://www.jkk.name/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      
        <title>Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog (Kottur et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-13_languagegame/</link>
      <pubDate>Mon, 13 Nov 2017 09:47:08 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-13_languagegame/</guid>
      
        <description>

&lt;p&gt;In reference games, two players communicate in a shared world with the goal of one learning what the other is referring to.
Their small scale and clear success criteria make them a convenient testbed for dialogue agents, going back decades, with recent work focusing on neural approaches.
This paper considers a simple game and constrains models in various ways to improve performance and see how their communication varies, a line of work also appearing in recent papers by Jacob Andreas (&lt;a href=&#34;http://aclweb.org/anthology/P/P17/P17-1022.pdf&#34; target=&#34;_blank&#34;&gt;ACL 2017&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.info/pdf/D/D17/D17-1310.pdf&#34; target=&#34;_blank&#34;&gt;EMNLP 2017&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The game in this case is to find out two properties of an object, where there are three possible properties, each with four possible values.
Given enough flexibility, models will explicitly encode every possible structure of the world as a separate symbol, which does not generalise well.
Limiting the vocabulary to one symbol per property and one per value helps, but in this particular game there are only 3 possible questions, and over two turns of dialogue the 12 value words are sufficient to encode the space.
Limiting even further, to 4 words for values and providing each turn in isolation to the answerer does lead to some compositionality, but clearly not full compositionality as they still make errors on unseen combinations of the inputs.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a short paper, so they can only do so much, but some experiments I am curious about are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Decrease the questioner vocabulary to 2. This avoids the problem that the questioner can express the task in one step by saying what is not needed. It&amp;rsquo;s still doable, by defining an order for questions, e.g. ask about attribute A vs. B first, then in the second step ask about either C or the other option from the first step. This is a little weird as symbols need to mean different things at different time steps, but would be interesting.&lt;/li&gt;
&lt;li&gt;Increase the number of attributes to 4. This also avoids the task expression problem, by forcing there to be compositionality on the questioner side (watching the video of the talk, someone asked this in the question time, and they didn&amp;rsquo;t know).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1320&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kottur-EtAl:2017:EMNLP2017,
  author    = {Kottur, Satwik  and  Moura, Jos\&#39;{e}  and  Lee, Stefan  and  Batra, Dhruv},
  title     = {Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2952--2957},
  url       = {https://www.aclweb.org/anthology/D17-1320}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-10_kginlstm/</link>
      <pubDate>Fri, 10 Nov 2017 15:37:15 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-10_kginlstm/</guid>
      
        <description>

&lt;p&gt;Structured information sources have been effectively used for entity detection and typing in linear models with the information encoded as binary features.
This paper looks at how to integrate vector representations of structured information into an LSTM.
The solution is an additional processing step during output generation, in which the vectors for relevant entities in the structured data are combined with the standard LSTM output (note, they do not affect the cell itself, so the information is not passed on through the recurrence).&lt;/p&gt;

&lt;p&gt;In this case the structured information is a set of tuples forming a graph of relations between entities, from either NELL or WordNet.
The actual encoding of entities is an application of prior work; vectors representing tuples are trained with the objective that the score for any tuple is higher than made-up tuples (where the score is $v_a M_r v_b$ for entities $a$ and $b$ in relation $r$).
The set of relevant entities for a particular word in the sentence is obtained by string matching, and then attention is used to combine them.
There is also a kind of gating mechanism to choose how big a role the entities play in the prediction, using a combination of the input, hidden state, and cell state.&lt;/p&gt;

&lt;p&gt;The results are interesting not only because this method helps, but because of how well the standard LSTM does on this task, matching or exceeding prior results.
This is even more impressive given how small ACE is (if I remember correctly).
The other key observations are that having a sequence level loss (using a CRF) helps, and NELL and WordNet seem to be providing different types of information (as using both leads to further improvements).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1132&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{yang-mitchell:2017:Long,
  author    = {Yang, Bishan  and  Mitchell, Tom},
  title     = {Leveraging Knowledge Bases in LSTMs for Improving Machine Reading},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1436--1446},
  url       = {http://aclweb.org/anthology/P17-1132}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-9_framesdataset/</link>
      <pubDate>Thu, 09 Nov 2017 19:47:08 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-9_framesdataset/</guid>
      
        <description>

&lt;p&gt;Another paper about a dataset of dialogues, but this time with structure.
Like the paper from yesterday, the aim is a dataset of task-oriented conversations, but with more complexity than prior work.
The difference is that this work includes a structured representation of the state of the conversation: frames.&lt;/p&gt;

&lt;p&gt;A frame is essentially a tuple describing a query, e.g. (Destination: Sydney, Origin: Ann Arbor, price: 1500 USD).
There are multiple frames in a dialogue (a departure from DSTC tasks), and utterances are labeled with dialogue acts that capture modifications to the frames as well as references to them.
This structure sounds fairly general, though the focus here was on vacation planning, where the user is buying a package.
The setup doesn&amp;rsquo;t maximise the potential complexity though, as there are a small number of set packages available, rather than the complex tradeoffs of flight+hotel combinations that exist in practise.
Looking at the example dialogues in the paper, it has complete sentences of some complexity.
One thing I&amp;rsquo;m still curious about is disagreements between annotators, as for the complete task the score was 0.62 +/- 5 (with dialogue acts being trickier than slot values, and no scores for frame references on their own).&lt;/p&gt;

&lt;p&gt;Comparing to the Stanford dataset this is smaller (11k vs. 1.4k), but has more turns per dialogue (11 vs. 15) and probably longer turns too, judging by the examples.
The tasks are completely different, but both come with small tables of information that are private to the two participants and required for almost every turn in the conversation.
Evaluating on both could be a great way to show the flexibility of a dialogue system, but the lack of frames for the Stanford data and the difficulty of running a human evaluation for this data limits the feasible types of multi-domain experiments.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/W17-5526&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{elasri-EtAl:2017:W17-55,
  author    = {El Asri, Layla  and  Schulz, Hannes  and  Sharma, Shikhar  and  Zumer, Jeremie  and  Harris, Justin  and  Fine, Emery  and  Mehrotra, Rahul  and  Suleman, Kaheer},
  title     = {Frames: a corpus for adding memory to goal-oriented dialogue systems},
  booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  month     = {August},
  year      = {2017},
  address   = {Saarbrucken, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {207--219},
  url       = {http://aclweb.org/anthology/W17-5526}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-8_graphdialogue/</link>
      <pubDate>Wed, 08 Nov 2017 18:46:04 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-8_graphdialogue/</guid>
      
        <description>

&lt;p&gt;Task-oriented dialogue systems are often focused on a very narrow task, to the point where the state can be described completely with a tuple (e.g. preferences for a restaurant).
This paper sets up a more challenging task with more complex language use, while still having a specific goal and directly relevant structured information.
They collected 11,000 dialogues, where two people have private lists of friends and are trying to identify which friend they have in common.
While this is a lot of data, the mechanical turk workers are clearly moving fast, with dialogues taking 1.5 minutes on average, and in 18% of cases they get the friend wrong.&lt;/p&gt;

&lt;p&gt;The algorithmic contribution is that the lists of people are represented as a graph, where nodes are properties like company and hobby.
The graph is used to generate vectors for each person by running a form of message passing over its structure.
During generation, the LSTM uses attention over these vectors to inform the output choice.&lt;/p&gt;

&lt;p&gt;A few interesting things in the output:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are cases where the output is incorrect, as in, says a fact about the structured information / knowledge base that is false.&lt;/li&gt;
&lt;li&gt;Evaluation is tricky, and over the metrics they consider sometimes this wins, but sometimes the baseline system (rules) does better. In particular, success on bot-bot evaluation doesn&amp;rsquo;t seem to clearly transfer to bot-human experiments.&lt;/li&gt;
&lt;li&gt;The utterances are very fluent, but that may be because it&amp;rsquo;s essentially copying from the training data. It looks like there is diversity in the dataset, but a lot of utterances do fit a template of &amp;ldquo;I have X who Y&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1162&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{he-EtAl:2017:Long4,
  author    = {He, He  and  Balakrishnan, Anusha  and  Eric, Mihail  and  Liang, Percy},
  title     = {Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1766--1776},
  abstract  = {We study a \emph{symmetric collaborative dialogue} setting
	in which two agents, each with private knowledge,
	must strategically communicate to achieve a common goal.
	The open-ended dialogue state in this setting poses new challenges for existing
	dialogue systems.
	We collected a dataset of 11K human-human dialogues,
	which exhibits interesting lexical, semantic, and strategic elements.
	To model
	both structured knowledge and unstructured language,
	we propose a neural model with dynamic knowledge graph embeddings
	that evolve as the dialogue progresses.
	Automatic and human evaluations show that our model is both more effective
	at achieving the goal and more human-like than baseline neural and rule-based
	models.},
  url       = {http://aclweb.org/anthology/P17-1162}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-7_spineparsinglstm/</link>
      <pubDate>Tue, 07 Nov 2017 20:42:45 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-7_spineparsinglstm/</guid>
      
        <description>

&lt;p&gt;This paper brings together work on neural dependency parsing with the idea of non-terminal spines as a way to represent constituency structure.
Within the transition parsing inference process they can naturally fit the generation of a new spines by gradually building up the spine, which makes for a very elegant inference process.&lt;/p&gt;

&lt;p&gt;Surprisingly, it doesn&amp;rsquo;t seem to matter what head choices are used to generate the spines (they tried leftmost word, rightmost word, and two standard schemes).
This contrasts with my own observations that the choice of head had a big impact (0.5 F) on accuracy.
I think the incrementally-built spines are the key difference.
Decisions about higher up in the spine are difficult to make when looking at a single word, but with the incremental construction there is information about a larger context.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/W17-6316&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{ballesteros-carreras:2017:IWPT,
  author    = {Ballesteros, Miguel  and  Carreras, Xavier},
  title     = {Arc-Standard Spinal Parsing with Stack-LSTMs},
  booktitle = {Proceedings of the 15th International Conference on Parsing Technologies},
  month     = {September},
  year      = {2017},
  address   = {Pisa, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {115--121},
  url       = {http://www.aclweb.org/anthology/W17-6316}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)</title>
      
      <link>http://www.jkk.name/post/2017-11-6_literarycharacters/</link>
      <pubDate>Mon, 06 Nov 2017 20:16:28 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-6_literarycharacters/</guid>
      
        <description>

&lt;p&gt;NLP tools seem like a natural fit for literary analysis, but the domain shift from news text is large enough to degrade performance to the point where tools are not useful.
Here the specific question is how many characters are there in novels?
NER + coreference would seem to be enough, but an off-the-shelf system fares poorly (and I doubt improvements in the last few years would change that story).&lt;/p&gt;

&lt;p&gt;The solution is to craft a kind of coreference system focused on getting all of the characters, but not necessarily every mention.
The most interesting new piece is how they identify rare characters: identify arguments of verbs that usually take people.
With this tool in hand they analyse patterns of character use over time to test hypotheses from literary analysis.&lt;/p&gt;

&lt;p&gt;Another key piece of this work was a tool to annotate a collection of books with character occurrences.
CHARLES, their tool, is built on top of &lt;a href=&#34;http://brat.nlplab.org/&#34; target=&#34;_blank&#34;&gt;brat&lt;/a&gt;, adding features to help multiple annotators coordinate labels (specifically handling the case of new character identification, which modifies the set of linkable entities).&lt;/p&gt;

&lt;p&gt;Finally, they released the character lists identified for the novels considered (&lt;a href=&#34;http://aclweb.org/anthology/attachments/D/D15/D15-1088.Attachment.zip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).
It would be interesting to modify a coreference resolution system to process these books, taking advantage of that information!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/D15-1088&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2016/pdf/1130_Paper.pdf&#34; target=&#34;_blank&#34;&gt;Annotation Tool Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{vala-EtAl:2015:EMNLP,
  author    = {Vala, Hardik  and  Jurgens, David  and  Piper, Andrew  and  Ruths, Derek},
  title     = {Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {769--774},
  url       = {http://aclweb.org/anthology/D15-1088}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-3_discourserelations/</link>
      <pubDate>Fri, 03 Nov 2017 15:40:32 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-3_discourserelations/</guid>
      
        <description>

&lt;p&gt;Automatically generating high quality meeting notes and summaries would be awesome, but involves solving many challenges.
Here, they assume speech recognition is already done and we also know the structure over utterances indicating which previous utterance each is a response to.
The task is to label each of those utterance-utterance pairs with a type (e.g. elaboration) and to select the key phrase of each utterance.&lt;/p&gt;

&lt;p&gt;Two datasets are used, the AMI and ICSO meeting corpora, which have all of the required information.
The new idea here is to jointly model the choice of link label and the key phrase, which is intuitive.
To show the value of joint modeling they run a version of the system with the same linear model, but with independent inference, which performs quite a bit worse.&lt;/p&gt;

&lt;p&gt;One neat follow up is that by combining the key phrases into a list you get a form of summary.
According to automatic metrics it is quite a bit better than running the summarisation system they compare to, though it&amp;rsquo;s still a long way from a human summary.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1090&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{qin-wang-kim:2017:Long,
  author    = {Qin, Kechen  and  Wang, Lu  and  Kim, Joseph},
  title     = {Joint Modeling of Content and Discourse Relations in Dialogues},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {974--984},
  url       = {http://aclweb.org/anthology/P17-1090}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Shazeer et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-1_mixtureofexperts/</link>
      <pubDate>Wed, 01 Nov 2017 21:57:27 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-1_mixtureofexperts/</guid>
      
        <description>

&lt;p&gt;Mixture of experts can be seen as an ensemble approach in which we assume that each of our models is effective under different circumstances and so we combine them by switching between which we use to make our decision.
From this perspective the idea can be applied to any set of models, but here the idea is to train (1) the expert models, (2) our method of choosing between them, and (3) a set of common model components, all at the same time.&lt;/p&gt;

&lt;p&gt;The particular set up here is that they modify a series of LSTM layers, adding a new layer in between each pair of LSTMs.
The new layer has a set of small feed-forward networks (the experts) and an even simpler network that chooses which expert to use.
One big benefit of this is that a lot of computation can be avoided when we know some of the small feed-forward components are going to be ignored.
As a result, they can scale up to massive networks while still having reasonable runtimes.&lt;/p&gt;

&lt;p&gt;Some key things to make this all work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Enough machines to train it! Also, there is a careful mixture of data and model parallelism during training.&lt;/li&gt;
&lt;li&gt;Some noise in the expert selection process&lt;/li&gt;
&lt;li&gt;A loss that directly encourages the use of multiple experts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing mentioned in passing is how this relates to a form of dropout (which can be viewed as training a set of overlapping experts, kind of).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://openreview.net/pdf?id=B1ckMDqlg&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{45929,
	title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
	author  = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
	year  = {2017},
  booktitle = {ICLR},
	URL = {https://openreview.net/pdf?id=B1ckMDqlg},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)</title>
      
      <link>http://www.jkk.name/post/2017-10-31_realtimecaptioning/</link>
      <pubDate>Tue, 31 Oct 2017 13:23:13 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-31_realtimecaptioning/</guid>
      
        <description>

&lt;p&gt;For any given task, automatic systems are fast, while annotation is accurate.
This work is about bridging that gap to provide a way for a team of annotators to produce real-time high quality labels.
The specific application is speech transcription, in which automatic systems are not accurate, while average people are slow (experts can transcribe in real time, but are very expensive).&lt;/p&gt;

&lt;p&gt;The solution is to carefully break up the task and combine annotations back together.
To get it to work well there are a range of subtle design decisions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;People hear the entire audio stream, but with their section at normal volume and the rest quieter. This allows them to focus their effort while still understanding the context.&lt;/li&gt;
&lt;li&gt;The alignment process combines annotations with guidance from a language model and a model of typos based on keyboard layout.&lt;/li&gt;
&lt;li&gt;Words are locked in shortly after being typed, to encourage workers to go on rather than revising their own errors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Follow up work added several more ideas to improve performance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Time warping, slowing down to half speed for their section, then going to 1.5x for the rest.&lt;/li&gt;
&lt;li&gt;Use ASR as well, either as another worker (with very uncorrelated errors), or as a starting point for human editing (or vice versa).&lt;/li&gt;
&lt;li&gt;Use A* search rather than a greedy algorithm for the alignment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Performance does not reach the level of a professional, but is far better than ASR.
From the paper it&amp;rsquo;s tricky to see a final cost, but it is certainly far lower than the professional.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://doi.acm.org/10.1145/2380116.2380122&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Lasecki:2012:RCG:2380116.2380122,
 author = {Lasecki, Walter and Miller, Christopher and Sadilek, Adam and Abumoussa, Andrew and Borrello, Donato and Kushalnagar, Raja and Bigham, Jeffrey},
 title = {Real-time Captioning by Groups of Non-experts},
 booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
 series = {UIST &#39;12},
 year = {2012},
 isbn = {978-1-4503-1580-7},
 location = {Cambridge, Massachusetts, USA},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2380116.2380122},
 doi = {10.1145/2380116.2380122},
 acmid = {2380122},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {captioning, crowdsourcing, deaf, hard of hearing, real-time, text alignment, transcription},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-30_neuralsequence/</link>
      <pubDate>Mon, 30 Oct 2017 13:28:30 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-30_neuralsequence/</guid>
      
        <description>

&lt;p&gt;Language is bursty, with rare words occurring in clumps, the simplest example being an unusual name that occurs a lot in one news article, but not in other articles.
This paper is about how to modify a neural language model to take this into consideration, by adapting the model over time.&lt;/p&gt;

&lt;p&gt;The main idea is to have one model of overall word usage (global) and a separate model that shifts over time to take into consideration the current text (local).
The idea of adapting is not new (as the paper makes clear), but the key here is an update rule that is a modified form of RMSprop, combining the local and global models.
It also seems like performing the updates after every 5 words is important, balancing frequency with informativeness (though no ablation of frequencies is presented).
Conveniently, this is orthogonal to many other ideas and can essentially be stapled on top of a range of sequential architectures, consistently leading to improvements.&lt;/p&gt;

&lt;p&gt;One question left open is how this would work in generation.
The paper describes how it could be applied and could provide improvements, but it also seems likely to risk the repetitive outputs seen in many dialogue systems.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.07432.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv170907432K,
   author = {{Krause}, B. and {Kahembwe}, E. and {Murray}, I. and {Renals}, S.},
    title = &amp;quot;{Dynamic Evaluation of Neural Sequence Models}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1709.07432},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
     year = 2017,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170907432K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Searching for Activation Functions (Ramachandran et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-27_swishactivation/</link>
      <pubDate>Fri, 27 Oct 2017 11:22:45 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-27_swishactivation/</guid>
      
        <description>

&lt;h2 id=&#34;update&#34;&gt;Update&lt;/h2&gt;

&lt;p&gt;After prior work came to light that uses the same non-linearity, this paper was updated to focus more on the search process used to investigate the space.
One new takeaway for me was how diverse the effective activation functions were (see figures in the new version of the paper).&lt;/p&gt;

&lt;h2 id=&#34;original-post&#34;&gt;Original Post&lt;/h2&gt;

&lt;p&gt;Non-linear functions are the key to the representation power of neural networks.
Many different ones have been proposed, though it is difficult to make theoretical claims of their properties and so the choice of which to use is generally empirical.
This paper proposes a new non-linearity, $\text{swish}(x) = x \cdot \text{sigmoid}(x)$.&lt;/p&gt;

&lt;p&gt;Interestingly, it was chosen by a combination of exhaustive search and search with reinforcement learning across a range of functions, evaluating on CIFAR-10 with a small model.
ReLU variants were consistently second-best to swish variants, and generally the more complicated functions performed worse.
They do mention two functions that performed well, but didn&amp;rsquo;t generalise: $\text{cos}(x) - x$ and $\text{max}(x, \text{tanh}(x))$, which look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.jkk.name/img/post/activation-functions.png&#34; alt=&#34;Four activation functions&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In a range of experiments in vision and machine translation swish does at least as well or slightly better than the alternatives.
It also seems more robust to network depth and to work across different network structures.
As for why it works so well, there are two main ideas: (1) it adds smoothness to the ReLU, (2) it has some sensitivity to negative inputs.
Both of these seem particularly important at the start of training.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.05941.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv171005941R,
   author = {{Ramachandran}, P. and {Zoph}, B. and {Le}, Q.~V.},
    title = &amp;quot;{Swish: a Self-Gated Activation Function}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1710.05941},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171005941R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-26_multimodalwordembeddings/</link>
      <pubDate>Thu, 26 Oct 2017 20:47:12 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-26_multimodalwordembeddings/</guid>
      
        <description>

&lt;p&gt;Word2Vec and other approaches provide a single vector representing a word&amp;rsquo;s meaning, giving words spatially defined relationships capturing relatedness.
A natural extension is to consider regions in that space and allow some words to take up larger or smaller regions.
Another natural idea is to allow a single word to have multiple representations, to capture the different senses.
This paper considers both of those ideas, using multiple gaussian distributions per word.&lt;/p&gt;

&lt;p&gt;Using gaussians has the nice property that there is a closed form for calculating the amount of overlap between them, which is used as a measure of similarity.
Following ideas from word2vec, during learning the aim is to increase similarity between words that occur together and decrease it between random pairs that do not occur together.
Once the word representations are learned, KL divergence is used for similarity, along with the standard approaches that only look at the gaussian centres.&lt;/p&gt;

&lt;p&gt;In practise, two spherical distributions per word is sufficient.
Performance is better than word2vec and several other approaches for multi-sense word embeddings.
There was one puzzling line about the model suffering larger variance problems, but it was not quantified.&lt;/p&gt;

&lt;p&gt;It would be very interesting to inject some knowledge, such as from WordNet, to guide the number of gaussians per word, rather than giving them all N.
The paper also doesn&amp;rsquo;t get into details about the learned space, for example, are the two senses often far apart or close together? (in the latter case it is learning a slightly non-linear spatial representation).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1151&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{athiwaratkun-wilson:2017:Long,
  author    = {Athiwaratkun, Ben  and  Wilson, Andrew},
  title     = {Multimodal Word Distributions},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1645--1656},
  url       = {http://aclweb.org/anthology/P17-1151}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice (Mi and Huang, 2015)</title>
      
      <link>http://www.jkk.name/post/2017-10-25_shiftreducedp/</link>
      <pubDate>Wed, 25 Oct 2017 14:44:13 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-25_shiftreducedp/</guid>
      
        <description>

&lt;p&gt;This paper is a follow-up to yesterday&amp;rsquo;s, where the approach is implemented and evaluated on English and Chinese, with very strong results.
The novel contribution is the idea of introducing alternating steps in the dynamic program to do unary steps (not a novel idea in general, but novel in its application to the dynamic programming version of shift-reduce parsing).&lt;/p&gt;

&lt;p&gt;What I found interesting here were the clear benefits of the dynamic program (DP) version.
One way of viewing this is that the DP gives a more intelligent type of beam, avoiding the issue where the beam is filled with minor variations on a theme.
Results are given for various beam sizes in both approaches, but it would be interesting to see a graph where the x-axis is number of items built.
I suspect in that situation, the gap would be smaller.
On speed, there is the nice theoretical bound of $O(n)$ for this approach, but that obscures a grammar constant related to the item structure.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/N15-1108&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{mi-huang:2015:NAACL-HLT,
  author    = {Mi, Haitao  and  Huang, Liang},
  title     = {Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {May--June},
  year      = {2015},
  address   = {Denver, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {1030--1035},
  url       = {http://www.aclweb.org/anthology/N15-1108}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)</title>
      
      <link>http://www.jkk.name/post/2017-10-24_dynamictransition/</link>
      <pubDate>Tue, 24 Oct 2017 13:06:04 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-24_dynamictransition/</guid>
      
        <description>

&lt;p&gt;This paper from 2011 explores the relationship between transition based parsing and dynamic programming based parsing.
They show how to convert common dependency parsing systems (Arc-Standard and Arc-Eager) into dynamic programs, and how doing the reverse on a dynamic program gives the Arc-Hybrid approach (which has since been used in many places, and is now joined by additional systems like Arc-Swift).&lt;/p&gt;

&lt;p&gt;The benefit of this transformation is that we can find exact answers without massive beams.
The drawback is that the feature set is restricted.
This paper is theoretical, so it doesn&amp;rsquo;t give a direct measure of this tradeoff, though &lt;a href=&#34;http://www.anthology.aclweb.org/D/D13/D13-1071.pdf&#34; target=&#34;_blank&#34;&gt;follow up work&lt;/a&gt; shows that avoiding search errors is indeed beneficial.&lt;/p&gt;

&lt;p&gt;With all of the positive results using neural networks for multi-task learning, one thought this work leads to is whether we could treat different inference methods as different tasks.
In other words, have a single model encoding the input, then have multiple inference algorithms with different extensions of that model, all trained simultaneously.
The variation in available context for the different algorithms may force generality in the core representation shared across them.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P/P11/P11-1068.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kuhlmann-gomezrodriguez-satta:2011:ACL-HLT2011,
  author    = {Kuhlmann, Marco  and  G\&#39;{o}mez-Rodr\&#39;{i}guez, Carlos  and  Satta, Giorgio},
  title     = {Dynamic Programming Algorithms for Transition-Based Dependency Parsers},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {673--682},
  url       = {http://www.aclweb.org/anthology/P11-1068}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Mastering the game of Go without human knowledge (Silver et al., Nature 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-23_alphagozero/</link>
      <pubDate>Mon, 23 Oct 2017 21:12:57 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-23_alphagozero/</guid>
      
        <description>

&lt;p&gt;This paper is an extension of the original AlphaGo work on using reinforcement learning to build a Go-player.
Interestingly, the changes have simplified the overall model, as well as enabling it to do even better than the previous model, but now without any supervised training.&lt;/p&gt;

&lt;p&gt;One key change is that there is a single core neural network learning to represent the game state.
On top of that there are either a set of layers that produce an evaluation of the quality of a position, or there are a set of layers that place a distribution over moves.
This ties in nicely to a lot of work happening at the moment on multi-task learning in NLP and elsewhere.&lt;/p&gt;

&lt;p&gt;Getting into the details, they use monte-carlo tree search to choose actions during training, then update the model to better match the outcomes observed.
Starting from a completely random initialisation, the argument for why this works is that at every point in self-play the MCTS informed outcomes are just slightly better than the current model.
That edge is enough to provide a useful signal, without being such a drastic shift because in self-play the two sides are closely matched.
Interestingly, while the unsupervised model is worse at predicting what expert human players will do in a game, it is still better at predicting which player will win.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaGoZero,
  author = {Silver, David  and  Schrittwieser, Julian  and  Simonyan, Karen  and  Antonoglou, Ioannis  and  Huang, Aja  and  Guez, Arthur  and  Hubert, Thomas  and  Baker, Lucas  and  Lai, Matthew  and  Bolton, Adrian  and  Chen, Yutian  and  Lillicrap, Timothy  and  Hui, Fan  and  Sifre, Laurent  and  van den Driessche, George  and  Graepel, Thore  and  Hassabis, Demis},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  year = {2017},
  volume = {550},
  issue = {7676},
  pages = {354-359},
  publisher = {Macmillan Publishers Limited, part of Springer Nature},
  doi = {10.1038/nature24270},
  url = {http://www.nature.com/nature/journal/v550/n7676/abs/nature24270.html#supplementary-information},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Attention Is All You Need (Vaswani et al., ArXiv 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-20_onlyattention/</link>
      <pubDate>Fri, 20 Oct 2017 15:25:23 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-20_onlyattention/</guid>
      
        <description>

&lt;p&gt;Recurrent neural networks like LSTMs and GRUs have limited scope for parallelisation because each step depends on the one before it.
This architecture also means that many steps of computation separate two words that are far apart, making it difficult to capture long-distance relations.
A range of approaches have been used to try to address these issues, such as convolutional structures and other forms of recurrence (e.g. QRNNs).
The idea in this work is to use attention, applied multiple times, to get a network that is fast while still capturing positional information.&lt;/p&gt;

&lt;p&gt;To explain the structure I put together the figure below, which captures the network structure with a few simplifications:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.jkk.name/img/post/google-attention.png&#34; alt=&#34;Google Attention Network&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There a few ideas being brought together here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Positional encoding&lt;/em&gt;, which is a vector of the same length as the word representation, but that depends only on the position in the input. Here they use $f(pos, dim) = sin(pos / 10000^{2 dim / d_w})$ for even dimensions and the cosine equivalent for odd dimensions (where $d_w$ is the number of dimensions.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multi-head attention&lt;/em&gt;, where rather than running attention once on the full vector, multiple linear transforms are applied to get smaller vectors.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Scaled dot product attention&lt;/em&gt;, the equation is shown in the figure, the key new idea is to rescale by the square root of the dimensionality so that larger vectors don&amp;rsquo;t produce excessively sharp distributions after the softmax is applied. The more general form of this described in the paper has keys ($K$), queries ($Q$) and values ($V$), but the network uses the same vector for the key and value. I show the query with a solid line and the values/keys with dotted lines. The matrix $V$ is formed by using the $v$ vectors as rows, while $Q$ is formed by duplicating $q$ in every row. Note, on the left hand side attention is over all input words, while on the right it is only over&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Layer normalisation&lt;/em&gt;, a way to rescale weights to keep vector outputs in a nice range, from &lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34; target=&#34;_blank&#34;&gt;Ba, Kiros and Hinton (ArXiv 2016)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Other details, (1) When the outputs are words, the vectors used to initially represent each input word are also used to represent the outputs and in the final linear transformation (though with some rescaling). (2) They use a formula I haven&amp;rsquo;t seen before to adjust the learning rate during training, (3) dropout in several places and label smoothing are used for regularization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simplifications in the figure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For multi-head attention I only show two transforms, while in practise they used 8.&lt;/li&gt;
&lt;li&gt;The shaded regions are duplicated 6 times to create a stack, with the output of one region acting as the input to the next copy of it. The links from left to right are always from the top of the input stack.&lt;/li&gt;
&lt;li&gt;The musical repeat signs indicate that the structure is essentially the same. On the output side this isn&amp;rsquo;t quite true since the attention boxes only take inputs to their left (since output to the right doesn&amp;rsquo;t exist when they are being calculated).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In terms of experiments, it works at least as well if not better than prior approaches, and is a lot faster for machine translation (no speed numbers are given for parsing).
There is also some nice analysis of what it ends out using the attention mechanism to focus on for each word.
It seems like it can provide a way to effectively disambiguate the sense of a word based on its context.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Google also has some blog posts up
&lt;a href=&#34;https://research.googleblog.com/2017/08/transformer-novel-neural-network.html&#34; target=&#34;_blank&#34;&gt;about the paper&lt;/a&gt;
and
&lt;a href=&#34;https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html&#34; target=&#34;_blank&#34;&gt;about the library&lt;/a&gt;
they released.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{arxiv:1706.03762,
  author    = {Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {ArXiv},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</title>
      
      <link>http://www.jkk.name/post/2017-10-19_mace/</link>
      <pubDate>Thu, 19 Oct 2017 17:08:50 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-19_mace/</guid>
      
        <description>

&lt;p&gt;The standard way to get high quality annotations is to get labels from multiple people and take a majority vote.
Getting multiple annotations costs more, and the quality of annotators can vary considerably (with spamming at one extreme).
One way to avoid the quality issue is to restrict who can do the task (must have done X previous tasks with an accept rate of Y), but that limits the pool of available workers.
Another approach is to try to estimate the quality of annotator work using a statistical model.&lt;/p&gt;

&lt;p&gt;Here a generative model is used, with the following structure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$T_i$, the true label, sampled with a uniform prior over labels&lt;/li&gt;
&lt;li&gt;$S_{ij}$, a binary variable indicating if the person is spamming or not, sampled as a Bernoulli variable with a Beta prior&lt;/li&gt;
&lt;li&gt;$A_{ij}$, the annotator&amp;rsquo;s decision, if they are spamming it is sampled from a multinomial with parameters specific to them (with a Dirichlet prior), otherwise it is the true label&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$A$ is observed, but $T$ and $S$ are not, so they use expectation maximization to get both model parameters and variable values.
To deal with nonconvexity they use 100 random restarts, deciding which is best based on how well the model describes the data.
Note - this model (and the code) was the basis of the error detection paper I &lt;a href=&#34;http://www.jkk.name/post/2017-10-13_errordetection/&#34; target=&#34;_blank&#34;&gt;wrote about&lt;/a&gt; recently.&lt;/p&gt;

&lt;p&gt;For predicting annotator quality the model is consistently effective across three datasets, though the Beta and Dirichlet priors are key for one (where annotator agreement was high on average).
For determining the correct answer it is slightly better than majority vote, though the gains are small.
The real advantage comes in deciding whether to discard data, where the choice of what to discard can be guided by the estimate of quality (this is what the error detection paper was doing).
A range of synthetic experiments also show positive results, though their design shares the assumptions about behaviour that are baked into the model.&lt;/p&gt;

&lt;p&gt;I found a few results particularly interesting:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;As the number of annotators is decreased, the benefit of this approach over majority vote grows to be quite substantial (the main experiments are for data with 10 annotators).&lt;/li&gt;
&lt;li&gt;If you do use majority vote, use an odd number of annotators. Switching to an even number mainly seems to create ties. The right number is also very data dependent.&lt;/li&gt;
&lt;li&gt;Providing gold information as supervision within EM doesn&amp;rsquo;t help much unless it is quite substantial (20%+ of the data)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/N13-1132.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hovy-EtAl:2013:NAACL-HLT,
  author    = {Hovy, Dirk  and  Berg-Kirkpatrick, Taylor  and  Vaswani, Ashish  and  Hovy, Eduard},
  title     = {Learning Whom to Trust with MACE},
  booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2013},
  address   = {Atlanta, Georgia},
  publisher = {Association for Computational Linguistics},
  pages     = {1120--1130},
  url       = {http://www.aclweb.org/anthology/N13-1132}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-18_neuralamr/</link>
      <pubDate>Wed, 18 Oct 2017 21:31:05 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-18_neuralamr/</guid>
      
        <description>

&lt;p&gt;This is another paper concerned with the challenge of sparsity in AMR parsing, specifically that there are an enormous number of output symbols in the parse trees and most are seen infrequently.
The system they develop is based on the encoder-decoder with attention approach, which has previously done poorly for AMR, partially because of sparsity.&lt;/p&gt;

&lt;p&gt;Their solution is to merge certain types of symbols into groups (dates, named entities, rare verbs, constants, etc) and have a standard way to map from the surface form to the output symbol.
This is an alternative to the approach from the paper I &lt;a href=&#34;http://www.jkk.name/post/2017-10-12_amralignment/&#34; target=&#34;_blank&#34;&gt;wrote about&lt;/a&gt; last week.
They also introduce a completely separate idea, which is a different way to take an AMR graph and turn it into a linear sequence.
This change is necessary to make the output follow the form their model generates - a sequence (though there has been work on tree based LSTMs on the output side, so AMR could be directly generated, and I believe there has been some work on applying that to AMR).&lt;/p&gt;

&lt;p&gt;Together these changes do substantially improve performance over previous encoder-decoder based work for AMR.
However, there is still a substantial gap between the system and state-of-the-art, presumably because of the additional resources that other systems indirectly use by running external systems for NER, dependency parsing, etc.
Given the recent success of multi-task learning with neural nets, it would be interesting to see if those resources could be used here to further boost performance.
It may also be productive to combine these ideas with the graph abstraction ideas from AMR alignment paper.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/E/E17/E17-1035.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{peng-EtAl:2017:EACLlong1,
  author    = {Peng, Xiaochang  and  Wang, Chuan  and  Gildea, Daniel  and  Xue, Nianwen},
  title     = {Addressing the Data Sparsity Issue in Neural AMR Parsing},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  month     = {April},
  year      = {2017},
  address   = {Valencia, Spain},
  publisher = {Association for Computational Linguistics},
  pages     = {366--375},
  url       = {http://www.aclweb.org/anthology/E17-1035}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-17_nedisambiguation/</link>
      <pubDate>Tue, 17 Oct 2017 20:33:58 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-17_nedisambiguation/</guid>
      
        <description>

&lt;p&gt;Several NLP tasks aim to identify information regarding entities, such as when two sections of text are referring to the same thing, or which thing out of a large set (e.g. things in Wikipedia) a piece of text is about.
This paper focuses on a subset of entity linking, trying to determine which entity out of a set of candidates is the correct one (in a way a kind of reranker for entity linking).&lt;/p&gt;

&lt;p&gt;The task is based on a really cool dataset from Google+UMass, which collected text that was hyperlinked to wikipedia articles.
The idea is that the text (&lt;em&gt;mention&lt;/em&gt;) is probably a reference to the thing the article describes, so it is an easy way to get entity linked data for free.
Here, the data is filtered to mentions that aren&amp;rsquo;t too rare (more than 10 occurrences) and where the mention isn&amp;rsquo;t used to refer to too many different entities (the two most common entities account for over 10% of occurrences).
Then, the set of things that this mention is used to refer to somewhere are treated as a list of candidates, and the task is to choose which one is correct in a given context.&lt;/p&gt;

&lt;p&gt;The model is of the common style at the moment:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The context is processed using a recurrent neural network to produce a set of vectors&lt;/li&gt;
&lt;li&gt;Attention is used to produce vectors that combine the context with a candidate entity&lt;/li&gt;
&lt;li&gt;A feedforward neural network produces a score that is maxed over to get a final decision&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On the wikilinks based dataset this performs quite a bit better than other models, but it is behind on the smaller manually curated datasets used elsewhere (YAGO and PPRforNED, which link entities in the CoNLL 2003 shared task).
Interestingly, augmenting the training data for YAGO with data from wikilinks does improve performance.
For future users of the wikilinks data there is also some nice analysis at the end of remaining challenges, which are spit between mistakes in the data (unsurprising given the approximate collection process), answers that are too general or specific, tricky cases, and the long tail (which would be even longer without the filtering used in these experiments).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/K/K17/K17-1008.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{eshel-EtAl:2017:CoNLL,
  author    = {Eshel, Yotam  and  Cohen, Noam  and  Radinsky, Kira  and  Markovitch, Shaul  and  Yamada, Ikuya  and  Levy, Omer},
  title     = {Named Entity Disambiguation for Noisy Text},
  booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)},
  month     = {August},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {58--68},
  url       = {http://aclweb.org/anthology/K17-1008}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-16_forumrnn/</link>
      <pubDate>Mon, 16 Oct 2017 20:55:07 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-16_forumrnn/</guid>
      
        <description>

&lt;p&gt;Attention - a weighted average over a set of vectors representing context - has consistently produced positive results.
Here we see an example of how it can be applied in the case of modeling a threaded discussion.&lt;/p&gt;

&lt;p&gt;Attention is applied in two ways.
First, over a fixed set of vectors.
This is intended to provide a mechanism to choose between several different sub-models contained within a single model.
Put differently, the vectors provide a set of latent representations that capture each of the different types of posts in the subreddit.
Second, attention over the current utterance is used in the process of predicting responses (at training time only).
This provides an additional source of input to the model, by forcing it to explain the response utterances using the same representations as a source of information.&lt;/p&gt;

&lt;p&gt;The application is a new task, using values assigned to posts = upvotes - downvotes (i.e. Reddit karma).
Predicting the specific value is hard, so the task is split into 7 binary decisions about whether a post has a score higher or lower than some value.
On this task the new approach provides consistent gains, though overall performance remains low (53 - 56%).
Confusingly though, one of the figures (number 4) seems to suggest that it was a single multi-way decision, not a set of binary decisions.
I&amp;rsquo;m also curious about the data, in particular what the distribution of scores is.
The paper mentions it is Zipfian, but surely it would be something double-sided with a massive peak at 0 and a rapid drop in either direction?&lt;/p&gt;

&lt;p&gt;Overall, this is further evidence of the versatility of the idea of attention!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/D17-1242.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{cheng-fang-ostendorf:2017:EMNLP2017,
  author    = {Cheng, Hao  and  Fang, Hao  and  Ostendorf, Mari},
  title     = {A Factored Neural Network Model for Characterizing Online Discussions in Vector Space},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2286--2296},
  url       = {https://www.aclweb.org/anthology/D17-1242}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-13_errordetection/</link>
      <pubDate>Fri, 13 Oct 2017 13:32:19 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-13_errordetection/</guid>
      
        <description>

&lt;p&gt;Active learning doesn&amp;rsquo;t seem to get much attention in NLP, probably because of fear that developing data based on the errors of one model will introduce a particular sampling bias.
This paper is a nice example of a problem it can be applied to that doesn&amp;rsquo;t raise that issue: detecting all the errors in a system&amp;rsquo;s output.&lt;/p&gt;

&lt;p&gt;The scenario is that you have a bunch of models for doing a task (e.g. POS tagging) and a new dataset with no labeled data, which you would like to label.
Having a person label the data would take a long time and doesn&amp;rsquo;t take advantage of these systems.
At the same time, we can&amp;rsquo;t just run the systems and use their output because they aren&amp;rsquo;t perfect, particularly out of domain.
We could run the systems and check their output, which could be faster than annotating directly, but would still take a long time.
If we don&amp;rsquo;t mind having some errors, we can check just some output, but how do we decide what to check?&lt;/p&gt;

&lt;p&gt;This paper applies the generative model from &lt;a href=&#34;http://www.aclweb.org/anthology/N13-1132&#34; target=&#34;_blank&#34;&gt;MACE&lt;/a&gt; to build a generative model of system outputs.
The model is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For each example, sample the true label with a uniform prior&lt;/li&gt;
&lt;li&gt;Then, for each classifier, sample from a Bernoulli distribution to decide if they are good or not&lt;/li&gt;
&lt;li&gt;A good classifier returns the true label, a not good classifier samples from a multinomial over the options&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we don&amp;rsquo;t know the parameters of the model, or the true labels, use expectation maximisation to learn.&lt;/p&gt;

&lt;p&gt;This work takes that model, trains it and uses it to identify the sample that is most uncertain.
A person annotates it, the correct label replaces one of the system predictions, and EM is run again.
This is repeated until either there appear to be no more errors, or annotators run out of time.&lt;/p&gt;

&lt;p&gt;How well does it work?
The main metric is precision: how many of the instances asked for annotation actually have errors.
For POS tagging on WSJ text, the taggers initially get 2.5% of words wrong.
To get that down to 1.1% the precision is 33%, and to get it to 0.65% precision is 17.6%.
On an out of domain dataset, the error rate is 10% initially, and is down to 5% with a precision of 50%.
Put differently, in a dataset of 25,000 tokens, with 2,500 errors, after checking 2,500 tokens, there are only 1,250 errors (another 2,500 checks brings it down to 730).
It also works well for NER, and consistently does better than the alternative they compare to (consider the taggers a committee and find the examples with highest entropy, i.e. greatest disagreement).&lt;/p&gt;

&lt;p&gt;This seems like a natural fit for &lt;a href=&#34;https://prodi.gy/&#34; target=&#34;_blank&#34;&gt;prodigy&lt;/a&gt; and something that could be broadly useful.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/P/P17/P17-1107.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{rehbein-ruppenhofer:2017:Long,
  author    = {Rehbein, Ines  and  Ruppenhofer, Josef},
  title     = {Detecting annotation noise in automatically labelled data},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1160--1170},
  url       = {http://aclweb.org/anthology/P17-1107}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-12_amralignment/</link>
      <pubDate>Thu, 12 Oct 2017 19:52:34 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-12_amralignment/</guid>
      
        <description>

&lt;p&gt;Abstract Meaning Representation (AMR) structures represent sentence meaning with labeled nodes (concepts) that are related to the words in the sentence, but not explicitly linked to them.
This is a problem for most parsing algorithms, which need a way to efficiently decompose the structure in order to learn how to generate it.
In dependency parsing there are no abstract nodes to generate, in constituency parsing there is a very small set of node types, and for CCG, TAG, etc the labels come from a constrained space.
The solution for many AMR parsers is to have a process for generating the concepts as a first step towards parsing, and to automatically align the training data to guide this concept generation stage.&lt;/p&gt;

&lt;p&gt;The first idea in this paper is about the set of AMR concepts.
Some concepts are easy to link, as the concept clearly maps to a single word in the sentence.
Around a quarter of concepts have a more complex relation, where a set of concepts link to a set of words, for example, named entities.
The idea for these is to identify common subgraphs by abstracting some lexical items.
For example, a teacher and a worker both get mapped to a person concept that is the ARG0 of the lexical item (teach, or work in this case).
This can allow for the generation of entirely novel concepts (e.g. &amp;ldquo;concept&amp;rdquo;-er), giving a 0.6 boost to recall for CAMR simply by making these additional concepts available.
Using a bidirectional LSTM with a character CNN to generate features on likely concepts, there is a gain of 1.0 F1 for the parser.&lt;/p&gt;

&lt;p&gt;The second idea is to improve the alignments used to train concept generation by taking into consideration the graph structure.
To use an aligner developed for machine translation the graph needs to be turned into a linear sequence, but that can lead to strange jumps.
The idea here is to take that into consideration by modifying the calculation of the cost of distortion (i.e. jumping) to be reshaped based on the graph structure.
For optimal alignment quality they consider aligning in either direction, directly changing the distance metric in the English-AMR direction, and just rescaling it to be less sensitive when appropriate for AMR-English.
This is definitely higher precision than prior approaches, but lower recall.
It&amp;rsquo;s hard to tell whether this helps, since the evaluation doesn&amp;rsquo;t separate it out from the first idea (results in section 5.3 are not on the same dataset as 5.1).&lt;/p&gt;

&lt;p&gt;Given how separate this is from CAMR, it would be interesting to see if it helps other systems similarly.
With concept identification at 83 F there is still plenty of scope for improvement, though there is no analysis of which types of concepts remain the most problematic.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/D/D17/D17-1130.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{wang-xue:2017:EMNLP2017,
  author    = {Wang, Chuan  and  Xue, Nianwen},
  title     = {Getting the Most out of AMR Parsing},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {1268--1279},
  url       = {https://www.aclweb.org/anthology/D17-1130},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-11_multimt/</link>
      <pubDate>Wed, 11 Oct 2017 17:29:04 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-11_multimt/</guid>
      
        <description>

&lt;p&gt;This paper is a detailed analysis of a surprisingly effective simple idea: train a machine translation system with sentence pairs from multiple languages, adjusting the input to have an extra token at the end that says what the target language is.
To deal with class imbalance, data is oversampled to have all language pairs be equally represented (though even without that, it works fairly well).&lt;/p&gt;

&lt;p&gt;The biggest advantage of this approach is that a single model can handle translation between many pairs, rather than needing $O(n^2)$ models for $n$ languages.
The performance is slightly lower on average, but the single model can manage with far fewer parameters.
In one example, twelve models are combined into a single model with as many parameters as one of the twelve, and the results are lower by just 0.76 BLEU on average.
Another advantage of the model is the ability to handle code-switched language, though they didn&amp;rsquo;t have evaluation datasets to get an quantitative measure of accuracy.&lt;/p&gt;

&lt;p&gt;Having this model also opens up the possibility of translating between pairs of languages with no parallel training data (A -&amp;gt; B).
As long as there is data (A -&amp;gt; C) and (D -&amp;gt; B), sentences from A can be fed in with B as the target language.
For closely related languages this works very well, and in particular, better than going via another language such that there is data for the two language pairs.
For example, going from Portuguese to Spanish with the multilingual model scores 24.75, whereas going via English scores 21.62 and a model with explicit training data gets 31.50.
Going between less related languages is less successful, with direct Spanish to Japanese scoring 9.14, and going via English scoring 18.00.
One thing I wish the paper had is more exploration of this result - what does it get right when scoring 9.14?
For the time being at least, going via a third language still seems necessary, and presumably the best language to use is whichever one the performance is highest on.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/Q/Q17/Q17-1024.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.04558.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv version&lt;/a&gt; which appears to be the same aside from one extra figure of the model architecture.&lt;/p&gt;

&lt;p&gt;As an aside, it is interesting to see the timeline for this paper:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;November 2016, Submission to ArXiv and in the TACL submission batch&lt;/li&gt;
&lt;li&gt;March 2017, TACL revision batch&lt;/li&gt;
&lt;li&gt;October 2017, TACL published&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1081,
	author    = {Johnson, Melvin  and Schuster, Mike  and Le, Quoc  and Krikun, Maxim  and Wu, Yonghui  and Chen, Zhifeng  and Thorat, Nikhil  and Vigas, Fernanda  and Wattenberg, Martin  and Corrado, Greg  and Hughes, Macduff  and Dean, Jeffrey},
	title     = {Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	journal   = {Transactions of the Association for Computational Linguistics},
	volume    = {5},
	year      = {2017},
	issn      = {2307-387X},
	url       = {https://www.transacl.org/ojs/index.php/tacl/article/view/1081},
	pages     = {339--351}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Search-based Neural Structured Learning for Sequential Question Answering (Iyyer et al., ACL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-10_seqqa/</link>
      <pubDate>Tue, 10 Oct 2017 13:43:36 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-10_seqqa/</guid>
      
        <description>

&lt;p&gt;Semantic parsing datasets generally consist of (question, answer) pairs, where each pair is completely independent of the rest (one exception is ATIS, which has multi-turn conversations, though most work doesn&amp;rsquo;t use them).
In reality, we often ask a series of simple questions that together form a complex one, for example &amp;ldquo;What flights are available from Detroit to Sydney? And how much is the price if I don&amp;rsquo;t want to leave before 8am?&amp;rdquo;
This work explores these kinds of sequential questions with a new dataset and algorithm.&lt;/p&gt;

&lt;p&gt;The dataset was formed by asking crowd workers to rephrase questions from the WikiTableQuestions dataset into sequences of shorter questions.
This naturally constrains the types of questions (in particular, they reference a single table only), but covers a range of domains.
With 6,066 question sequences, and on average 2.9 questions / sequence, it&amp;rsquo;s a large dataset by semantic parsing standards.
However, there are no logical forms, only the row, column, or cell(s) that contain the answer.&lt;/p&gt;

&lt;p&gt;To solve the problem, they treat it as choosing a sequence of actions, where each action generate a part of the execution instructions.
The model follows the recent approach of considering the contents of the database as part of the calculation (e.g. by taking the dot product of the vector for a cell and the vector for the question).&lt;/p&gt;

&lt;p&gt;The system has consistently better performance than other QA systems on the new dataset (though no results are shown for the WikiTableQuestions dataset).
At only 12.8% of sequences completely correct, there is plenty of scope for improvement.
Based on the description of the operators there are definitely additional abilities that would be useful, so this model has potential to improve.
That said, it seems difficult to generalise the model to handle more complicated databases with multiple interconnected tables.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://aclweb.org/anthology/P/P17/P17-1167.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{iyyer-yih-chang:2017:Long,
  author    = {Iyyer, Mohit  and  Yih, Wen-tau  and  Chang, Ming-Wei},
  title     = {Search-based Neural Structured Learning for Sequential Question Answering},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1821--1831},
  url       = {http://aclweb.org/anthology/P17-1167}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title> Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)</title>
      
      <link>http://www.jkk.name/post/2017-10-09_parsing-autoencoder/</link>
      <pubDate>Mon, 09 Oct 2017 14:31:24 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-09_parsing-autoencoder/</guid>
      
        <description>

&lt;p&gt;Semantic parsing datasets are small because they are expensive to produce (logical forms don&amp;rsquo;t occur naturally and writing them down takes time).
The idea here is to do semi-supervised learning by implementing both a parser and a generator, which are trained together as a form of autoencoder where the intermediate representation is natural language.&lt;/p&gt;

&lt;p&gt;The architecture has four LSTMs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Bidirectional LSTM over a logical form.&lt;/li&gt;
&lt;li&gt;One directional LSTM attending to the first LSTM&amp;rsquo;s hidden states, generating a sentence.&lt;/li&gt;
&lt;li&gt;Bidirectional LSTM over the sentence generated by the second LSTM.&lt;/li&gt;
&lt;li&gt;One directional LSTM attending to the third LSTM&amp;rsquo;s hidden states, generating a logical form.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Usually a component like the second LSTM would choose the max word at each position (or use beam search), but here they want this whole thing to be differentiable, so the distribution over words is used.
At evaluation time only the second half (3+4) is used, with the test sentence as input.&lt;/p&gt;

&lt;p&gt;With this structure, a loss function is defined that compares the input to (1) and the output of (4), which in both cases is a logical form.
As a result, they don&amp;rsquo;t need (logical form, sentence) pairs to train, they can use automatically generated logical forms.
Of course, with only logical forms it would do something random with the intermediate representation, so some supervised examples are also needed (in which case the two halves are trained independently).&lt;/p&gt;

&lt;p&gt;The results are not state-of-the-art, but good on all three tasks (Geoquery, NLmaps, SAIL), and on two they show am improvement over training (3+4) with only supervised data.
Varying the amount of training data gives a less clear picture.
On Geoquery with 5-25% of the data, this approach clearly helps, particularly if the queries are real rather than generated (which is a realistic scenario), but then there is no improvement for 50% or 75%, and at 100% the improvement is small.
On NLmaps there was no generator, and the differences at different data %s seem like noise.
SAIL has the most clear benefit, though it&amp;rsquo;s a particularly small dataset, consisting of paths in just four maps.&lt;/p&gt;

&lt;p&gt;This is a cool idea that seems effective in certain situations.
The generator is key, and it&amp;rsquo;s possible that performance on GeoQuery would be higher with a more sophisticated one (e.g. a tree structured generator, rather than the ngram model used here).
One idea mentioned in the conclusion is to try reversing the setup (3-4-1-2) and training with natural language examples that have no logical form.
How to tradeoff the different data scenarios seems like an interesting challenge!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://aclweb.org/anthology/D16-1116&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kovcisky-EtAl:2016:EMNLP2016,
  author    = {Ko\v{c}isk\&#39;{y}, Tom\&#39;{a}\v{s}  and  Melis, G\&#39;{a}bor  and  Grefenstette, Edward  and  Dyer, Chris  and  Ling, Wang  and  Blunsom, Phil  and  Hermann, Karl Moritz},
  title     = {Semantic Parsing with Semi-Supervised Sequential Autoencoders},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  pages     = {1078--1087},
  url       = {https://aclweb.org/anthology/D16-1116}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-06-madlibs/</link>
      <pubDate>Fri, 06 Oct 2017 13:31:43 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-06-madlibs/</guid>
      
        <description>

&lt;p&gt;Humor is an incredibly difficult problem, as this paper makes clear in its background section.
Most work has considered very specific types of jokes (e.g. &amp;ldquo;that&amp;rsquo;s what she said&amp;rdquo;, or pairs of words that sound similar to form riddles).
This work contributes (1) a new task, (2) an evaluation method, and (3) an example system.&lt;/p&gt;

&lt;p&gt;The task is Mad Libs, where a story has some words removed and people choose new words to make the story funny.
If you are familiar with the normal version, one key difference is that here people have access to the complete story when they are choosing their words.
A set of 40 &amp;lsquo;stories&amp;rsquo; were written based on Simple Wikipedia articles, and workers on Mechanical Turk wrote words to fill them, with filtering based on judging by other workers.&lt;/p&gt;

&lt;p&gt;The evaluation method involved recruiting a set of judges on Mechanical Turk and asking a series of questions to measure humour for a given response.
As well as judging the overall story, they were asked to select which words contributed the most.
By aggregating these selections as votes, each word was scored as funny or not.&lt;/p&gt;

&lt;p&gt;The system is a linear classifier with a range of features, including scores from a language model.
On its own, it performs very poorly, but using it as a filter to restrict the space of words a person can choose from actually leads to better performance than people on their own.
Of course, it&amp;rsquo;s difficult to analyse the source of improvement;
The authors theorise that it is because it prevents people from selecting words that only they would see is funny.
Another interpretation is that the constraint gives them a smaller space to think about and so they can find more interesting plays on words.&lt;/p&gt;

&lt;p&gt;Finally, as a non-expert in this area, this paper had some nice discussion of the tradeoffs between different ways of generating humour (incongruous vs. coherent content strategies).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1068&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hossain-EtAl:2017:EMNLP2017,
  author    = {Hossain, Nabil  and  Krumm, John  and  Vanderwende, Lucy  and  Horvitz, Eric  and  Kautz, Henry},
  title     = {Filling the Blanks (hint: plural noun) for Mad Libs Humor},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {649--658},
  url       = {https://www.aclweb.org/anthology/D17-1068},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-05-deftnn/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-05-deftnn/</guid>
      
        <description>

&lt;p&gt;This paper proposes two techniques for speeding up neural network execution on GPUs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Reduce computation when doing matrix-multiply by removing rows.&lt;/li&gt;
&lt;li&gt;Reduce communication on the GPU by halving the number of bits used to represent numbers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Either of these gives a speed up of ~1.5x and together they give ~2x, across a range of different computer vision tasks+models.&lt;/p&gt;

&lt;h2 id=&#34;core-ideas-in-detail&#34;&gt;Core ideas in detail&lt;/h2&gt;

&lt;p&gt;The first idea, reducing work by eliminating parts of the computation, has been considered before.
In the past, however, the focus was on saving memory in models, and so the most common strategy was to move to a sparse matrix where weights close to zero are dropped.
Here the focus is on speed and they show that while the sparse approach saves memory it can end up being slower because of hardware behaviour.
Instead, they eliminate entire rows of the matrix, which means there is less computation, but it remains dense (and therefore fast).
Rows are identified by measuring correlation between outputs and greedily eliminating rows that correlate highly with the rest of the output.&lt;/p&gt;

&lt;p&gt;The natural question to ask is whether this hurts performance.
First, they do two things to avoid problems, (1) a scale factor is used to make sure the outputs are of the same range that they would have been with the full matrix, and (2) they restart training to fine-tune the network once pruning is set up.
With high enough pruning accuracy does fall, but speed ups can be gained before that is a problem (the exact point depends on the task).&lt;/p&gt;

&lt;p&gt;The second idea relates to numerical representation, and is motivated by measurements of where the bottlenecks are in communication.
Many AI researchers have tried switching to 16 bit representations to save space and time, but here they develop a different floating point encoding that gives more bits to the exponent, and fewer to the mantissa.&lt;/p&gt;

&lt;h2 id=&#34;thoughts&#34;&gt;Thoughts&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;It would be interesting to see the interaction of this work with the investigation of networks without non-linear functions that can still learn non-linear behaviour because of numerical approximations.&lt;/li&gt;
&lt;li&gt;In the context of language, the weight reduction approach would be interesting to analyse. Specifically, what do we lose in our word vectors depending on the task?&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve always had some interest in making things faster. It would be interesting to know where the remaining bottlenecks are (after applying these changes).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Hill:MICRO:2017,
  author = {Parker Hill, Animesh Jain, Mason Hill1, Babak Zamirai, Chang-Hong Hsu, Michael A. Laurenzano, Scott Mahlke, Lingjia Tang and Jason Mars},
  title = {DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission},
  booktitle = {The 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  year = {2017},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Ordering Pizza for an Event with Vegetarians</title>
      
      <link>http://www.jkk.name/post/pizza/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/pizza/</guid>
      
        <description>

&lt;h2 id=&#34;how-much-vegetarian-pizza-should-i-order&#34;&gt;How much vegetarian pizza should I order?&lt;/h2&gt;

&lt;p&gt;This question frequently comes up in the world of free food at university events. In my experience (as someone who does not eat meat pizzas), often not enough is ordered. Let&amp;rsquo;s try to come up with a model to tell us how much to order. Set it up like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are $N$ people.&lt;/li&gt;
&lt;li&gt;There are $P$ pizzas.&lt;/li&gt;
&lt;li&gt;The fraction of people who are vegetarian is $V$.&lt;/li&gt;
&lt;li&gt;Assume everyone eats the same amount of pizza, and all the pizza is eaten (ie. each person eats $\frac{P}{N}$).&lt;/li&gt;
&lt;li&gt;Assume people randomly sample from the available pizzas, subject to the constraint that some eat only vegetarian pizzas.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, let the fraction of vegetarian pizzas we get be $k$, and we can write down the number of vegetarian pizzas in two ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How many we order: $P * k$&lt;/li&gt;
&lt;li&gt;How many are eaten: (pizzas eaten by vegetarians) + (vegetarian pizzas eaten by others) = $\frac{P}{N} * (N * V) + \frac{P}{N} * (N * (1 - V)) * k$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since all the pizza we order is eaten, these are equal.
$N$ and $P$ are both positive numbers, so we can safely cancel the $N$s and divide through by $P$, giving:&lt;/p&gt;

&lt;p&gt;$V + (1 - V) * k = k$&lt;/p&gt;

&lt;p&gt;To satisfy this equation, $k = 1$. Therefore all the pizza should be vegetarian :)&lt;/p&gt;

&lt;p&gt;Of course, these assumptions aren&amp;rsquo;t quite right (for example, not everyone samples randomly from the available pizza), so here are some more useful suggestions too:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Do order more than the proportion of vegetarians.&lt;/li&gt;
&lt;li&gt;Place the vegetarian pizza at the end of the line of pizzas, or in a separate location with clear signage discouraging non-vegetarians from eating it.&lt;/li&gt;
&lt;li&gt;Order a diverse set of popular meat pizzas (people tend to want variety, so this encourages them to try more meat pizzas).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For other peoples&amp;rsquo; thoughts on this question see
&lt;a href=&#34;http://www.seriouseats.com/2014/07/etiquette-ordering-pizza-for-a-group-manner-matters.html&#34; target=&#34;_blank&#34;&gt;Serious Eats&lt;/a&gt; and
&lt;a href=&#34;https://www.quora.com/What-are-the-best-pizza-toppings-to-get-for-a-big-group&#34; target=&#34;_blank&#34;&gt;Quora&lt;/a&gt;&lt;/p&gt;
</description>
      

    </item>
    
    <item>
      
        <title></title>
      
      <link>http://www.jkk.name/home/about/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/about/</guid>
      
        <description>&lt;p&gt;My email address is: jkummerf@umich.edu&lt;/p&gt;

&lt;p&gt;I am a Postdoctoral Research Fellow, working at the &lt;a href=&#34;https://www.umich.edu/&#34; target=&#34;_blank&#34;&gt;University of Michigan&lt;/a&gt;, in &lt;a href=&#34;https://www.cse.umich.edu/&#34; target=&#34;_blank&#34;&gt;Computer Science and Engineering&lt;/a&gt;.
My research focus is Natural Language Processing - in particular, I am working on &lt;a href=&#34;https://sapphire.eecs.umich.edu&#34; target=&#34;_blank&#34;&gt;Project Sapphire&lt;/a&gt;, a collaboration with IBM, developing a conversational academic adviser.
I am also affiliated with two research groups: &lt;a href=&#34;http://lit.eecs.umich.edu/&#34; target=&#34;_blank&#34;&gt;Language and Information Technologies&lt;/a&gt;, and &lt;a href=&#34;http://web.eecs.umich.edu/~wlasecki/croma.html&#34; target=&#34;_blank&#34;&gt;Crowds+Machines&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I completed my PhD in the &lt;a href=&#34;http://web.eecs.umich.edu/~wlasecki/croma.html&#34; target=&#34;_blank&#34;&gt;UC Berkeley NLP Group&lt;/a&gt;, advised by &lt;a href=&#34;http://www.cs.berkeley.edu/~klein&#34; target=&#34;_blank&#34;&gt;Dan Klein&lt;/a&gt;, with a thesis on new algorithms related to syntactic parsing: error analysis, formalism conversion, and graph parsing.
I completed my BSc (Adv) with honours and medal in the &lt;a href=&#34;http://www.schwa.org&#34; target=&#34;_blank&#34;&gt;University of Sydney Schwa Lab&lt;/a&gt; advised by &lt;a href=&#34;http://sydney.edu.au/engineering/it/~james&#34; target=&#34;_blank&#34;&gt;James Curran&lt;/a&gt;, with a thesis on an algorithm for faster CCG parsing.
I received my Higher School Certificate at &lt;a href=&#34;http://www.emanuelschool.nsw.edu.au/&#34; target=&#34;_blank&#34;&gt;Emanuel School&lt;/a&gt;, receiving the Premier&amp;rsquo;s Award for my results in English, Mathematics, Physics, and Cosmology.&lt;/p&gt;

&lt;p&gt;My &lt;a href=&#34;http://www.jkk.name/doc/jonathan-kummerfeld-cv.pdf&#34;&gt;CV&lt;/a&gt; is available as a pdf.&lt;/p&gt;
</description>
      

    </item>
    
    <item>
      
      <title>
        "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
        
          Greg Durrett, Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, Rebecca S. Portnoff, Sadia Afroz, Damon McCoy, Kirill Levchenko, Vern Paxson
        
        (EMNLP,
        2017)
      </title>
      
      <link>http://www.jkk.name/publication/emnlp17forums/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/emnlp17forums/</guid>
      
        <description>
          One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects.  We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums.  Each of these forums constitutes its own &#34;fine-grained domain&#34; in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.
        </description>
      

    </item>
    
    <item>
      
        <title>IE/NER from Cybercriminal Forums</title>
      
      <link>http://www.jkk.name/data/cybercrime-forums/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/cybercrime-forums/</guid>
      
        <description>Forum posts with annotations of products. <a href="https://evidencebasedsecurity.org/forums/#data" target="_blank">https://evidencebasedsecurity.org/forums/#data</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection",
        
          Youxuan Jiang, Jonathan K. Kummerfeld, Walter S. Lasecki
        
        (ACL (short),
        2017)
      </title>
      
      <link>http://www.jkk.name/publication/acl17paraphrase/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl17paraphrase/</guid>
      
        <description>
          Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.
        </description>
      

    </item>
    
    <item>
      
        <title>Crowdsourced Paraphrases</title>
      
      <link>http://www.jkk.name/data/paraphrasing-sample/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/paraphrasing-sample/</guid>
      
        <description>Paraphrases collected while conducting experiments on factors influencing crowd performance. <a href="http://aclweb.org/anthology/attachments/P/P17/P17-2017.Datasets.zip" target="_blank">http://aclweb.org/anthology/attachments/P/P17/P17-2017.Datasets.zip</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Tools for Automated Analysis of Cybercriminal Markets",
        
          Rebecca S. Portnoff, Sadia Afroz, Greg Durrett, Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, Damon McCoy, Kirill Levchenko, Vern Paxson
        
        (WWW,
        2017)
      </title>
      
      <link>http://www.jkk.name/publication/www17forums/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/www17forums/</guid>
      
        <description>
          Underground forums are widely used by criminals to buy and sell a host of stolen items, datasets, resources, and criminal services.  These forums contain important resources for understanding cybercrime.  However, the number of forums, their size, and the domain expertise required to understand the markets makes manual exploration of these forums unscalable. In this work, we propose an automated, top-down approach for analyzing underground forums.  Our approach uses natural language processing and machine learning to automatically generate high-level information about underground forums, first identifying posts related to transactions, and then extracting products and prices. We also demonstrate, via a pair of case studies, how an analyst can use these automated approaches to investigate other categories of products and transactions. We use eight distinct forums to assess our tools: Antichat, Blackhat World, Carders, Darkode, Hack Forums, Hell, L33tCrew and Nulled. Our automated approach is fast and accurate, achieving over 80% accuracy in detecting post category, product, and prices.
        </description>
      

    </item>
    
    <item>
      
        <title>One-Endpoint Crossing Graph Parser</title>
      
      <link>http://www.jkk.name/software/1ec-parsing/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/1ec-parsing/</guid>
      
        <description>A range of tools related to one-endpoint crossing graphs - parsing, format conversion, and evaluation. <a href="https://github.com/jkkummerfeld/1ec-graph-parser" target="_blank">https://github.com/jkkummerfeld/1ec-graph-parser</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Parsing with Traces: An O($n^4$) Algorithm and a Structural Representation",
        
          Jonathan K. Kummerfeld, Dan Klein
        
        (TACL,
        2017)
      </title>
      
      <link>http://www.jkk.name/publication/tacl17parsing/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/tacl17parsing/</guid>
      
        <description>
          General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.  We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.
        </description>
      

    </item>
    
    <item>
      
        <title>Spine and Arc version of the Penn Treebank</title>
      
      <link>http://www.jkk.name/data/shp-ptb/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/shp-ptb/</guid>
      
        <description>Code to convert the standard Penn Treebank into a version where each word is assigned a spine of non-terminals, and arcs to indicate attachments from one spine to another. <a href="https://github.com/jkkummerfeld/1ec-graph-parser/tree/master/format-conversion" target="_blank">https://github.com/jkkummerfeld/1ec-graph-parser/tree/master/format-conversion</a></description>
      

    </item>
    
    <item>
      
        <title>Example Talk</title>
      
      <link>http://www.jkk.name/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/talk/example-talk/</guid>
      
        <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://gcushen.github.io/hugo-academic-demo/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
      

    </item>
    
    <item>
      
      <title>
        "Algorithms for Identifying Syntactic Errors and Parsing with Graph Structured Output",
        
          Jonathan K. Kummerfeld
        
        (EECS Department, University of California, Berkeley,
        2016)
      </title>
      
      <link>http://www.jkk.name/publication/thesis16parsing/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/thesis16parsing/</guid>
      
        <description>
          Representation of syntactic structure is a core area of research in Computational Linguistics, disambiguating distinctions in meaning that are crucial for correct interpretation of language. Development of algorithms and statistical models over the past three decades has led to systems that are accurate enough to be deployed in industry, playing a key role in products such as Google Search and Apple Siri. However, syntactic parsers today are usually constrained to tree representations of language, and performance is interpreted through a single metric that conveys no linguistic information regarding remaining errors.

In this dissertation, we present new algorithms for error analysis and parsing. The heart of our approach to error analysis is the use of structural transformations to identify more meaningful classes of errors, and to enable comparisons across formalisms. For parsing, we combine a novel dynamic program with careful choices in syntactic representation to create an efficient parser that produces graph structured output. Together, these developments allowed us to evaluate the outstanding challenges in parsing and to address a key weakness in current work.

First, we present a search algorithm that, given two structures, finds a sequence of modifications leading from one structure to the other. We applied this algorithm to syntactic error analysis, where one structure is the output of a parser, the other is the correct parse, and each modification corresponds to fixing one error. We constructed a tool based on the algorithm and analyzed variations in behavior between parsers, types of text, and languages. Our observations shine light on several assumptions about syntactic errors, showing some to be true and others to be false. For example, prepositional phrase attachment errors are indeed a major issue, while coordination scope errors do not hurt performance as much as expected.

Next, we describe an algorithm that builds a parse in one syntactic representation to match a parse in another representation. Specifically, we build phrase structure parses from Combinatory Categorial Grammar derivations. Our approach follows the philosophy of CCG, defining specific phrase structures for each lexical category and generic rules for combinatory steps. The new parse is built by following the CCG derivation bottom-up, gradually building the corresponding phrase structure parse. This produced significantly more accurate parses than past work, and enabled us to compare performance of several parsers across formalisms.

Finally, we address a weakness we observed in phrase structure parsers: the exclusion of syntactic trace structures for computational convenience. We present an efficient dynamic programming algorithm that constructs the graph structure that has the highest score under an edge-factored scoring function. We define a parse representation compatible with the algorithm, and show how certain linguistic distinctions dramatically impact coverage. We also show various ways to modify the algorithm to improve performance by exploiting properties of observed linguistic structure. This approach to syntactic parsing is the first to cover virtually all structure encoded in the Penn Treebank.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "An Empirical Analysis of Optimization for Max-Margin NLP",
        
          Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, Dan Klein
        
        (EMNLP,
        2015)
      </title>
      
      <link>http://www.jkk.name/publication/emnlp15learn/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/emnlp15learn/</guid>
      
        <description>
          Despite the convexity of structured max-margin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal  optimization methods are often more robust and progress faster than dual methods. This advantage  is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Error-Driven Analysis of Challenges in Coreference Resolution",
        
          Jonathan K. Kummerfeld, Dan Klein
        
        (EMNLP,
        2013)
      </title>
      
      <link>http://www.jkk.name/publication/emnlp13analysis/</link>
      <pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/emnlp13analysis/</guid>
      
        <description>
          Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.
        </description>
      

    </item>
    
    <item>
      
        <title>Coreference Error Analysis</title>
      
      <link>http://www.jkk.name/software/coreference-analysis/</link>
      <pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/coreference-analysis/</guid>
      
        <description>A tool for classifying errors in coreference resolution. <a href="https://github.com/jkkummerfeld/berkeley-coreference-analyser" target="_blank">https://github.com/jkkummerfeld/berkeley-coreference-analyser</a></description>
      

    </item>
    
    <item>
      
      <title>
        "An Empirical Examination of Challenges in Chinese Parsing",
        
          Jonathan K. Kummerfeld, Daniel Tse, James R. Curran, Dan Klein
        
        (ACL (short),
        2013)
      </title>
      
      <link>http://www.jkk.name/publication/acl13analysis/</link>
      <pubDate>Thu, 01 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl13analysis/</guid>
      
        <description>
          Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "High-velocity Clouds in the Galactic All Sky Survey. I. Catalog",
        
          Vanessa A. Moss, Naomi M. McClure-Griffiths, Tara Murphy, D. J. Pisano, Jonathan K. Kummerfeld, James R. Curran
        
        (The Astrophysical Journal Supplement Series,
        2013)
      </title>
      
      <link>http://www.jkk.name/publication/astro13clouds/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/astro13clouds/</guid>
      
        <description>
          We present a catalogue of high-velocity clouds (HVCs) from the Galactic All Sky Survey (GASS) of southern-sky neutral hydrogen, which has 57 mK sensitivity and 1 km/s velocity resolution and was obtained with the Parkes Telescope. Our catalogue has been derived from the stray-radiation corrected second release of GASS. We describe the data and our method of identifying HVCs and analyse the overall properties of the GASS population. We catalogue a total of 1693 HVCs at declinations &lt; 0 deg, including 1111 positive velocity HVCs and 582 negative velocity HVCs. Our catalogue also includes 295 anomalous velocity clouds (AVCs). The cloud line-widths of our HVC population have a median FWHM of ~19 km/s, which is lower than found in previous surveys. The completeness of our catalogue is above 95% based on comparison with the HIPASS catalogue of HVCs, upon which we improve with an order of magnitude in spectral resolution. We find 758 new HVCs and AVCs with no HIPASS counterpart. The GASS catalogue will shed an unprecedented light on the distribution and kinematic structure of southern-sky HVCs, as well as delve further into the cloud populations that make up the anomalous velocity gas of the Milky Way.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output",
        
          Jonathan K. Kummerfeld, David Hall, James R. Curran, Dan Klein
        
        (EMNLP,
        2012)
      </title>
      
      <link>http://www.jkk.name/publication/emnlp12analysis/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/emnlp12analysis/</guid>
      
        <description>
          Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors.  We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.
        </description>
      

    </item>
    
    <item>
      
        <title>CCG to PST</title>
      
      <link>http://www.jkk.name/software/ccg2pst/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/ccg2pst/</guid>
      
        <description>A tool for converting CCG derivations into PTB-style phrase structure trees. <a href="https://github.com/jkkummerfeld/berkeley-ccg2pst" target="_blank">https://github.com/jkkummerfeld/berkeley-ccg2pst</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Robust Conversion of CCG Derivations to Phrase Structure Trees",
        
          Jonathan K. Kummerfeld, Dan Klein, James R. Curran
        
        (ACL (short),
        2012)
      </title>
      
      <link>http://www.jkk.name/publication/acl12conversion/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl12conversion/</guid>
      
        <description>
          We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.
        </description>
      

    </item>
    
    <item>
      
        <title>Parse Error Analysis</title>
      
      <link>http://www.jkk.name/software/parsing-analysis/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/parsing-analysis/</guid>
      
        <description>A tool for classifying mistakes in the output of parsers. <a href="https://github.com/jkkummerfeld/berkeley-parser-analyser" target="_blank">https://github.com/jkkummerfeld/berkeley-parser-analyser</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Mention Detection: Heuristics for the OntoNotes annotations",
        
          Jonathan K. Kummerfeld, Mohit Bansal, David Burkett, Dan Klein
        
        (CoNLL Shared Task,
        2011)
      </title>
      
      <link>http://www.jkk.name/publication/conll12coreference/</link>
      <pubDate>Wed, 01 Jun 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/conll12coreference/</guid>
      
        <description>
          Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Spatiotemporal Hierarchy of Relaxation Events, Dynamical Heterogeneities, and Structural Reorganization in a Supercooled Liquid",
        
          Raphael Candelier, Asaph Widmer-Cooper, Jonathan K. Kummerfeld, Olivier Dauchot, Giulio Biroli, Peter Harrowell, David R. Reichman
        
        (Physical Review Letters,
        2010)
      </title>
      
      <link>http://www.jkk.name/publication/prl10chemistry/</link>
      <pubDate>Wed, 01 Sep 2010 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/prl10chemistry/</guid>
      
        <description>
          We identify the pattern of microscopic dynamical relaxation for a two-dimensional glass-forming liquid. On short time scales, bursts of irreversible particle motion, called cage jumps, aggregate into clusters. On larger time scales, clusters aggregate both spatially and temporally into avalanches. This propagation of mobility takes place along the soft regions of the systems, which have been identified by computing isoconfigurational Debye-Waller maps. Our results characterize the way in which dynamical heterogeneity evolves in moderately supercooled liquids and reveal that it is astonishingly similar to the one found for dense glassy granular media.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Morphological Analysis Can Improve a CCG Parser for English",
        
          Matthew Honnibal, Jonathan K. Kummerfeld, James R. Curran
        
        (CoLing,
        2010)
      </title>
      
      <link>http://www.jkk.name/publication/coling10morph/</link>
      <pubDate>Sun, 01 Aug 2010 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/coling10morph/</guid>
      
        <description>
          Because English is a low morphology language, current statistical parsers tend to ignore morphology and accept some level of redundancy. This paper investigates how costly such redundancy is for a lexicalised grammar such as CCG.

We use morphological analysis to split verb inflectional suffixes into separate tokens, so that they can receive their own lexical categories. We find that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can benefit from morphological analysis, so long as the analysis is correct.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Faster Parsing by Supertagger Adaptation",
        
          Jonathan K. Kummerfeld, Jessika Roesner, Tim Dawborn, James Haggerty, James R. Curran, Stephen Clark
        
        (ACL,
        2010)
      </title>
      
      <link>http://www.jkk.name/publication/acl10adapt/</link>
      <pubDate>Thu, 01 Jul 2010 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl10adapt/</guid>
      
        <description>
          We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.
        </description>
      

    </item>
    
    <item>
      
        <title>Adaptive CCG Supertagging Model</title>
      
      <link>http://www.jkk.name/data/ccg-model/</link>
      <pubDate>Thu, 01 Jul 2010 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/ccg-model/</guid>
      
        <description>A model for the C&amp;C supertagger that gives the same results with smaller beam sizes, enabling faster parsing. TODO</description>
      

    </item>
    
    <item>
      
      <title>
        "Faster parsing and supertagging model estimation",
        
          Jonathan K. Kummerfeld, Jessika Roesner, James R. Curran
        
        (ALTA,
        2009)
      </title>
      
      <link>http://www.jkk.name/publication/alta09tagging/</link>
      <pubDate>Tue, 01 Dec 2009 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/alta09tagging/</guid>
      
        <description>
          Parsers are often the bottleneck for data acquisition, processing text too slowly to be widely applied. One way to improve the efficiency of parsers is to construct more confident statistical models. More training data would enable the use of more sophisticated features and also provide more evidence for current features, but gold standard annotated data is limited and expensive to produce.

We demonstrate faster methods for training a supertagger using hundreds of millions of automatically annotated words, constructing statistical models that further constrain the number of derivations the parser must consider. By introducing new features and using an automatically annotated corpus we are able to double parsing speed on Wikipedia and the Wall Street Journal, and gain accuracy slightly when parsing Section 00 of the Wall Street Journal.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Adaptive Supertagging for Faster Parsing",
        
          Jonathan K. Kummerfeld
        
        (The University of Sydney,
        2009)
      </title>
      
      <link>http://www.jkk.name/publication/thesis09adapt/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/thesis09adapt/</guid>
      
        <description>
          Statistical parsers are crucial for tackling the grand challenges of Natural Language Processing. The most effective approaches to these tasks are data driven, but parsers are too slow to be effectively used on large data sets. State-of-the-art parsers generally cannot process more than one sentence a second, and the fastest cannot process more than fifty sentences a second. The situation is even worse when they are applied outside of the domain of their training data. The fastest systems have two components, a parser, which has time complexity O(n3) and a supertagger, which has linear time complexity. By shifting work from the parser to the supertagger we dramatically improve speed.

This work demonstrates several major novel ideas that improve parsing efficiency. The core idea is that the tags chosen by the parser are gold standard data for its supertagger. This leads to the second surprising conceptual development, that decreasing tagging accuracy can improve parsing performance. To demonstrate these ideas required extensive development of the C&amp;C supertagger, including imple- mentation of more efficient estimation algorithms and parallelisation of the training process. This was particularly challenging as the C&amp;C supertagger is a state-of-the-art high performance system designed with a focus on speed rather than flexibility.

I was able to significantly improve performance on the standard evaluation corpus by using the parser to generate extremely large new resources for supertagger training. I have also shown that these methods provide significant benefits on another domain, Wikipedia text, without the cost of generating human annotated data sets. These parsing performance gains occur while supertagging accuracy decreases.

Despite extensive use of supertaggers to improve parsing efficiency there has been no comprehensive study of the interaction between a supertagger and a parser. I present the first systematic exploration of the relationship, show the potential benefits of understanding it, and demonstrate a novel algorithm for optimising the parameters that define it.

I have constructed models that process newspaper text 86% faster than previously, and Wikipedia text 30% faster, without any loss in accuracy and without the aid of extra gold standard resources in either domain. This work will lead directly to improvements in a range of Natural Language Processing tasks by enabling the use of far more parsed data.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Large-Scale Syntactic Processing: Parsing the Web",
        
          Stephen Clark, Ann Copestake, James R. Curran, Yue Zhang, Aurelie Herbelot, James Haggerty, Byung-Gyu Ahn, Curt Van Wyk, Jessika Roesner, Jonathan Kummerfeld, Tim Dawborn
        
        (Johns Hopkins University,
        2009)
      </title>
      
      <link>http://www.jkk.name/publication/report09jhu/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/report09jhu/</guid>
      
        <description>
          Scalable syntactic processing will underpin the sophisticated language technology needed for next generation information access. Companies are already using nlp tools to create web-scale question answering and &#34;semantic search&#34; engines. Massive amounts of parsed web data will also allow the automatic creation of semantic knowledge resources on an unprecedented scale. The web is a challenging arena for syntactic parsing, because of its scale and variety of styles, genres, and domains.

The goals of our workshop were to scale and adapt an existing wide-coverage parser to Wikipedia text; improve the efficiency of the parser through various methods of chart pruning; use self-training to improve the efficiency and accuracy of the parser; use the parsed wiki data for an innovative form of bootstrapping to make the parser both more efficient and more accurate; and finally use the parsed web data for improved disambiguation of coordination structures, using a variety of syntactic and semantic knowledge sources.

The focus of the research was the C&amp;C parser (Clark and Curran, 2007c), a state-of-the-art statistical parser based on Combinatory Categorial Grammar (ccg). The parser has been evaluated on a number of standard test sets achieving state-of-the-art accuracies. It has also recently been adapted successfully to the biomedical domain (Rimell and Clark, 2009). The parser is surprisingly efficient, given its detailed output, processing tens of sentences per second. For web-scale text processing, we aimed to make the parser an order of magnitude faster still. The C&amp;C parser is one of only very few parsers currently available which has the potential to produce detailed, accurate analyses at the scale we were considering.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Classification of Verb Particle Constructions with the Google Web1T Corpus",
        
          Jonathan K. Kummerfeld, James R. Curran
        
        (ALTA,
        2008)
      </title>
      
      <link>http://www.jkk.name/publication/alta08vpc/</link>
      <pubDate>Mon, 01 Dec 2008 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/alta08vpc/</guid>
      
        <description>
          Manually maintaining comprehensive databases of multi-word expressions, for example Verb-Particle Constructions (VPCs), is infeasible. We describe a new classifier for potential VPCs, which uses information in the Google Web1T corpus to perform a simple linguistic constituency test. Specifically, we consider the fronting test, comparing the frequencies of the two possible orderings of the given verb and particle. Using only a small set of queries for each verb-particle pair, the system was able to achieve an F-score of 78.4% in our evaluation while processing thousands of queries a second.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "The densest packing of AB binary hard-sphere homogeneous compounds across all size ratios",
        
          Jonathan K Kummerfeld, Toby S Hudson, Peter Harrowell
        
        (The Journal of Physical Chemistry B,
        2008)
      </title>
      
      <link>http://www.jkk.name/publication/chem08packing/</link>
      <pubDate>Fri, 01 Aug 2008 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/chem08packing/</guid>
      
        <description>
          This paper considers the homogeneous packing of binary hard spheres in an equimolar stoichiometry, and postulates the densest packing at each sphere size ratio. Monte Carlo simulated annealing optimizations are seeded with all known atomic inorganic crystal structures, and the search is performed within the degrees of freedom associated with each homogeneous AB structure type. Structures isopointal to the FeB structure type are found to have the highest packing fraction at all sphere size ratios. The optimized structures match or improve on the best previously demonstrated packings of this type, and show that compound structures can pack more densely than segregated close-packed structures at all radius ratios less than 0.62.
        </description>
      

    </item>
    
    <item>
      
        <title>Publications</title>
      
      <link>http://www.jkk.name/home/publications/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 -0500</pubDate>
      
      <guid>http://www.jkk.name/home/publications/</guid>
      
        <description></description>
      

    </item>
    
    <item>
      
        <title>Contact</title>
      
      <link>http://www.jkk.name/home/contact/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/contact/</guid>
      
        <description></description>
      

    </item>
    
    <item>
      
        <title>Recent Posts</title>
      
      <link>http://www.jkk.name/home/posts/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/posts/</guid>
      
        <description></description>
      

    </item>
    
    <item>
      
        <title>Data</title>
      
      <link>http://www.jkk.name/home/data/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/data/</guid>
      
        <description></description>
      

    </item>
    
    <item>
      
        <title>Software</title>
      
      <link>http://www.jkk.name/home/software/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/software/</guid>
      
        <description></description>
      

    </item>
    
  </channel>
</rss>
