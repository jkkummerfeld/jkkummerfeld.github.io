<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/</link>
      <atom:link href="https://www.jkk.name/index.xml" rel="self" type="application/rss+xml" />
    <description>Jonathan K. Kummerfeld</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Tue, 01 Jun 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>Jonathan K. Kummerfeld</title>
      <link>https://www.jkk.name/</link>
    </image>
    
    <item>
      <title>Exploring Self-Identified Counseling Expertise in Online Support Forums</title>
      <link>https://www.jkk.name/publication/acl21counseling/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl21counseling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing</title>
      <link>https://www.jkk.name/publication/acl21fair-work/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl21fair-work/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Chord Embeddings: Analyzing What They Capture and Their Role for Next Chord Prediction and Artist Attribute Prediction</title>
      <link>https://www.jkk.name/publication/evomusart21/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/evomusart21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overview of the Eighth Dialog System Technology Challenge: DSTC8</title>
      <link>https://www.jkk.name/publication/dstc8-ieee/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/dstc8-ieee/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring the Value of Personalized Word Embeddings</title>
      <link>https://www.jkk.name/publication/coling20personal/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/coling20personal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inconsistencies in Crowdsourced Slot-Filling Annotations: A Typology and Identification Methods</title>
      <link>https://www.jkk.name/publication/coling20svp/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/coling20svp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels</title>
      <link>https://www.jkk.name/publication/emnlp-findings20srl/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp-findings20srl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Compositional Demographic Word Embeddings</title>
      <link>https://www.jkk.name/publication/emnlp20demographics/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp20demographics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Improving Low Compute Language Modeling with In-Domain Embedding Initialisation</title>
      <link>https://www.jkk.name/publication/emnlp20lm/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp20lm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness</title>
      <link>https://www.jkk.name/publication/emnlp20taboo/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp20taboo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-12_taboo/</link>
      <pubDate>Mon, 12 Oct 2020 10:28:13 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-12_taboo/</guid>
      <description>&lt;p&gt;When we crowdsource data for tasks like SRL and sentiment analysis we only care about accuracy.
For tasks where workers write new content, such as paraphrasing and creating questions, we also care about data diversity.
If our data is not diverse then models trained on it will not be robust in the real world.
The core idea of this paper is to encourage creativity by constraining workers.&lt;/p&gt;
&lt;p&gt;We use three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Collect some data.&lt;/li&gt;
&lt;li&gt;Create a taboo list of words / phrases based on the data collected.&lt;/li&gt;
&lt;li&gt;Return to step 1, but tell workers they can&amp;rsquo;t use things in the taboo list.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We explored this idea for task-oriented dialogue.
We identified taboo words using an SVM with a bag-of-words to identify common words associated with specific intents or slot values.
Depending on the taboo word, we got quite different paraphrases.
For example, for the sentence &amp;ldquo;What is the capital of Florida?&amp;rdquo; we collected paraphrases with various taboo words:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Taboo Word&lt;/th&gt;
&lt;th&gt;Paraphrases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;what city is the state capital of florida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;florida&lt;/td&gt;
&lt;td&gt;what is the capital of the sunshine state&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capital&lt;/td&gt;
&lt;td&gt;where is the seat of government in florida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;what&lt;/td&gt;
&lt;td&gt;tell me the name of florida&amp;rsquo;s capital&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These examples show interesting variations, but to see if the variations are significant we tried collecting new test sets for five intent classification datasets and four slot filling datasets.
With just two taboo words, a BERT based model trained on the original dataset did considerably worse on our new data.
The drop varied from 2 to 33 points, with a median of 9.
This indicates that we are capturing ways of expressing these intents that are not well covered by the original data.&lt;/p&gt;
&lt;p&gt;Addressing this issue is simple - train on data collected with our method!
Interestingly, this approach is complementary to the outlier-based approach from 
&lt;a href=&#34;https://www.jkk.name/publication/naacl19outliers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;our NAACL 2019 paper&lt;/a&gt;.
Examples collected using one approach are hard for models trained on data from the other.
Fortunately, training with data from a mix of the two leads to strong results on both.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m particularly excited about this work because the general idea could be applied in so many ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change the task.&lt;/li&gt;
&lt;li&gt;Vary the type of taboo item (e.g. phrases instead of words).&lt;/li&gt;
&lt;li&gt;Try other ways of selecting taboo items.&lt;/li&gt;
&lt;li&gt;Use a different mapping from taboo items to tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, more specific versions of this idea have already been used.

&lt;a href=&#34;https://dl.acm.org/doi/10.1145/985692.985733&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luis von Ahn&amp;rsquo;s ESP game&lt;/a&gt; for image captioning used a taboo list.
Each image had a list of complete labels previously assigned to the image.
New labels could not match an existing label.
That work predates this paper by 16 years, but hasn&amp;rsquo;t had much traction in the NLP community, possibly because of the limitation of requiring complete matches on labels (making it impractical for sentences or even phrases).
I&amp;rsquo;m hopeful that our more general version will be useful in a range of crowdsourcing efforts.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp20taboo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp20taboo,
  title     = {Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness},
  author    = {Larson, Stefan and Zheng, Anthony and Mahendran, Anish and Tekriwal, Rishi and Cheung, Adrian and Guldan, Eric and Leach, Kevin and Kummerfeld, Jonathan K.},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://www.jkk.name/pub/emnlp20taboo.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Compositional Demographic Word Embeddings (Welch et al., EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-10_demographicembeddings/</link>
      <pubDate>Sat, 10 Oct 2020 20:25:11 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-10_demographicembeddings/</guid>
      <description>&lt;p&gt;Most work in NLP uses datasets with a diverse set of speakers.
In practise, everyone speaks / writes slightly differently and our models would be better if they accounted for that.
This has been the motivation for a line of work by 
&lt;a href=&#34;http://cfwelch.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Charlie Welch&lt;/a&gt; that I&amp;rsquo;ve been a collaborator on (in

&lt;a href=&#34;https://www.jkk.name/publication/cicling19personal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CICLing 2019&lt;/a&gt;,

&lt;a href=&#34;https://www.jkk.name/publication/ieee19personal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Intelligent Systems 2019&lt;/a&gt;,

&lt;a href=&#34;https://www.jkk.name/publication/coling20personal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoLing 2020&lt;/a&gt;,
and this paper).&lt;/p&gt;
&lt;p&gt;Here, the question is how to improve language modeling for a new user of a service who voluntarily provided some demographic information, but you have no other data for.
Our solution is a language model that (1) has a separate word embedding space for each individual demographic value, and (2) forms a word embedding for a given user by composing the embeddings for their demographics.
In experiments on Reddit, this leads to improvements in performance for all demographic groups.&lt;/p&gt;
&lt;p&gt;In the process, we also developed a way to extract demographics of Reddit users.
Prior work has either inferred demographics or looked at flairs (labels in user profiles).
We use self-reported information in posts, such as &amp;ldquo;I am a [blah]&amp;rdquo;.
We use simple regular expressions, which are enough to get two or more demographic values for 61,000 users.
There is also relatively little overlap with a flair based method (less than 0.5% of ours are in a set based on flairs).&lt;/p&gt;
&lt;p&gt;It is important to note that a range of ethical issues exist around the use of demographics in machine learning.
We discuss a range of issues in the paper, but I also wanted to mention a few here.
First, to collect our data, we identified self-reported demographics in Reddit text.
This avoids some of the problems with inferring demographics, but it does mean our sample is biased (it only contains people who wish to publicly share demographics online).
Second, we must consider how our work may be used.
There is a potential positive (improved performance for specific groups), but also the risk that in order to use our ideas developers require users to disclose information or try to infer it automatically.
Third, there is the risk that our work is interpreted as implying that how someone speaks is a consequence of their demographics.
For more detailed discussion of these and other issues see the &amp;ldquo;Limitations and Ethical Considerations&amp;rdquo; section of the paper.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2010.02986.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp20demographics,
  title     = {Compositional Demographic Word Embeddings},
  author    = {Welch, Charles and Kummerfeld, Jonathan K. and P{\&#39;e}rez-Rosas, Ver{\&#39;o}nica and Mihalcea, Rada},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://arxiv.org/pdf/2010.02986.pdf},
  abstract  = {Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-04_crowdsrl/</link>
      <pubDate>Sun, 04 Oct 2020 15:10:12 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-04_crowdsrl/</guid>
      <description>&lt;p&gt;My 
&lt;a href=&#34;https://www.jkk.name/post/2020-09-25_crowdqasrl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions.
This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.&lt;/p&gt;
&lt;p&gt;The core new idea is a filtering process in which workers identify &lt;em&gt;incorrect&lt;/em&gt; answers for a task.
This is the first step of a three stage process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Five workers iteratively filter the options for a label (either for a predicate or argument) until there are only three.&lt;/li&gt;
&lt;li&gt;Five workers select the correct answer.&lt;/li&gt;
&lt;li&gt;If the workers disagree or any of them indicates uncertainty, ask an expert to annotate the example.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To make this work, we use an automatic system for identifying spans for predicates and arguments.
This is better than going straight to the second step because it makes the set of labels less overwhelming, focusing effort on the subtle distinctions between the options.&lt;/p&gt;
&lt;p&gt;This mixture of crowd and expert effort achieves high accuracy (94%) while only having 12% of examples annotated by experts.
The cost is about 52 cents per label for crowd work plus the cost of the expert.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a little tricky to compare the cost with prior work.
Comparing to other work on SRL, we spend more on the crowd, but less on experts.
Whether that trade-off is worth it will depend on the cost of experts.
In practise, our experts are often members of the research team and so their time is a stronger constraint than the crowdsourcing budget.
In that case, our approach comes out ahead as we can get more data annotated per unit of expert effort (by a factor of four).
The QA-SRL work is quite a bit cheaper, at 54 cents per predicate with 2.9 roles on average (which would be ~$2 + expert effort for our approach), but the type of annotations collected are quite different, with ours providing labels from the sense inventory in Propbank.&lt;/p&gt;
&lt;p&gt;I see a range of interesting potential improvements for future work.
First, bringing in methods of worker training in order to improve their accuracy and so reduce the need for duplicate effort.
Second, combining with ideas from other work, such as having a model deciding whether examples are easy or hard and changing how they are processed accordingly, or using QA-SRL annotation to inform the process.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp-findings20srl.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1314632048070594560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My Tweet&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp-findings20srl,
  title     = {A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels},
  author    = {Jiang, Youxuan and Zhu, Huaiyu and Kummerfeld, Jonathan K. and Li, Yunyao and Lasecki, Walter},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  shortvenue = {Findings of EMNLP},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://www.jkk.name/pub/emnlp-findings20srl.pdf},
  abstract  = {Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95\% accuracy for predicate labels and 93\% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56\% to 14\% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Qualification Labour: A Fair Wage Isn&#39;t Enough if Workers Need to Do 5,000 Low Paid Tasks to Qualify for Your Task</title>
      <link>https://www.jkk.name/publication/hcomp20fair/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/hcomp20fair/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-09-29_pretraininglm/</link>
      <pubDate>Tue, 29 Sep 2020 13:38:24 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-29_pretraininglm/</guid>
      <description>&lt;p&gt;This paper explores two questions.
First, what is the impact of a few key design decisions for word embeddings in language models?
Second, based on the first answer, how can we improve results in the situation where we have 10 million+ words of text, but only 1 GPU for training?&lt;/p&gt;
&lt;h2 id=&#34;the-impact-of-tying-freezing-and-pretraining&#34;&gt;The impact of tying, freezing, and pretraining&lt;/h2&gt;
&lt;p&gt;It is standard practise to tie the input and output embeddings of language models (i.e., use the same weights in both places), training them together and initialising them randomly.
Several papers have shown that this improves results by providing more frequent updates to the input embeddings.
But if you have data available for pretraining it is less clear that this is the right approach.
To explore this I&amp;rsquo;m going to use a few symbols:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/pretraining-lm-vary-key.jpg&#34; alt=&#34;Key for Variations on LM table&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here are the results of training an 
&lt;a href=&#34;https://github.com/salesforce/awd-lstm-lm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWD-LSTM&lt;/a&gt; with all variations of these parameters, evaluated on the standard LM development set of the PTB (Std) and a variation that has actual words instead of unk (Rare):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/pretraining-lm-vary.jpg&#34; alt=&#34;Variations on LM (for written form of table see bottom of page)&#34;&gt;&lt;/p&gt;
&lt;p&gt;Light blue shows the standard configuration and light red shows our proposal.
The table is ranked by performance on Std and has four clear sections:&lt;/p&gt;
&lt;p&gt;(a) Frozen random output embeddings.&lt;/p&gt;
&lt;p&gt;(b) Frozen pretrained output embeddings.&lt;/p&gt;
&lt;p&gt;(c) Frozen random input embeddings.&lt;/p&gt;
&lt;p&gt;(d) Various configurations.&lt;/p&gt;
&lt;p&gt;I was surprised by the dramatic difference between input and output embeddings here.
Freezing the output embeddings, even with a good embedding space, leads to terrible performance.
In contrast, freezing input embeddings is fine if they are pretrained, and has a far smaller impact when they are random.&lt;/p&gt;
&lt;p&gt;Evaluating with rare words, the big picture is mostly the same, but pretraining has a bigger impact.
One interesting difference is that the top five models all use pretrained input embeddings, with a large gap from there to the next results.
At the same time, pretraining the output embeddings seems to have only a small impact (when holding all other variables fixed).
Finally, the best results freeze the input embeddings.
Our explanation is that embeddings become inconsistent when they aren&amp;rsquo;t frozen.
The vectors for words in the training set are moved but the ones seen only in pretraining stay where they are, leading to an inconsistent embedding space.&lt;/p&gt;
&lt;p&gt;The paper then goes through a series of experiments to explore this, varying data domain, similarity of pretraining data, and more.
Here I&amp;rsquo;m going to jump straight to the final results.
The table below considers a dataset with 43 million in-domain tokens for pretraining and 7 million for LM training.
The other models are the standard AWD-LSTM, an n-gram language model, and two version of GPT-2 (without finetuning):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/pretraining-final.jpg&#34; alt=&#34;Final results (for written form of table see bottom of page)&#34;&gt;&lt;/p&gt;
&lt;p&gt;For word level prediction perplexity is reduced by 4.
However, if we train and test with BPE there is no improvement (see the 
&lt;a href=&#34;https://arxiv.org/abs/1911.11423&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SHA-RNN paper&lt;/a&gt; for some issues with comparing BPE and word evaluation).
So if your application works with BPE this finding isn&amp;rsquo;t useful, but for word-level modeling it probably is.&lt;/p&gt;
&lt;p&gt;A few notes about this work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A natural next step would be to explore ways to train the language model with more data.
Modifying the AWD-LSTM code to support training sets larger than GPU memory could render pretraining unnecessary (though at the cost of much longer training).
In some experiments (not in the paper), we found that when the pretraining set and training set were the same, pretraining didn&amp;rsquo;t improve performance, but it did speed up training.&lt;/li&gt;
&lt;li&gt;Properties of evaluation datasets have shaped the direction of work on language modeling.
It&amp;rsquo;s important to think beyond the hyperparameters that are easy to vary (e.g., hidden vector dimensions) when adapting a model for a new scenario.&lt;/li&gt;
&lt;li&gt;Writing robust research code is hard.
We tried getting several other models to run with our variations, but going beyond reproducing results to actually modifying code proved hard.
Even for the AWD-LSTM, we failed to reproduce results except when we went back to one of the earliest releases.&lt;/li&gt;
&lt;li&gt;This paper was saved by author response.
The initial reviews were 3.5, 2.5, 3.5 and based on the response and reviewer discussion the 2.5 went to a 4.
The response contained answers to reviewer questions, including a bunch of statistics about the data that are now in the final paper.
I have always been a fan of author response.
It can lead to more informed acceptance decisions and more useful feedback to authors.
To achieve that, both authors and reviewers need to engage with it though.
In particular, reviewers need to give something of substance to be responded to and they need to carefully read and consider the response.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp20lm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp20lm,
  title     = {Improving Low Compute Language Modeling with In-Domain Embedding Initialisation},
  author    = {Welch, Charles and Mihalcea, Rada and Kummerfeld, Jonathan K.},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2020},
  url       = {https://www.jkk.name/pub/emnlp20lm.pdf},
  abstract  = {Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;tables-in-written-form&#34;&gt;Tables in written form&lt;/h2&gt;
&lt;h3 id=&#34;table-with-training-variations&#34;&gt;Table with training variations&lt;/h3&gt;
&lt;p&gt;Each section is presented separately below, with the model described using five words followed by the result on the standard data and the result on the data with rare words.&lt;/p&gt;
&lt;p&gt;First section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tied   frozen   dice  frozen   dice, 680, 1120&lt;/li&gt;
&lt;li&gt;untied frozen   dice  frozen   dice, 680, 1120&lt;/li&gt;
&lt;li&gt;untied unfrozen dice  frozen   dice, 680, 431&lt;/li&gt;
&lt;li&gt;untied unfrozen train frozen   dice, 220, 372&lt;/li&gt;
&lt;li&gt;untied frozen   train frozen   dice, 218, 360&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Second section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;untied frozen   dice  frozen   train, 121, 202&lt;/li&gt;
&lt;li&gt;untied unfrozen dice  frozen   train, 95.0, 170&lt;/li&gt;
&lt;li&gt;untied unfrozen train frozen   train, 91.3, 147&lt;/li&gt;
&lt;li&gt;tied   frozen   train frozen   train, 90.7, 136&lt;/li&gt;
&lt;li&gt;untied frozen   train frozen   train, 90.7, 136&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Third section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;untied frozen   dice  unfrozen dice, 82.2, 143&lt;/li&gt;
&lt;li&gt;untied frozen   dice  unfrozen train, 81.4, 142&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fourth section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;untied unfrozen dice  unfrozen dice, 65.3, 120&lt;/li&gt;
&lt;li&gt;untied unfrozen dice  unfrozen train, 64.1, 113&lt;/li&gt;
&lt;li&gt;untied unfrozen train unfrozen dice, 62.5, 105&lt;/li&gt;
&lt;li&gt;untied unfrozen train unfrozen train, 61.7, 98.5&lt;/li&gt;
&lt;li&gt;untied frozen   train unfrozen train, 61.6, 97.1&lt;/li&gt;
&lt;li&gt;tied   unfrozen dice  unfrozen dice, 61.3, 112&lt;/li&gt;
&lt;li&gt;untied frozen   train unfrozen dice, 61.1, 98.1&lt;/li&gt;
&lt;li&gt;tied   unfrozen train unfrozen train, 59.8, 98.7&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;final-results-table&#34;&gt;Final results table&lt;/h3&gt;
&lt;p&gt;Models with word level evaluation, giving development results then test results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N-Gram, 92.3, 95.0&lt;/li&gt;
&lt;li&gt;Baseline AWD-LSTM, 52.8, 53.5&lt;/li&gt;
&lt;li&gt;Our approach, 49.0, 49.4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Models with BPE evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N-Gram, 56.7, 55.3&lt;/li&gt;
&lt;li&gt;GPT-2 (112m), 46.4, 43.8&lt;/li&gt;
&lt;li&gt;Baseline AWD-LSTM, 37.8, 36.7&lt;/li&gt;
&lt;li&gt;Our approach, 38.3, 37.2&lt;/li&gt;
&lt;li&gt;GPT-2 (774m), 32.5, 33.7&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Dice Icon by Andrew Doane from the Noun Project.
Fire and Snowflake Icons by Freepik from &lt;a href=&#34;http://www.flaticon.com&#34;&gt;www.flaticon.com&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)</title>
      <link>https://www.jkk.name/post/2020-09-25_crowdqasrl/</link>
      <pubDate>Fri, 25 Sep 2020 10:17:18 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-25_crowdqasrl/</guid>
      <description>&lt;p&gt;Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments.
Over the last few years, 
&lt;a href=&#34;https://www.cs.washington.edu/people/faculty/lsz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luke Zettlemoyer&amp;rsquo;s Group&lt;/a&gt; has been exploring using question-answer pairs to represent this structure.
This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank.
However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.&lt;/p&gt;
&lt;p&gt;I got three main things out of this paper:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It shifted my approach to crowdsourcing to consider workers more like traditional expert annotators.&lt;/li&gt;
&lt;li&gt;It reinforced the idea that small shifts in crowd workflows can have a major impact on annotation quality.&lt;/li&gt;
&lt;li&gt;QA-SRL can capture roles not covered by PropBank.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The work also provides a new dataset that will be useful for future work on this problem, and useful benchmarks of systems and measurements of data quality.
Expanding on the three points above:&lt;/p&gt;
&lt;p&gt;Crowd workers: The paper argues in favour of putting more time into training workers.
Most of the work I&amp;rsquo;ve seen in NLP for crowdsourcing (including my own) focuses on modifying task design or using ML post-processing to improve results.
Here, they run a large-scale qualification task and filter workers based on their performance, then train those workers by paying them to read a set of instructions (23 text-dense slides) and do two small annotation rounds with feedback after each one.
This increases the upfront cost, but reduces the cost of annotation by reducing the need for multiple annotations of each item.
The paper doesn&amp;rsquo;t provide quite enough detail to quantify the cost.
We do know that to get to 11 workers they needed to train 30 workers at a cost of 2 hours each plus 30 minutes of researcher time each.
If we assume 60 workers did the preliminary round, each taking 5 minutes, and that workers cost &lt;span&gt;$&lt;/span&gt;12 / hour (&lt;span&gt;$&lt;/span&gt;10 to the workers, &lt;span&gt;$&lt;/span&gt;2 to Amazon), that&amp;rsquo;s almost &lt;span&gt;$&lt;/span&gt;800 plus 15 hours of researcher time.
For a large annotation effort, the savings during annotation will make that worth it (or, as in this case, it will lead to higher quality data).
I am curious which aspect was more important though - filtering the pool of workers, or training workers.&lt;/p&gt;
&lt;p&gt;Workflow impact: In previous QA-SRL work, one worker wrote a question and its answers and two workers checked the question and independently added answers.
Here, two workers independently write a question+answer and a third work consolidates the annotations into a final annotation.
The cost for a label is about the same (54c / predicate vs. 51c / predicate), but coverage is considerably higher.
The design space for crowd workflows is huge and this is another example of how important it is to explore.
It&amp;rsquo;s also possible that the changes in recruitment and training were more critical than the workflow shift, but the study didn&amp;rsquo;t include evaluation with only one or the other.&lt;/p&gt;
&lt;p&gt;QA-SRL vs. PropBank: This may be less surprising to someone who works more on SRL, but they found their approach captured many implicit roles that PropBank does not.
Specifically, of 100 annotated arguments that were not in PropBank, 68 were valid implicit arguments.
I&amp;rsquo;m curious about what those implicit arguments are capturing.
Maybe targeted re-annotation could be used to add them to PropBank (identifying relevant sentences by trace parsing).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.626/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/plroit/qasrl-gs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1309592830537543681?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My Tweet&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{roit-etal-2020-controlled,
    title = &amp;quot;Controlled Crowdsourcing for High-Quality {QA}-{SRL} Annotation&amp;quot;,
    author = &amp;quot;Roit, Paul  and
      Klein, Ayal  and
      Stepanov, Daniela  and
      Mamou, Jonathan  and
      Michael, Julian  and
      Stanovsky, Gabriel  and
      Zettlemoyer, Luke  and
      Dagan, Ido&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.626&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.626&amp;quot;,
    pages = &amp;quot;7008--7013&amp;quot;,
    abstract = &amp;quot;Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Practical Obstacles to Deploying Active Learning (Lowell, et al., EMNLP 2019)</title>
      <link>https://www.jkk.name/post/2020-09-17_activelearningbrittle/</link>
      <pubDate>Thu, 17 Sep 2020 15:08:35 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-17_activelearningbrittle/</guid>
      <description>&lt;p&gt;Training models requires massive amounts of labeled data.
We usually sample data iid from the target domain (e.g. newspapers), but it seems intuitive that this means we wast effort labeling samples that are obvious or easy and so not informative during training.
Active Learning follows that intuition, labeling data incrementally, selecting the next example(s) to label based on what a model considers uncertain.
Lots of work has shown this can be effective for that model, but if the labeled dataset is then used to train another model will it also do well?&lt;/p&gt;
&lt;p&gt;For text classification this paper finds the answer is no: training model X on iid samples is as good or better than training on samples collected while active learning with model Y.
They show this through experiments with four datasets and three models, training on up to 25% of the available data.
For named entity recognition the story is different in my opinion - iid is consistently slightly worse, though the gains from active learning are small in all cases (0 to 0.6 point gain for the better model, 0.4 to 1.7 for the weaker model).
One caveat is that these models are not state-of-the-art.
For CoNLL 2003 NER, many models score 
&lt;a href=&#34;https://nlpprogress.com/english/named_entity_recognition.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;around 93&lt;/a&gt;, but these models are getting 70-90.
On OntoNotes, the best results are close to 90, but these models get 74-85.
This is still an interesting result, but I&amp;rsquo;m left with a few questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This work focused on a low data scenario. What if I have a lot of data? It may be that sampling iid and active learning based samples were similar here because either way the data was capturing the core phenomena. The challenge here is that you can&amp;rsquo;t run this experiment easily with an existing dataset (unless it is truly massive).&lt;/li&gt;
&lt;li&gt;How does the sampled data differ from iid data? Is there a significant shift in the distribution of class types?&lt;/li&gt;
&lt;li&gt;What about using a hybrid approach, with some data sampled iid and other data sampled randomly?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, what I take away from this work is that active learning may not be the right choice for building a small dataset in NLP.
For large datasets, building models, or other tasks and domains the conclusions are less clear, though it is certainly worth being aware of the risk that a dataset made with active learning may not be equally useful to all models.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D19-1003.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Twitter discussion in 
&lt;a href=&#34;https://twitter.com/zacharylipton/status/1019222882482905088&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018&lt;/a&gt;) and 
&lt;a href=&#34;https://twitter.com/zacharylipton/status/1165692913290043398?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{lowell-etal-2019-practical,
    title = &amp;quot;Practical Obstacles to Deploying Active Learning&amp;quot;,
    author = &amp;quot;Lowell, David  and
      Lipton, Zachary C.  and
      Wallace, Byron C.&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)&amp;quot;,
    month = &amp;quot;nov&amp;quot;,
    year = &amp;quot;2019&amp;quot;,
    address = &amp;quot;Hong Kong, China&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/D19-1003&amp;quot;,
    doi = &amp;quot;10.18653/v1/D19-1003&amp;quot;,
    pages = &amp;quot;21--30&amp;quot;,
    abstract = &amp;quot;Active learning (AL) is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. In AL, one iteratively selects training examples for annotation, often those for which the current model is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice, one does not have the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)</title>
      <link>https://www.jkk.name/post/2020-09-07_chartdialogs/</link>
      <pubDate>Mon, 07 Sep 2020 14:41:34 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-07_chartdialogs/</guid>
      <description>&lt;p&gt;Natural language interfaces to computer systems are an exciting area with new workshops (
&lt;a href=&#34;https://aclanthology.org/volumes/2020.nli-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WNLI&lt;/a&gt; at ACL and 
&lt;a href=&#34;https://intex-sempar.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IntEx-SemPar&lt;/a&gt; at EMNLP), a range of datasets (including my own work on 
&lt;a href=&#34;https://www.jkk.name/publication/acl18sql/&#34;&gt;text-to-SQL&lt;/a&gt;), and many papers.
Most work focuses on either (1) commands for simple APIs, (2) generating a database query, or (3) generating general purpose code.
This paper considers an interesting application: interaction with data visualisation tools.&lt;/p&gt;
&lt;p&gt;Using the full flexibility of these tools is a tall order, so this work focuses on commands to modify style parameters of a figure.
For that setting, the problem can be framed as task-oriented dialogue in which each style parameter (e.g. x-axis font size) is a slot that needs to be defined.
Using this framing of the problem, the paper presents a new dataset of 3,200 conversations in which a person modifies the style of a plot.
These were collected on Mechanical Turk by having one worker describe a target plot and another worker manipulating values for parameters to match it.
There are 12 plot types with 3-13 properties, with the target plot randomly generated.
Baseline approaches do fairly well, but far short of a human (either another worker or one of the authors).&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a large resource with high agreement between annotators and the paper presents detailed analysis and helpful examples.
One experiment I&amp;rsquo;d be curious to see is results with a fixed number of training examples per plot type (or per slot type).
Histograms and scatter plots appear particularly difficult in the breakdown of results by plot type, but they are also the types with the fewest examples (a tenth as many as the type with the most).&lt;/p&gt;
&lt;p&gt;I find this general topic exciting because it brings together several areas of NLP and it seems feasible to create a useful system in the near future.
Hopefully there will be progress on models for this dataset and development of additional resources.
In particular, there was a decision here to limit generation to slot-values, which is powerful, but does not capture the full flexibility of matplotlib (at least not without further work on representing more features this way).
Arbitrary code generation would be a fantastic extension, though creating the data would require some creativity as the approach used here wouldn&amp;rsquo;t directly work.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.328.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{shao-nakashole-2020-chartdialogs,
    title = &amp;quot;{C}hart{D}ialogs: {P}lotting from {N}atural {L}anguage {I}nstructions&amp;quot;,
    author = &amp;quot;Shao, Yutong  and
      Nakashole, Ndapa&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.328&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.328&amp;quot;,
    pages = &amp;quot;3559--3574&amp;quot;,
    abstract = &amp;quot;This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61{\%} plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)</title>
      <link>https://www.jkk.name/post/2020-09-03_checklist/</link>
      <pubDate>Thu, 03 Sep 2020 14:44:29 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-03_checklist/</guid>
      <description>&lt;p&gt;It is difficult to predict how well a model will work in the real world.
Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to.
This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories.
Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.&lt;/p&gt;
&lt;p&gt;The paper organises these tests along two axes.
One is the type of test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invariance: Giving the same answer when changes are made that should not impact the model prediction.&lt;/li&gt;
&lt;li&gt;Directional: Giving an answer that differs in a way that matches the intended impact of a change.&lt;/li&gt;
&lt;li&gt;Minimum Function Tests: A range of other tests that consider specific cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other axis is the linguistic property being varied:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vocabulary Change&lt;/li&gt;
&lt;li&gt;Named Entity Variation&lt;/li&gt;
&lt;li&gt;Temporal Shift&lt;/li&gt;
&lt;li&gt;Negation&lt;/li&gt;
&lt;li&gt;Semantic Role Swap&lt;/li&gt;
&lt;li&gt;Various Other Changes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, an invariance test on vocabulary would be that replacing words with their synonyms should not change the result.&lt;/p&gt;
&lt;p&gt;The paper tests the idea on (1) sentiment analysis on SST-2, (2) identifying matching questions on QQP, and (3) machien comprehension on SQuAD.
Researchers / developers using the method are more effective at finding issues than those asked to write tests without this framework to approach the problem.&lt;/p&gt;
&lt;p&gt;Understanding system errors has been an interest of mine for a long time now (back to my 2012 parsing paper) and from my experience with startups it is definitely challenging to develop effective tests for NLP models.
I&amp;rsquo;m curious to see how this approach works out when used iteratively.
When users modify their model or data to address the problems do they actually fix them or just overfit to the new set of tests?
Another open question is how to apply these to problems with more structured output (e.g. text-to-SQL).
Some would easily apply, e.g. invariance tests, while others would be more difficult.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.442/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{ribeiro-etal-2020-beyond,
    title = &amp;quot;Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist&amp;quot;,
    author = &amp;quot;Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.442&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.442&amp;quot;,
    pages = &amp;quot;4902--4912&amp;quot;,
    abstract = &amp;quot;Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>DSTC 7 track 1: Next Utterance Selection</title>
      <link>https://www.jkk.name/data/dstc7/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/dstc7/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DSTC 8 track 2: Next Utterance Selection</title>
      <link>https://www.jkk.name/data/dstc8/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/dstc8/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Colaboratoy Notebook for Coreference Resolution with SpanBERT</title>
      <link>https://www.jkk.name/software/colab-spanbert/</link>
      <pubDate>Mon, 30 Mar 2020 16:00:26 -0500</pubDate>
      <guid>https://www.jkk.name/software/colab-spanbert/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Paper Reading</title>
      <link>https://www.jkk.name/post/paper-reading/</link>
      <pubDate>Sun, 26 Jan 2020 16:09:06 -0500</pubDate>
      <guid>https://www.jkk.name/post/paper-reading/</guid>
      <description>&lt;p&gt;Keeping up with research is hard.
I&amp;rsquo;ve previously made lists of papers I wanted to read, and then only gotten to a small fraction of them.
Simply resolving to read more papers hasn&amp;rsquo;t worked for me.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m trying out a new approach.
The goals are (1) read less of more papers, and (2) read more papers that are critical to my work.
Sometimes just the introduction or abstract is enough for me to get the ideas I need from the paper.
I want to read the whole paper only if it is really relevant to me.
The problem is that it&amp;rsquo;s easy to start reading a paper and then just keep going, and without a process it can be easy to put off starting at all.
This is the process I&amp;rsquo;ve worked out (in Chrome) to do this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go through the proceedings for a conference on the ACL anthology and read every title. Based on the title, decide whether to read the abstract. Based on the abstract, decide whether to read the introduction, in which case open the paper in a tab.&lt;/li&gt;
&lt;li&gt;Bookmark all tabs. Either use &lt;code&gt;Shift+Command+D&lt;/code&gt; or Bookmarks -&amp;gt; Bookmark All Tabs.&lt;/li&gt;
&lt;li&gt;Export the folder of bookmarks to a file. To do this, go to &lt;code&gt;chrome://bookmarks&lt;/code&gt;, select the new folder then use the menu on the far right of the blue bar to select Export Bookmarks.&lt;/li&gt;
&lt;li&gt;Run the code below, with &lt;code&gt;bookmarks_DATE.html&lt;/code&gt; as input (note, requires &lt;code&gt;PyPDF2&lt;/code&gt;). This produces a pdf with only the introduction of each paper (approximately).&lt;/li&gt;
&lt;li&gt;Read through the pdf this produces and flag the papers to read all of.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This will hopefully produce a list that is short enough to read all of (and maybe write blog posts about!).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Get the paper URLs
import sys
papers = {}
for line in sys.stdin:
    if &#39;aclanthology.org&#39; in line:
        content = line.strip()
        url = content.split()[1].split(&#39;&amp;quot;&#39;)[1][:-1] + &amp;quot;.pdf&amp;quot;
        name = content.split(&amp;quot; - ACL Anthology&amp;quot;)[0].split(&amp;quot;&amp;gt;&amp;quot;)[-1]
        papers[name] = url

# Download the papers
import io, requests
PDFs = {}
for name, url in papers.items():
    r = requests.get(url, auth=(&#39;usrname&#39;, &#39;password&#39;), verify=False,stream=True)
    assert 200 &amp;lt;= r.status_code &amp;lt; 400
    r.raw.decode_content = True
    PDFs[name] = io.BytesIO(r.content)

# Get the Introductions
from PyPDF2 import PdfFileReader, PdfFileWriter
import string
pdf_writer = PdfFileWriter()
for name, raw_pdf in PDFs.items():
    pdf = PdfFileReader(raw_pdf)
    page0 = pdf.getPage(0)
    pdf_writer.addPage(page0)
    text = page0.extractText().split(&#39;\n&#39;)
    done = False
    for part in text:
        # Try to find the start of section 2
        if part.startswith(&#39;2&#39;) and len(part) &amp;gt; 1:
            if part[1] in string.ascii_letters:
                done = True
    if not done:
        page1 = pdf.getPage(1)
        start = page1.extractText().split(&#39;\n&#39;)[0]
        # Try to find the start of section 2
        if start.startswith(&#39;2&#39;) and len(start) &amp;gt; 1:
            if start[1] in string.ascii_letters:
                done = True
        if not done:
            pdf_writer.addPage(page1)

with open(&#39;example.pdf&#39;, &#39;wb&#39;) as out:
    pdf_writer.write(out)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing the Surprising Variability in Word Embedding Stability Across Languages</title>
      <link>https://www.jkk.name/publication/arxiv20embeddings/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/arxiv20embeddings/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Crowdsourced Detection of Emotionally Manipulative Language</title>
      <link>https://www.jkk.name/publication/chi20anchor/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/chi20anchor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NOESIS II: Predicting Responses, Identifying Success, and Managing Complexity in Task-Oriented Dialogue</title>
      <link>https://www.jkk.name/publication/ws-aaai-dstc20task2/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/ws-aaai-dstc20task2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overview of the seventh Dialog System Technology Challenge: DSTC7</title>
      <link>https://www.jkk.name/publication/csl20dstc/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/csl20dstc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>No-Press Diplomacy: Modeling Multi-Agent Gameplay</title>
      <link>https://www.jkk.name/publication/neurips19diplomacy/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/neurips19diplomacy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Eighth Dialog System Technology Challenge</title>
      <link>https://www.jkk.name/publication/ws-neurips-convai19dstc/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/ws-neurips-convai19dstc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Evaluation for Intent Classification and Out-of-Scope Prediction</title>
      <link>https://www.jkk.name/publication/emnlp19data/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp19data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Training Data Voids: Novel Attacks Against NLP Content Moderation</title>
      <link>https://www.jkk.name/publication/ws-cscw19voids/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/ws-cscw19voids/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Approaching Conferences</title>
      <link>https://www.jkk.name/post/2019-09-22_conferenceapproach/</link>
      <pubDate>Sun, 22 Sep 2019 10:49:35 -0400</pubDate>
      <guid>https://www.jkk.name/post/2019-09-22_conferenceapproach/</guid>
      <description>&lt;p&gt;Am I getting the most out of the time I put into conferences?
This year NAACL and ACL ran mentoring programs to help newer members of the community and in the process of giving advice I started to question whether my own approach to conferences was effective.
Most online advice is aimed at students attending for the first time.
What about a more experienced researcher?
I&amp;rsquo;ve fallen into certain patterns without stepping back to think about whether they are effective and what could be better.&lt;/p&gt;
&lt;h2 id=&#34;during-sessions&#34;&gt;During sessions&lt;/h2&gt;
&lt;p&gt;There are four main options during a session in the main conference:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Attend a talk.
Pro: Typically the most exciting work is presented in talks.
Con: Most talks are poorly presented. Even in a good talk, it is easy to sit and listen but not take it in.&lt;/li&gt;
&lt;li&gt;Attend the poster session.
Pro: Easy to spend more or less time on each poster.
Con: Easy to start skimming titles and figures without learning much (particularly when tired).&lt;/li&gt;
&lt;li&gt;Talk to people.
Pro: A great way to (1) learn about what other people are doing / thinking about right now, (2) make connections / network, (3) make specific people aware of your own work.
Con: Sometimes conversations become just small talk, which is not always worthwhile (Edit: see clarification in the postscript). It can also be hard to approach new people.&lt;/li&gt;
&lt;li&gt;Relax.
Pro: Being focused and engaged is tiring, a break can be rejuvenating.
Con: Missing out on work being presented.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is no perfect fixed combination of these.
The strategy I want to try next time is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Over breakfast, choose 2-3 talks from each session that cover must-see work for my interests.
Look through posters for the topics I have an interest in and flag must-see items.
If the conference provides breakfast, eat elsewhere to do this and then show up for the last 30 minutes to chat.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When a session starts, if I&amp;rsquo;m in an interesting conversation, keep talking.
Otherwise, go to the talks selected that morning and drop by posters in between.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Take notes in the conference handbook on every talk I attend and every poster I do more than read the title of.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This minimises the number of decisions during the day and focuses on the work I am most interested in.
Of the options above, taking a break isn&amp;rsquo;t included.
I&amp;rsquo;m hoping this approach (and more below) will make that unnecessary in general.&lt;/p&gt;
&lt;h2 id=&#34;during-breaks&#34;&gt;During breaks&lt;/h2&gt;
&lt;p&gt;At the first few conferences I went to, I was constantly meeting new people because I didn&amp;rsquo;t know anyone.
Now, I tend to talk to the same people at every conference.
I do want to catch up with those people, but it means I&amp;rsquo;m not making the most of the diverse group the conference brings together.&lt;/p&gt;
&lt;p&gt;Who should I be trying to meet?
As a student, I was most interested in meeting faculty working in my area.
Now, I want to look for (1) the students doing work I am excited about and (2) faculty at places I intend to apply to.
My reasoning on the students is that (1) they will have more to say about their work than their advisor, (2) they are the future of the field, and (3) if there is scope for collaboration then it will be easier to get their advisor on board if they are excited than vice versa.&lt;/p&gt;
&lt;p&gt;How should I try to meet people?
Conferences are so big these days that simply hoping to bump into someone won&amp;rsquo;t work.
One solution is to contact people ahead of time and plan to meet during a specific break.
The same idea can apply to lunch.
Usually I have just joined lunch groups in an ad hoc way, but having a plan for the nucleus of a group and then picking up more people on the day would be more effective.&lt;/p&gt;
&lt;h2 id=&#34;in-the-evening&#34;&gt;In the evening&lt;/h2&gt;
&lt;p&gt;I always go to the conference receptions (ie. Welcome / Social) and plan to continue.
The key questions in my mind are about industry hosted events and when to go to sleep.&lt;/p&gt;
&lt;p&gt;I usually don&amp;rsquo;t get invited to industry events these days.
One thing I could do differently is be more proactive in talking to people from the companies hosting events to tell them I am interested.
When I am invited, I think it is worth going as it&amp;rsquo;s another chance to meet people in a setting where it is easy to join and leave conversations (like conference breaks).&lt;/p&gt;
&lt;p&gt;Sleep is crucial for conferences.
I&amp;rsquo;ve held that opinion for a while, but on reflection I have not gone far enough.
It&amp;rsquo;s easy to keep hanging out and stay up late then set an alarm to make the morning session.
Almost all people need 7-8 hours of sleep, not counting the time before we fall asleep, and the specific sleep hours need to be consistent (see âWhy We Sleepâ by Matthew Walker).
There are two options here: either plan to go back to the hotel at 10pm to be up at 7am, or plan to miss the morning session, staying out till 12am, waking up at 9am.
On the topic of sleep, I&amp;rsquo;m noticing jet lag more as I get older and while the university won&amp;rsquo;t pay for lodging before the conference, I should seriously consider it anyway (but not tire myself out by doing in a million tourist activities).&lt;/p&gt;
&lt;h2 id=&#34;workshop-days&#34;&gt;Workshop days&lt;/h2&gt;
&lt;p&gt;Previously I&amp;rsquo;ve jumped between workshops, trying to squeeze in every keynote talk from someone whose work I am interested in.
In future, I plan to take go to a single workshop all day.
Reflecting on the keynotes I&amp;rsquo;ve seen, most are just conference talks stitched together.
I&amp;rsquo;m not learning a lot that is new.
In contrast, being at a single workshop means engaging with a sub-community.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In many ways, the plans outlined above are very similar to what I do already.
I am eager to see how the changes work out and also hope that having stepped back like this I will feel less uncertain about my choices at the next conference.&lt;/p&gt;
&lt;p&gt;While putting this together I found this series of tweets from Chinmay Kulkarni interesting.
Our opinions differ on certain points, so check them out: 
&lt;a href=&#34;https://twitter.com/chinmay/status/988410612316286976?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://twitter.com/chinmay/status/988410612316286976?s=20&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;postscript-responses&#34;&gt;Postscript: Responses&lt;/h2&gt;
&lt;p&gt;After 
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1175843866878066689&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sharing this&lt;/a&gt; on Twitter, there was some interesting discussion.
For posterity, I&amp;rsquo;m summarising that and other comments I got here:&lt;/p&gt;
&lt;p&gt;General&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Be willing to leave a poster, talk, or conversation. It may feel polite to stay, but time is valuable, so once something no longer seems interesting, move on to something else. This is one reason to sit on the aisle in a talk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note taking is important because there is simply too much happening to remember. One suggestion was to track who you talked to and what it was about.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consider going to random things and trying to understand them. This can lead to unexpected links to your own work and may be more interesting (as everything is new).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I clarified my small talk point above. I definitely see it is valuable, but if every conversation is small talk then you are missing an opportunity to have a conversation you couldn&amp;rsquo;t have outside of a conference.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;People vary in their preferences regarding staying up late or not. Some see it is valuable time to connect. Others get the same from morning activities like running groups.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Talks vs. Posters&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Talks can give a lot of content in a brief period and convey the presenter&amp;rsquo;s view of the most important idea.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Staying for a complete talk session can expose you to work that is relevant to your interests, but you might have otherwise missed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Talks are often recorded now, so you can watch them later instead (but be honest with yourself about whether you will).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Demos and Industry track presentations may have content not in the paper.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Attending a talk is a nice way to connect with someone. It means you are guaranteed to find them and there is a starting place for conversation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interactions at posters can make you feel part of the community in a way talks don&amp;rsquo;t.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As the community grows we may need to explore other structures. For example, at RSS, work is presented as both a four minute talk and a poster session (NAACL tried a 1-minute-madness at least once, which is a similar idea).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>No-Press Diplomacy: Modeling Multi-Agent Gameplay (Paquette et al., 2019)</title>
      <link>https://www.jkk.name/post/2019-09-13_diplomacynopress/</link>
      <pubDate>Fri, 13 Sep 2019 13:00:23 -0400</pubDate>
      <guid>https://www.jkk.name/post/2019-09-13_diplomacynopress/</guid>
      <description>&lt;p&gt;Games have been a focus of AI research for decades, from Samuel&amp;rsquo;s checkers program in the 1950s, to Deep Blue playing Chess in the 1990s, and AlphaGo playing Go in the 2010s.
All of those are two-player sequential games.
In this paper (to appear at NeurIPS), we looked at Diplomacy, a seven player game with simultaneous turns.&lt;/p&gt;
&lt;p&gt;The paper makes three main contributions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A neural model that plays the game.&lt;/li&gt;
&lt;li&gt;Software to play the game (determining the outcomes of player actions is a non-trivial problem).&lt;/li&gt;
&lt;li&gt;Experiments with supervised learning and reinforcement learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our paper only considers the version of the game where players can not talk to each other (No Press).
Engaging in conversation in the game is a fascinating challenge that will involve a lot more work.&lt;/p&gt;
&lt;p&gt;How well does the bot play the game?
It convincingly beats prior systems designed for the game.
Playing against it, I saw an impressive improvement over the course of the project.
Early on I won trivially with mostly conservative moves.
Later I had to carefully consider my moves, and was unable to win as certain powers (e.g. Austria).
Eventually I was unable to beat the bot without playing several times, using observations from one game to inform my strategy in subsequent games.
I am not an expert player, but I doubt a human playing one power in the game with no prior knowledge can win against the bot.
However, I think a single bot playing against six skilled humans would almost definitely lose (we did not test this setting).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1909.02128&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{neurips19diplomacy,
  author    = {Paquette, Philip and Lu, Yuchen and Bocco, Steven and Smith, Max O. and Ortiz-Gagn{\&#39;e}, Satya and Kummerfeld, Jonathan K. and Pineau, Joelle and Singh, Satinder and Courville, Aaron},
  title     = {No-Press Diplomacy: Modeling Multi-Agent Gameplay},
  booktitle = {Advances in Neural Information Processing Systems 32},
  year      = {2019},
  month     = {December},
  pages     = {},
  url       = {},
  arxiv     = {https://arxiv.org/abs/1909.02128},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>IRC Disentanglement</title>
      <link>https://www.jkk.name/data/irc/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/irc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SLATE: A Super-Lightweight Annotation Tool for Experts</title>
      <link>https://www.jkk.name/software/slate/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/software/slate/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DSTC7 Task 1: Noetic End-to-End Response Selection</title>
      <link>https://www.jkk.name/publication/ws-acl-convai19dstc7/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/ws-acl-convai19dstc7/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)</title>
      <link>https://www.jkk.name/post/2019-07-10_disentanglement/</link>
      <pubDate>Wed, 10 Jul 2019 11:19:06 -0400</pubDate>
      <guid>https://www.jkk.name/post/2019-07-10_disentanglement/</guid>
      <description>&lt;p&gt;This post is about my own paper to appear at ACL later this month.
What is interesting about this paper will depend on your research interests, so that&amp;rsquo;s how I&amp;rsquo;ve broken down this blog post.&lt;/p&gt;
&lt;p&gt;A few key points first:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://jkk.name/irc-disentanglement/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data and code&lt;/a&gt; are available on Github.&lt;/li&gt;
&lt;li&gt;The 
&lt;a href=&#34;https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; is also available.&lt;/li&gt;
&lt;li&gt;The general-purpose span labeling and linking 
&lt;a href=&#34;https://jkk.name/slate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;annotation tool&lt;/a&gt; we used is also appearing at ACL.&lt;/li&gt;
&lt;li&gt;Check out 
&lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSTC 8 Track 2&lt;/a&gt;, which is based on this work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;you-study-discourse&#34;&gt;You study discourse&lt;/h3&gt;
&lt;p&gt;We investigated discourse structure when multiple conversations are occurring in the same stream of communication.
In our case, the stream is a technical support channel for Ubuntu on Internet Relay Chat (IRC).
We annotated each message with which message(s) it was a response to.
As far as we are aware, this is the first large-scale corpus with this kind of discourse structure in synchronous chat.
Here is an example from the data, with annotations marked by edges and colours:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/irc-disentanglement-example.png&#34; alt=&#34;IRC Disentanglement Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t frame the paper as being about reply-structure though.
Instead, we focus on a byproduct of these annotations - conversation disentanglement.
Given our graph of reply-structure, each connected component is a single conversation (as shown by each colour in the example).
The key prior work on the disentanglement problem is 
&lt;a href=&#34;https://aclanthology.org/P08-1095&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elsner and Charniak (2008)&lt;/a&gt;, who released the largest annotated resource for the task, with 2,500 messages manually separated into conversations.
We annotated their data with our annotation scheme and 75,000 additional messages.&lt;/p&gt;
&lt;p&gt;We built a set of simple models for predicting reply-structure and did some analysis of assumptions about discourse from prior disentanglement work, but there is certainly more scope for study here.
One direction would be to develop better models for this task.
Another would be to study patterns in the data to understand how people are able to follow the conversation.&lt;/p&gt;
&lt;h3 id=&#34;you-work-on-dialogue&#34;&gt;You work on dialogue&lt;/h3&gt;
&lt;p&gt;There has been a lot of work recently using the Ubuntu dataset from 
&lt;a href=&#34;https://github.com/rkadlec/ubuntu-ranking-dataset-creator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lowe et al., (2015)&lt;/a&gt;, which was produced by heuristically disentangling conversations from the same IRC channel we use.
Their work opened up a fantastic research opportunity by providing 930,000 conversations for training and evaluating dialogue systems.
However, they were unable to evaluate the quality of their conversations because they had no annotated data.&lt;/p&gt;
&lt;p&gt;Using our data, we found that only 20% of their conversations are a true prefix of a conversation (since their next utterance classification task cuts the conversation off part-way, being a true prefix is all that matters).
Many conversations are missing messages, and some have extra messages from other conversations.
Unsurprisingly, our trained model does better, producing conversations that are a true prefix 81% of the time.
We also noticed that their heuristic was incorrectly linking messages far apart in time.
This is not tested by our evaluation set, so we constructed this figure, which shows the problem is quite common:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/irc-disentanglement-comparison.png&#34; alt=&#34;IRC Disentanglement Comparison&#34;&gt;&lt;/p&gt;
&lt;p&gt;The purple results are based on the output of our model over the entire Ubuntu IRC logs.
That output is the basis of 
&lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSTC 8 Track 2&lt;/a&gt;.
Once the competition finishes (October 20th, 2019) we will release all of the conversations.&lt;/p&gt;
&lt;h3 id=&#34;you-am-interested-in-studying-online-communities&#34;&gt;You am interested in studying online communities&lt;/h3&gt;
&lt;p&gt;This is not my area of expertise, but our data and models could enable the exploration of interesting questions.
For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the structure of the community? By looking at who asks for help and who responds we could see patterns of behaviour.&lt;/li&gt;
&lt;li&gt;How does a community evolve over time? This data spans 15 years, during which there were many Ubuntu releases, Stackoverflow was created, other Ubuntu forums were created, etc. It seems likely that those events and more would be reflected in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It would be interesting to apply the model to other communities, but that would require additional in-domain data to get good results.
We have no plans to collect additional data at this stage, and for other channels there are copyright questions that might be difficult to resolve (the Ubuntu channels have an open access license).&lt;/p&gt;
&lt;h3 id=&#34;you-mainly-care-about-neural-network-architectures&#34;&gt;You mainly care about neural network architectures&lt;/h3&gt;
&lt;p&gt;We experimented with a bunch of ideas that didn&amp;rsquo;t improve performance, so our final model is very simple (a feedforward network with features representing the logs and sentences represented by averaging and max-pooling GloVe embeddings).
Maybe that means there is an opportunity for you to improve on our results with a fancy model?
One of our motivations for making such a large new resource was to make it possible to train sophisticated models.&lt;/p&gt;
&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;This project has been going since I started at Michigan as a postdoc funded by a grant from IBM.
The final paper is the result of collaboration with a large group of people from Michigan and IBM.
Thank you!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{acl19disentangle,
  author    = {Kummerfeld, Jonathan K. and Gouravajhala, Sai R. and Peper, Joseph and Athreya, Vignesh and Gunasekara, Chulaka and Ganhotra, Jatin and Patel, Siva Sankalp and Polymenakos, Lazaros and Lasecki, Walter S.},
  title     = {A Large-Scale Corpus for Conversation Disentanglement},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  location  = {Florence, Italy},
  month     = {July},
  year      = {2019},
  url       = {https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf},
  arxiv     = {https://arxiv.org/abs/1810.11118},
  software  = {https://jkk.name/irc-disentanglement},
  data      = {https://jkk.name/irc-disentanglement},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Large-Scale Corpus for Conversation Disentanglement</title>
      <link>https://www.jkk.name/publication/acl19disentangle/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl19disentangle/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SLATE: A Super-Lightweight Annotation Tool for Experts</title>
      <link>https://www.jkk.name/publication/acl19slate/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl19slate/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Outlier Detection for Improved Data Quality and Diversity in Dialog Systems</title>
      <link>https://www.jkk.name/publication/naacl19outliers/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/naacl19outliers/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Crowdsourcing Services</title>
      <link>https://www.jkk.name/post/crowdsourcing-services/</link>
      <pubDate>Sun, 14 Apr 2019 10:49:35 -0400</pubDate>
      <guid>https://www.jkk.name/post/crowdsourcing-services/</guid>
      <description>&lt;p&gt;Crowdsourcing, collecting annotations of data from a distributed group of people online, is a major source of data for AI research.
The original idea involved people doing it as volunteers (e.g. Folding@home) or as a byproduct of some other goal (e.g. reCAPTCHA), but most of the data collected in AI today is from paid workers.
Recently, 
&lt;a href=&#34;http://users.umiacs.umd.edu/~hal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hal DaumÃ© III&lt;/a&gt; mentioned on 
&lt;a href=&#34;https://twitter.com/haldaume3/status/1113889907586535425&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt; that Figure Eight, a paid crowdsourcing service, had removed their free licenses for academics, and asked for alternatives (Note, Figure Eight has since been acquired by Appen).
A bunch of people had suggestions which I wanted to record for my own future reference, hence this blog post.&lt;/p&gt;
&lt;p&gt;These fell into a few categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Crowd providers, which directly connect with workers.&lt;/li&gt;
&lt;li&gt;Crowd enhancers, which provide a layer on top of the providers that adds features (e.g. active learning, nice templates, sophisticated workflows).&lt;/li&gt;
&lt;li&gt;Annotation tools, which are designed to integrate with crowd providers (or your own internal workers).&lt;/li&gt;
&lt;li&gt;Interfaces, which make it easier to use one of the crowd providers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I decided not to break the first two categories apart because it was sometimes unclear whether a service was using their own crowd or providing a layer over another, but I have roughly sorted them.
Where possible I have included pricing, though some services did not make it easy to find.
Take note of the description in each case because the data collected varies substantially.
Also note that many tasks can be structured as a classification task (e.g. &amp;ldquo;Is this coreference link correct?&amp;quot;), making many of these services more flexible than the &amp;lsquo;text classification&amp;rsquo; label below may seem (though structuring your task so costs don&amp;rsquo;t explode may require some thought).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.mturk.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mechanical Turk&lt;/a&gt;, a small set of templates and the option to define a web UI that does whatever you want. Cost is a 20% fee on top of whatever you choose to pay workers (though note it jumps to 40% if you have more than 10 assignments for a HIT!).&lt;/li&gt;
&lt;li&gt;Figure Eight (now part of 
&lt;a href=&#34;http://appen.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Appen&lt;/a&gt;), included for completeness, did not investigate further due to the cost.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.gethybrid.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hybrid&lt;/a&gt;, seems to be any task you can define in text (including with links?). 40% fee, though there is a 
&lt;a href=&#34;http://www.gethybrid.io/faq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discount&lt;/a&gt; of some type for academic and non-profit institutions.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://prolific.ac/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prolific&lt;/a&gt;, seems to be that you just provide a link to a site for annotations (originally intended for survey research). 30% fee. Last year they had a 
&lt;a href=&#34;https://blog.prolific.ac/announcing-2018-junior-grant-winners/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;research grant&lt;/a&gt; program.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://gorilla.sc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gorilla&lt;/a&gt;, designed for social science research, but could be used for any classification or free text task. Costs $1.19 / response, though note that you construct a questionnaire with a series of questions. There are also 
&lt;a href=&#34;https://gorilla.sc/support/reference/subscription-FAQ#subscription-types&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discounts&lt;/a&gt; available when collecting thousands of responses.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://scale.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scale&lt;/a&gt;, classification tasks for 8c / annotation. There is an academic program, but details are not available online (mentioned 
&lt;a href=&#34;https://twitter.com/umbrant/status/1114312024970764290&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://aws.amazon.com/sagemaker/groundtruth/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon SageMaker Ground Truth&lt;/a&gt;, text classification for 8c / label, decreasing after 50,000 annotations + a workflow fee of 1.2c / label.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://imerit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iMerit&lt;/a&gt;, NER, classification, and sentiment tasks. When used on the Amazon Marketplace they are 5 dollars / hour (India based workers) or 25 (US based workers).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mechanical-turk-integration-interfaces&#34;&gt;Mechanical Turk Integration Interfaces&lt;/h2&gt;
&lt;p&gt;These are interfaces for Mechanical Turk that provide an easier way to set up HITs without having to mess with Amazon&amp;rsquo;s APIs yourself.
Both are free, but have slightly different features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.cromalab.net/LegionTools/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LegionTools&lt;/a&gt;, self-hosted or not, includes key features for real-time systems.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/webis-de/mturk-manager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MTurk Manager&lt;/a&gt;, self-hosted, includes features for custom views of responses from workers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;annotation-user-interfaces&#34;&gt;Annotation User Interfaces&lt;/h2&gt;
&lt;p&gt;There are many annotation tools for NLP (e.g. my own, 
&lt;a href=&#34;https://www.jkk.name/slate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SLATE&lt;/a&gt;!), but these annotation tools are designed to integrate with providers above to collect annotations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://prodi.gy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prodigy&lt;/a&gt;, span classification (e.g. NER), multiple choice questions (which can be used to do a wide range of tasks), and relations (see 
&lt;a href=&#34;https://prodi.gy/features/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;examples&lt;/a&gt;). Cost is whatever you pay a crowd provider + 390 for a lifetime license, or 10k for a university-wide lifetime license, though they also often give free licenses to academics. One distinctive property is that you download and run it yourself, providing complete control over your data.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lighttag.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LightTAG&lt;/a&gt;, span classification and links. Cost is 1c / annotation + the cost from a crowd provider, but there is an academic license that makes it free.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Look Who&#39;s Talking: Inferring Speaker Attributes from Personal Longitudinal Dialog</title>
      <link>https://www.jkk.name/publication/cicling19personal/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/cicling19personal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DSTC7 Task 1: Noetic End-to-End Response Selection</title>
      <link>https://www.jkk.name/publication/ws-aaai-dstc19task1/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/ws-aaai-dstc19task1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning from Personal Longitudinal Dialog Data</title>
      <link>https://www.jkk.name/publication/ieee19personal/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/ieee19personal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dialog System Technology Challenge 7</title>
      <link>https://www.jkk.name/publication/ws-neurips-convai18dstc/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/ws-neurips-convai18dstc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution (Chen et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-11-08_corefdata/</link>
      <pubDate>Thu, 08 Nov 2018 11:29:32 -0500</pubDate>
      <guid>https://www.jkk.name/post/2018-11-08_corefdata/</guid>
      <description>&lt;p&gt;The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset).
Some of these are discussed in my 
&lt;a href=&#34;https://www.jkk.name/publication/conll11coreference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoNLL Shared Task submission paper&lt;/a&gt;, the biggest being the choice to not annotate mentions that are not coreferent.
This paper describes a new dataset that has a different set of compromises, specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A broader definition of coreference (e.g. appositives are coreferent)&lt;/li&gt;
&lt;li&gt;All mentions annotated&lt;/li&gt;
&lt;li&gt;Different annotation methods for different subsets of the data (training data is double annotated and then adjudicated, while the development and test data is triple annotated, all pairs of annotations are adjudicated, then the outcomes are merged by voting)&lt;/li&gt;
&lt;li&gt;A variety of genres, but generally simpler language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dataset is 10x the size of OntoNotes and freely available, which is fantastic.
The source text is 2/3rds the RACE dataset (English reading comprehension exams from China), and 1/3rd scraped websites.
Measurements of annotator agreement suggest the annotations are not as consistent as OntoNotes, but still good enough to be a useful resource.
I do disagree with one aspect of the paper&amp;rsquo;s analysis - the results show a substantial gain in performance when providing gold mentions, suggesting to me that it remains an important challenge in coreference resolution.
I&amp;rsquo;m also curious whether my 
&lt;a href=&#34;https://www.jkk.name/publication/emnlp13analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coreference analysis tool&lt;/a&gt; would find different patterns in errors on this dataset compared to OntoNotes.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D18-1016&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://preschool-lab.github.io/PreCo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Chen:EMNLP:2018,
  author    = {Chen, Hong and Fan, Zhenhua and Lu, Hao and Yuille, Alan and Rong, Shu},
  title     = {PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  pages     = {172--181},
  location  = {Brussels, Belgium},
  url       = {https://aclanthology.org/D18-1016},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-09-04_featureengineering/</link>
      <pubDate>Tue, 04 Sep 2018 10:37:23 -0400</pubDate>
      <guid>https://www.jkk.name/post/2018-09-04_featureengineering/</guid>
      <description>&lt;p&gt;A common argument in favour of neural networks is that they do not require &amp;lsquo;feature engineering&amp;rsquo;, manually defining functions that produce useful representations of the input data (e.g. a function that checks if a word is in a list of cities and returns 1 or 0).
This paper argues that there is in fact still value in such functions.&lt;/p&gt;
&lt;p&gt;The task is named entity recognition and the model is a CRF with a bidirectional LSTM using character and word embeddings.
The functions in this case are (1) part of speech tags, (2) word shapes, and (3) gazetteers.
Importantly, as well as receiving these as inputs, the model has to predict them as outputs (in both cases using predictions, not gold values).
The improvement on the test set is substantial, ~0.8 F1.
Ablation indicates that POS tags and word shape are particularly important, and having both the input and output is important.
Interestingly, the shift on the development set is more marginal, ~0.3 F1, and the ablation doesn&amp;rsquo;t show as clear trends.&lt;/p&gt;
&lt;p&gt;Overall, my takeaway is that these kinds of features (which are not very hard to define) are worth the effort.
However, there are a few more values I would have liked to see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-task learning (they kind of get at this with one ablation, but it is on non-gold output)&lt;/li&gt;
&lt;li&gt;Cross-validation results (given the difference between dev and test)&lt;/li&gt;
&lt;li&gt;ELMo (the paper argues that it is orthogonal, which is reasonable, but I&amp;rsquo;m still curious)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1808.09075&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Wu:2018:EMNLP,
  author    = {Wu, Minghao and Liu, Fei and Cohn, Trevor},
  title     = {Evaluating the Utility of Hand-crafted Features in Sequence Labelling},
  booktitle = {EMNLP},
  year      = {2018},
  url       = {https://arxiv.org/abs/1808.09075},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Neural POS tagging</title>
      <link>https://www.jkk.name/software/neural-tagger/</link>
      <pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/software/neural-tagger/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Text to SQL Baseline</title>
      <link>https://www.jkk.name/software/text2sql-baseline/</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/software/text2sql-baseline/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Text to SQL datasets</title>
      <link>https://www.jkk.name/data/text-to-sql/</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/text-to-sql/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Improving Text-to-SQL Evaluation Methodology</title>
      <link>https://www.jkk.name/publication/acl18sql/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl18sql/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples (Vidur Joshi et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-06-12_parseradaptation/</link>
      <pubDate>Tue, 12 Jun 2018 20:33:00 -0400</pubDate>
      <guid>https://www.jkk.name/post/2018-06-12_parseradaptation/</guid>
      <description>&lt;p&gt;Virtually all systems trained using data have trouble when applied to datasets that differ even slightly - even switching from Wall Street Journal text to New York Times text can hurt parsing performance slightly.
Extensive work has explored how to adapt to new domains (including 
&lt;a href=&#34;https://www.jkk.name/publication/acl10adapt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one of my own&lt;/a&gt;), but generally these approaches only made up a fraction of the gap in performance.&lt;/p&gt;
&lt;p&gt;This paper shows two interesting new approaches to this issue:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use 
&lt;a href=&#34;https://allennlp.org/elmo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ELMo&lt;/a&gt;, a type of word representation trained on massive amounts of text.&lt;/li&gt;
&lt;li&gt;Train a span-based parser with partial annotations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first is straightforward, and further demonstrates the effectiveness of ELMo.
To give a sense of how much this helps, the Charniak parser goes from 92 on the WSJ to 85 on the Brown corpus, while this model goes from 94 to 90.
The second idea takes advantage of 
&lt;a href=&#34;https://aclanthology.org/P17-1076.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a recent parsing model&lt;/a&gt; with a simple approach:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Independently assign a score to every span of a sentence, indicating whether it is part of the parse.&lt;/li&gt;
&lt;li&gt;Find the maximum scoring set of spans using a dynamic program.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The structure of the scoring step allows for a convenient form of partial annotations.
Simply label the tricky spans in a sentence (e.g. to indicate where a prepositional phrase attaches / does not attach).
During training on partially annotated sentences, only the labeled spans are used to update the model.
This gives dramatic gains across multiple datasets.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1805.06556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Joshi:2018:ACL,
  author    = {Joshi, Vidur and Peters, Matthew and Hopkins, Mark},
  title     = {Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples},
  booktitle = {ACL},
  year      = {2018},
  url       = {https://arxiv.org/abs/1805.06556},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data Collection for a Production Dialogue System: A Startup Perspective</title>
      <link>https://www.jkk.name/publication/naacl18data/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/naacl18data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effective Crowdsourcing for a New Type of Summarization Task</title>
      <link>https://www.jkk.name/publication/naacl18summary/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/naacl18summary/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Factors Influencing the Surprising Instability of Word Embeddings</title>
      <link>https://www.jkk.name/publication/naacl18embeddings/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/naacl18embeddings/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models (Weber et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-05-08_seq2seqsensitivity/</link>
      <pubDate>Tue, 08 May 2018 09:00:31 -0400</pubDate>
      <guid>https://www.jkk.name/post/2018-05-08_seq2seqsensitivity/</guid>
      <description>&lt;p&gt;We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models usually end up doing reasonably well.
This paper asks an important question - are those metrics measuring generalisability effectively?
In particular, if we sample our test set from a slightly different distribution of data, do models still work well?&lt;/p&gt;
&lt;p&gt;As a controlled set up they form a simple dataset as follows for each sentence:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go through the sentence left to right&lt;/li&gt;
&lt;li&gt;For each word generate three words in the output, where the output words are randomly sampled from a small vocabulary that is unique to each input word&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is clearly learnable and it seems reasonable that a sequence-to-sequence neural model with attention should be able to learn it.
Experiments show they do, getting close to 100% on a test set sampled the same way as the training set (input length 5-10, no symbol used twice).
However, if the test set is slightly different, with sequences of length 11-15, then results vary from 0% to 98% depending on the random seed in training (other variations also lead to large variations).
What this means is that sometimes the model is not learning to generalise.
They also show that the models that do generalise can only do so in one way (e.g. remain effective when length varies, or remain effective when symbols are used more than once in the input).&lt;/p&gt;
&lt;p&gt;A few takeaways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure your training and testing data are sampled from the distribution you are interested in&lt;/li&gt;
&lt;li&gt;More study of training data order and weight initialisation is needed (these are the two factors impacted by the random seed)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Incidentally, I am a co-author on an ACL paper that points out a similar issue for mapping text questions to SQL queries.
If we restrict the test set to be novel queries (i.e. the model has to generalise) performance falls through the floor.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1805.01445&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{Weber:2018:GenDeep,
  author    = {Weber, Noah and Shekhar, Leena and Balasubramanian, Niranjan},
    title = {The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models},
  journal = {Workshop on New Forms of Generalization in Deep Learning and NLP (NAACL 2018)},
     year = {2018},
      url = {https://arxiv.org/abs/1805.01445},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>World Knowledge for Abstract Meaning Representation Parsing</title>
      <link>https://www.jkk.name/publication/lrec18amr/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/lrec18amr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-04-16_lm_analysis/</link>
      <pubDate>Mon, 16 Apr 2018 20:55:22 -0400</pubDate>
      <guid>https://www.jkk.name/post/2018-04-16_lm_analysis/</guid>
      <description>&lt;p&gt;Assigning a probability distribution over the next word or character in a sequence (language modeling) is a useful component of many systems, such as speech recognition and translation.
Recently neural networks have come to dominate in performance, with a range of clever innovations in network structure.
This paper is not about new models, but rather explores the current evaluation and how well carefully tuned baseline models can do.&lt;/p&gt;
&lt;p&gt;The key observations for me were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are issues with the PTB dataset for character-level evaluation - it removes all punctuation, makes numbers &amp;lsquo;N&amp;rsquo;, and removes rare words (i.e. it is a character-level version of the token-level task).
Given that the original Penn Treebank exists, I would have been interested to see a comparison with the PTB without any simplification.
The other dataset, enwik8, makes sense as a testing ground for compression algorithms, but is a little odd for modeling language, since it is the first 100 million bytes of a Wikipedia XML dump.
The paper does have another dataset, WikiText, which sounds good, but then there is no character-level evaluation!&lt;/li&gt;
&lt;li&gt;The LSTM is able to achieve ~SotA results for character-level modeling.
The key seems to be careful design of the softmax that produces the final probability distribution:
(1) rare words are clustered and represented by a single value in the distribution calculation, and
(2) word vectors are shared between input and output.&lt;/li&gt;
&lt;li&gt;Dropout matters more than the network design, and multiple forms of dropout should be tuned jointly.
This comes from analysis of a set of models trained with random variation in hyperparameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1803.08240&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{2018arXiv180308240M,
   author = {{Merity}, S. and {Shirish Keskar}, N. and {Socher}, R.},
    title = {An Analysis of Neural Language Modeling at Multiple Scales},
  journal = {ArXiv e-prints},
     year = {2018},
      url = {https://arxiv.org/abs/1803.08240},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Provenance for Natural Language Queries (Deutch et al., 2017)</title>
      <link>https://www.jkk.name/post/2018-03-08_sql/</link>
      <pubDate>Thu, 08 Mar 2018 20:11:48 -0500</pubDate>
      <guid>https://www.jkk.name/post/2018-03-08_sql/</guid>
      <description>&lt;p&gt;Being able to query a database in natural language could help make data accessible to more people.
Systems that do this have to solve two challenges: (1) understanding the query and (2) expressing the response in a way the user will understand.
Recently there have been papers in the NLP community on the first challenge, but this paper comes from the DB community and considers the second.&lt;/p&gt;
&lt;p&gt;The approach assumes we have a syntactic parse of the query and an alignment between the parse and the SQL query it corresponds to (they rely on prior work for this query interpretation piece).
Given that, the new idea in this paper is to take the database results and use the alignment to insert values for each field into the original parse, and from there into the original question.
To avoid extremely long sentences (when there are multiple result rows) they define a procedure to identify ways to summarise results.&lt;/p&gt;
&lt;p&gt;However, I&amp;rsquo;m not convinced by the evaluation.
The dataset they use was collected by (1) enumerating the 196 types of queries people could ask using the Microsoft Academic Search service, and (2) a person manually writing a question for each query.
As a result, the questions feel very formulaic and also only cover cases that we already have a user-friendly interface for, making it unclear how well this will generalise to more natural data.
Still, this work explores an interesting problem and it&amp;rsquo;s cool to see a direct use of syntactic parsing!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.vldb.org/pvldb/vol10/p577-deutch.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{Deutch:2017,
  author = {Deutch, Daniel and Frost, Nave and Gilad, Amir},
  title = {Provenance for Natural Language Queries},
  journal = {Proceedings of the VLDB Endowment},
  volume = {10},
  number = {5},
  month = {Jan},
  year = {2017},
  issn = {2150-8097},
  pages = {577--588},
  doi = {10.14778/3055540.3055550},
  url = {http://www.vldb.org/pvldb/vol10/p577-deutch.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning (Tsvetkov et al., 2016)</title>
      <link>https://www.jkk.name/post/2018-03-05_curriculum/</link>
      <pubDate>Mon, 05 Mar 2018 21:09:58 -0500</pubDate>
      <guid>https://www.jkk.name/post/2018-03-05_curriculum/</guid>
      <description>&lt;p&gt;Usually when we learn, we have a curriculum designed to incrementally build understanding.
It seems reasonable that the same idea could be useful for machine learning, and indeed there is a large body of work on the topic.
This paper explores the specific question of whether a curriculum can help develop task-specific word vectors, and whether we can determine an effective curriculum automatically.&lt;/p&gt;
&lt;p&gt;They define a linear model with a range of features that characterise a paragraph of text, such as the number of distinct words, the number of prepositional phrases, and the average number of syllables per word.
Paragraphs are sorted by the model and used to train word vectors with word2vec.
These word vectors are then used as part of a model for a target task, giving a score that indicates the quality of the curriculum.
Based on this score the weights for the model are updated, using a form of Bayesian optimisation.&lt;/p&gt;
&lt;p&gt;One really nice aspect of this paper is the range of tasks considered: sentiment analysis, NER, POS tagging, and parsing.
Learning a curriculum does improve performance slightly, and which features are important varies across the tasks (indicating the importance of task-specific curriculums).
However, the models are somewhat restricted (as shown by the low absolute performance) because they do not change the word vectors during training.
For most of this paper that&amp;rsquo;s a reasonable decision, as it allows a clearer learning signal, but it would have been interesting to also see the impact on the normal training scenario and a state-of-the-art model.
In my experience (and in our soon-to-appear NAACL paper) we find that variations in word vectors can disappear during training for a downstream task.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P16-1013&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{tsvetkov-EtAl:2016:P16-1,
  author    = {Tsvetkov, Yulia  and  Faruqui, Manaal  and  Ling, Wang  and  MacWhinney, Brian  and  Dyer, Chris},
  title     = {Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {130--139},
  url       = {https://aclanthology.org/P16-1013}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (Wieting et al., 2017)</title>
      <link>https://www.jkk.name/post/2018-01-31_sentencerepfromparaphrases/</link>
      <pubDate>Wed, 31 Jan 2018 19:25:36 -0500</pubDate>
      <guid>https://www.jkk.name/post/2018-01-31_sentencerepfromparaphrases/</guid>
      <description>&lt;p&gt;It would be convenient to have a way to represent sentences in a vector space, similar to the way vectors are frequently used to represent input words for a task.
Quite a few sentence embeddings methods have been proposed, but none have really caught on.
Building on prior work by the same authors, the approach here is to define a neural network that maps a sentence to a vector, then train it with a loss function that measures similarity between the vectors for paraphrases.&lt;/p&gt;
&lt;p&gt;This paper scales up the approach, using millions of paraphrases, and explores a range of models.
To get the paraphrases they use translation (start with a sentence, translate it to another language and back, then assume the translation is a paraphrase).
For negative examples they use the sentence that the model currently thinks is most similar other than the correct one (choosing this from a large enough set is key).&lt;/p&gt;
&lt;p&gt;The best model is very simple - concatenate together the average of word vectors and the average of character trigram vectors.
That consistently beats prior work, including convolutional models, and LSTMs.
In a way, this is nice as it is a simple way to get a sentence representation!
On the other hand, this can&amp;rsquo;t possibly capture the semantics of a sentence fully since it doesn&amp;rsquo;t take word order into consideration at all.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1711.05732&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv171105732W,
  author        = {{Wieting}, J. and {Gimpel}, K.},
  title         = {Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations},
  journal       = {ArXiv e-prints},
  archivePrefix = {arXiv},
  eprint        = {1711.05732},
  primaryClass  = {cs.CL},
  year          = {2017},
  month         = {November},
  url           = {https://arxiv.org/abs/1711.05732},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-01-28_crowdassistant/</link>
      <pubDate>Sun, 28 Jan 2018 16:01:20 -0500</pubDate>
      <guid>https://www.jkk.name/post/2018-01-28_crowdassistant/</guid>
      <description>&lt;p&gt;There is a lot of interest in dialogue agents, but a lot of work sits at one of two extremes: either chit-chat agents that just chat, or task-oriented agents that aim to call a specific API for the user.
This work is about trying to integrate a range of systems from both categories, to get something more general purpose as a result.&lt;/p&gt;
&lt;p&gt;The core approach is a hybrid system that switches between different agents behind the scenes (an approach taken by a number of Alexa Prize teams).
The innovation here is that crowd workers will help with the decision (both suggesting things to say and voting on which response to use), and their votes will be used to learn a model to (partially) replace the people over time.&lt;/p&gt;
&lt;p&gt;Unfortunately, the improvement from a learned model of votes is only small (saves only 14% of the crowd effort), and the automated responses are rarely chosen (12% of the time).
That said, it seems like an interesting design with a lot of subtle decisions that require more exploration - the sets of agents (4-6 here, mostly narrow types), the voting scheme (only 1 or 2 votes needed here), choosing which agent responses to show (here, the proportion of previously accepted messages from this agent), and so on.
That choice of which responses to show is particularly tricky, as with this scheme a very domain specific agent might get voted down too much initially and never be chosen when the appropriate time comes.
One potentially interesting alternative would be to let the crowd workers choose which agent&amp;rsquo;s response to see, and possibly even post-edit slightly.&lt;/p&gt;
&lt;p&gt;Note - This post is the first of a (hopefully) regular series again.
However, rather than keeping it weekday-ly, I plan to do three times a week, at least until the ACL deadline.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{blah,
  title = {Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time},
  author    = {Huang, Ting-Hao (Kenneth) and Chang, Joseph Chee and Bigham, Jeffrey P.},
  booktitle = {CHI},
  year = {2018},
  url = {https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings (Yang et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-12_multidomainwordvector/</link>
      <pubDate>Tue, 12 Dec 2017 20:25:40 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-12_multidomainwordvector/</guid>
      <description>&lt;p&gt;To construct word vectors from multi-domain data, use a separate vector for each domain and add a loss term to encourage them to agree.
Here the loss is an l2 norm, weighted by a factor that depends on the frequency of the words in the two domains.
The factor is the harmonic mean of the normalised frequency in each domain (so the lower frequency dominates the factor, pulling it lower).
Across a range of tasks this consistently performs better than other approaches.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1312&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{yang-lu-zheng:2017:EMNLP2017,
  author    = {Yang, Wei  and  Lu, Wei  and  Zheng, Vincent},
  title     = {A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2898--2904},
  url       = {https://aclanthology.org/D17-1312}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The strange geometry of skip-gram with negative sampling (Mimno et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-12_wordvectorgeometry/</link>
      <pubDate>Tue, 12 Dec 2017 20:15:34 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-12_wordvectorgeometry/</guid>
      <description>&lt;p&gt;It turns out that if the vectors learned by word2vec are projected into a plane they all point in the same direction.
Also, the context vectors (which are part of the algorithm, but not retained afterwards) point the other way.
When visualising with t-SNE this effect is not visible because of the way the space is warped to optimise the t-SNE objective.&lt;/p&gt;
&lt;p&gt;This is surprising, and may seem problematic since it doesn&amp;rsquo;t fit our goals for what these vectors should be capturing.
However, it doesn&amp;rsquo;t seem to impact downstream tasks, for example, GloVe does not have this property, and doesn&amp;rsquo;t seem to derive a great benefit from it.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1308&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{mimno-thompson:2017:EMNLP2017,
  author    = {Mimno, David  and  Thompson, Laure},
  title     = {The strange geometry of skip-gram with negative sampling},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2873--2878},
  url       = {https://aclanthology.org/D17-1308}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Sequence Effects in Crowdsourced Annotations (Mathur et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-08_crowdbias/</link>
      <pubDate>Fri, 08 Dec 2017 19:49:09 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-08_crowdbias/</guid>
      <description>&lt;p&gt;Getting high quality annotations from crowdsourcing requires careful design.
This paper looks at how one annotation a worker does can influence their next annotation, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When scoring translations, a good example may make the next one look worse in comparison&lt;/li&gt;
&lt;li&gt;For labeling tasks, we may expect a long sequence of the same label to be rare (the gambler&amp;rsquo;s fallacy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To investigate this they fit a linear model with inputs (previous label, gold label, random noise) and see what the coefficients are.
Across multiple tasks, there is a non-zero correlation with the previous label.
Interestingly, there also seems to be a learning effect for good workers, where over time they become calibrated and show less sequence bias.
Fortunately, there is a simple solution - for each worker, give every annotator their documents in a different random order!
With that change, averaging over annotations should avoid this bias.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1306&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{mathur-baldwin-cohn:2017:EMNLP2017,
  author    = {Mathur, Nitika  and  Baldwin, Timothy  and  Cohn, Trevor},
  title     = {Sequence Effects in Crowdsourced Annotations},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2860--2865},
  url       = {https://aclanthology.org/D17-1306}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-07_rarewordvectors/</link>
      <pubDate>Thu, 07 Dec 2017 20:45:39 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-07_rarewordvectors/</guid>
      <description>&lt;p&gt;Word vectors are great for common words, but what about rare words?
People can have a fairly good understanding of a word given only a few instances, but it&amp;rsquo;s fairly standard to turn all words with a frequency of less than 5 into UNK when learning word vectors.&lt;/p&gt;
&lt;p&gt;One simple approach is to add up word vectors from the context of the rare word and use that as the representation.
This paper proposes using a tweaked version of word2vec: keep vectors for frequent words fixed, increase the learning rate, use a fixed width context window, initialise with the additive approach, and only subsample by discarding frequent words.
All of those make sense, though I am curious whether it would be better to just decrease subsampling or disable it entirely.&lt;/p&gt;
&lt;p&gt;The results are mixed, with the improvement over the additive approach data dependent.
That might partly reflect the tasks though - something downstream like POS tagging would have been interesting, particularly since the LSTM may already be capturing contextual information that covers what the additive approach has, but not what this adds.
Ultimately this is not a solution to this problem, but it&amp;rsquo;s an idea to keep in mind.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1030&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{herbelot-baroni:2017:EMNLP2017,
  author    = {Herbelot, Aur\&#39;{e}lie  and  Baroni, Marco},
  title     = {High-risk learning: acquiring new word vectors from tiny data},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {304--309},
  url       = {https://aclanthology.org/D17-1030}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Revisiting Selectional Preferences for Coreference Resolution (Heinzerling et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-06_coreferencearguments/</link>
      <pubDate>Wed, 06 Dec 2017 19:05:20 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-06_coreferencearguments/</guid>
      <description>&lt;p&gt;Selectional preferences in this context are about how some verbs are more likely to take certain types of arguments (e.g. people laugh, computers do not).
Many papers have added features or structures to coreference systems aiming to get at this kind of information.
This paper presents another way of doing it and experiments that probe how useful it is (punchline: not very).&lt;/p&gt;
&lt;p&gt;Their approach is to parse a large amount of text, producing noun-verb pairs.
They learn vector representations of the relations and try to create a single space containing both entities and relations (e.g. Michigan gets a vector, as does attended@dobj).
The goal is that entities end up in locations similar to the locations of relations they are selected for.&lt;/p&gt;
&lt;p&gt;For results, first it seems like these vector similarities do not correlate particularly strongly with being coreferent.
It could be that the feature on its own isn&amp;rsquo;t enough, or this representation might not be capturing it effectively.
Adding this to the Stanford coreference system they are able to get slight gains, though the improvement might not be statistically significant.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not sure exactly how to do this, but it would be neat if a vector at some point of the model could be modified to remove any correlation with these features, and see what that does to performance.
If performance remains high, then this actually is an uninformative feature, but if it drops that suggests the model is already learning it.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1138&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{heinzerling-moosavi-strube:2017:EMNLP2017,
  author    = {Heinzerling, Benjamin  and  Moosavi, Nafise Sadat  and  Strube, Michael},
  title     = {Revisiting Selectional Preferences for Coreference Resolution},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {1332--1339},
  url       = {https://aclanthology.org/D17-1138}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Neural Semantic Parsing over Multiple Knowledge-bases (Herzig et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-05_multidomainparsing/</link>
      <pubDate>Tue, 05 Dec 2017 19:28:33 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-05_multidomainparsing/</guid>
      <description>&lt;p&gt;One reason learning for semantic parsing is difficult is that the datasets are generally small.
Assuming some words behave similarly across domains, multi-domain parsing should improve performance by providing more data, which is essentially what this paper finds.
They consider several configurations, all based on a sequence to sequence LSTM:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Train a separate model for every domain.&lt;/li&gt;
&lt;li&gt;Use a single model. They do three subtypes here, (a) that&amp;rsquo;s it, (b) add an LSTM input at each step with the domain, (c) give the domain as a token at the start.&lt;/li&gt;
&lt;li&gt;Use a single encoder model, but a different decoder for each domain.&lt;/li&gt;
&lt;li&gt;Combine (1) and (3), have two encoders, one that is domain specific and one that is trained on all domains.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results show that any of these does better than (1), with (2b) doing best.
There also seems to be three sections: first the independent models (1), then the models with multiple decoders (3 and 4), then the variants of (2).
A natural thing to try would be a version of (4) with a single decoder, in which case the thing that is shared is the output space representation (rather than the input space as the motivation for the paper frames it).
From the paper it sounds like very little hyperparameter tuning was tried, which is a shame because it makes it less clear how definitive the results are.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-2098&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{herzig-berant:2017:Short,
  author    = {Herzig, Jonathan  and  Berant, Jonathan},
  title     = {Neural Semantic Parsing over Multiple Knowledge-bases},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {623--628},
  url       = {https://aclanthology.org/P17-2098}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-05_explainingpredictions/</link>
      <pubDate>Tue, 05 Dec 2017 15:40:45 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-05_explainingpredictions/</guid>
      <description>&lt;p&gt;Interpreting the behaviour of statistical models in NLP has been hard for a long time, but it has gotten even harder with nonlinear models.
The simplest method so far in NLP has been to look at the attention distributions in sequence to sequence models, but that doesn&amp;rsquo;t provide everything we need and obviously only applies when the model has attention.
For looking at the dynamics of the hidden state in an LSTM the Harvard NLP group built a cool 
&lt;a href=&#34;http://lstm.seas.harvard.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;visualisation&lt;/a&gt;, but what about structured outputs?&lt;/p&gt;
&lt;p&gt;This paper considers sequence to sequence models and determines which parts of the input were most important for determining each part of the output.
The steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use a variational autoencoder to get perturbed versions of the input&lt;/li&gt;
&lt;li&gt;Use logistic regression to get scores for every output symbol indicating how sensitive it is to variations in parts of the input&lt;/li&gt;
&lt;li&gt;Create a bipartite graph between inputs and outputs, then find high weight components in the graph&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These components serve as the representation of which parts of the input determine which parts of the output.
Experiments show results that match with past observations and intuitions, which is good for supporting the effectiveness of the method, but it&amp;rsquo;s a shame this didn&amp;rsquo;t uncover any exciting new patterns.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1042&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{alvarezmelis-jaakkola:2017:EMNLP2017,
  author    = {Alvarez-Melis, David  and  Jaakkola, Tommi},
  title     = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {412--421},
  url       = {https://aclanthology.org/D17-1042}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Local Detection Approach for Named Entity Recognition and Mention Detection (Xu et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-01_nonsequencener/</link>
      <pubDate>Fri, 01 Dec 2017 15:28:59 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-01_nonsequencener/</guid>
      <description>&lt;p&gt;The classic NER system is a model that has a lot of curated features, like lists of people, and does inference by choosing the top scoring tag sequence for the whole sentence, using Viterbi decoding.
The neural version swaps the curated features for word vectors, and viterbi inference for an LSTM (maybe with beam search).
This paper makes the argument that in reality people are very good at identifying an entity in isolation, so why do global decoding for the best tag sequence?&lt;/p&gt;
&lt;p&gt;Given that perspective, they make a model that scores every span of the sentence independently using a feedforward network.
To get an input representing context, they use a weighted sum of word embeddings, where the weights decay exponentially further from the span (FOFE = Fixed-size Ordinally Forgetting Encoding).
The authors point out that this gives a fixed length encoding that could be reversed to recover the original sequence (assuming arbitrary precision floating point numbers).
Thinking about the calculation though, a word ten positions away is having its vector scaled down by a factor of a thousand, so it probably has negligible impact on the decision.
They also apply this idea to the characters of the span itself in both directions.&lt;/p&gt;
&lt;p&gt;One tradeoff with the independent classification idea is that it can select overlapping spans.
This is a benefit in one sense, because it naturally handles nested entities (e.g. &amp;ldquo;[Member of the Order of [Australia]]&amp;quot;), but for partially overlapping spans we have to decide which to keep.
Their solution is to sort by model score and keep the higher scoring option.&lt;/p&gt;
&lt;p&gt;The experiments show this is comparable with previous work using LSTMs.
There were a few things I found interesting in the results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The FOFE encoding for characters is far worse than a CNN encoding when on their own, but give similar gains when combined with word level features. Since the FOFE essentially ignores the centre of long spans, this suggests they are both learning some representation of prefixes and suffixes.&lt;/li&gt;
&lt;li&gt;They don&amp;rsquo;t try it, but this model seems very amenable to gazetteers, which may be a way to further boost performance.&lt;/li&gt;
&lt;li&gt;They have an in-house dataset of 10,000 manually labeled documents (!), but it only gives a 3% gain on the KBP evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{xu-jiang-watcharawittayakul:2017:Long,
  author    = {Xu, Mingbin  and  Jiang, Hui  and  Watcharawittayakul, Sedtawut},
  title     = {A Local Detection Approach for Named Entity Recognition and Mention Detection},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1237--1247},
  url       = {https://aclanthology.org/P17-1114}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme (Zheng et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-30_taggingrelations/</link>
      <pubDate>Thu, 30 Nov 2017 20:01:41 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-30_taggingrelations/</guid>
      <description>&lt;p&gt;This paper considers the task of identifying named entities in a sentence and the relations between them.
The contribution is a way of formulating the task as tagging, so a bi-directional LSTM can be applied.&lt;/p&gt;
&lt;p&gt;The tags are like in NER (Begin, Inside, End, Single, Outside), but rather than Person, Location, etc, they label each entity with the relation it is participating in, and whether it is in role one or two for the relation.
Applying a two layer bidirectional LSTM to this set up gets to state-of-the-art precision on news data.
To get SotA F-score they modify the loss to place less weight on Outside tags, which raises recall at the cost of precision.&lt;/p&gt;
&lt;p&gt;One catch with this approach is handling multiple relations of the same type.
The solution here is to link pairs that are closest together (unclear what they do for nesting).
That doesn&amp;rsquo;t handle overlapping relations, which the authors say is particularly common in the BioInfer data (I&amp;rsquo;m curious how much it is hurting here too).
It&amp;rsquo;s unclear how this could be addressed without a radical redesign, since extending the tag scheme could lead to sparsity issues.&lt;/p&gt;
&lt;p&gt;I was not familiar with this data, so I looked back to the original paper the annotated test data came from: 
&lt;a href=&#34;https://aclanthology.org/P11-1055&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hoffman et al., (2011)&lt;/a&gt;.
There is no dev set, only a 395 sentence test set, so the standard practise is to use random 10% samples of the test data for development.
Also, if I understand it correctly, the data was annotated by manually confirming the output of systems, which means it will have recall errors.
If interest in this data grows, going back and annotating more seems worthwhile.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{zheng-EtAl:2017:Long,
  author    = {Zheng, Suncong  and  Wang, Feng  and  Bao, Hongyun  and  Hao, Yuexing  and  Zhou, Peng  and  Xu, Bo},
  title     = {Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1227--1236},
  url       = {https://aclanthology.org/P17-1113}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Abstractive Document Summarization with a Graph-Based Attentional Neural Model (Tan et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-29_abstractivesummarisation/</link>
      <pubDate>Wed, 29 Nov 2017 19:14:05 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-29_abstractivesummarisation/</guid>
      <description>&lt;p&gt;Most effective summarisation systems are extractive, selecting the most important sentences in a document and sticking them together.
Clearly that is not how people write summaries, but creating abstractive summaries means generating fluent language.
At the same time, most datasets are based on news text, where the first few sentences are a strong baseline summary (by design, as journalists need to assume that the reader could stop at any point).
This paper introduces several ideas to get state-of-the-art results on summarisation using an abstractive system.&lt;/p&gt;
&lt;p&gt;There are three core new ideas, one for decoding and two for the model.
The idea in decoding is a beam search in which the score is increased when adding bigrams that occur in the source but are not in the output.
In the model, they propose a new form of attention based on PageRank, similar to previous methods used for ranking sentences in summarisation.
For every pair of sentences plus the current decoder hidden vector, a similarity score is calculated ($h_1 M h_2$), where $M$ is a matrix of parameters.
This produces a matrix of similarities, which they run PageRank on with initialisation set so that all weight starts on the decoder hidden vector.
That produces a score for each input sentence, which is normalised to get attention values.
The second idea is that they don&amp;rsquo;t want to attend to the same sentence multiple times, so before normalising they subtract the previous score for that sentence (with it capped at 0 to avoid negative values).&lt;/p&gt;
&lt;p&gt;Together, these lead to state of the art results, beating both extractive and abstractive systems.
Though in human evaluation using the first three sentences as a summary remains a very strong baseline, only slightly behind this system on informativeness and ahead on coherence and fluency.
Ablation shows that the decoding idea has the biggest impact, but the graph based attention does help.
Interestingly, if the score in decoding is extremely biased to focus on the bigram addition aspect performance only decreases a little.
That may reflect the nature of the metric, which is based on ngram overlap.&lt;/p&gt;
&lt;p&gt;There are also a bunch of little details that may be crucial, like adding markers for entities (which seems like a possible space for a more elegant solution).
I&amp;rsquo;m not sure the beam search scoring idea has applications beyond summarisation, but thee modified attention might!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{tan-wan-xiao:2017:Long,
  author    = {Tan, Jiwei  and  Wan, Xiaojun  and  Xiao, Jianguo},
  title     = {Abstractive Document Summarization with a Graph-Based Attentional Neural Model},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1171--1181},
  url       = {https://aclanthology.org/P17-1108}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>SPINE: SParse Interpretable Neural Embeddings (Subramanian et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-28_interpretableembeddings/</link>
      <pubDate>Tue, 28 Nov 2017 16:51:09 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-28_interpretableembeddings/</guid>
      <description>&lt;p&gt;The first step in almost any neural network model for language is to look up a vector for each token in the input.
These vectors express relations between the words, but it is difficult to know exactly what relations.
This work proposes a way to modify a vector space of words to have more interpretable dimensions.&lt;/p&gt;
&lt;p&gt;The core idea is actually more general, it is a new loss that encourages sparsity in an auto-encoder.
In this case the model is very simple: input a word vector, apply an affine transformation and a pointwise nonlinearity, producing a hidden vector, then apply another affine transformation to get the output.
The loss is a combination of how well the input and output match (reconstruction loss), plus a function that is minimised when the average activation is below a threshold (average sparsity loss), and the new idea, a loss that is minimised at either 0 or 1 for each hidden value.
To get the hidden values to be bounded between 1 and 0, the nonlinearity used is a modified ReLU that stops increasing after reaching 1.
After training, the hidden values become the new word vectors.&lt;/p&gt;
&lt;p&gt;To evaluate interpretability they consider the top 4 words along each dimension, add a random word, and ask a person to identify the odd word out.
Using either word2vec or GloVe as the initial vectors and applying this method, the results shown a dramatic difference (~25 vs. ~70).
On downstream tasks the story is more mixed.
With 1,000 dimensional vectors, there is usually an improvement for GloVe, but not for word2vec, and the differences are generally small.
Apparently going up to 2,000 further improves interpretability scores, but &amp;lsquo;at a severe cost&amp;rsquo; for the downstream tasks.
Going the other direction, to 500, hurts interpretability, and probably doesn&amp;rsquo;t improve downstream performance (it isn&amp;rsquo;t mentioned).&lt;/p&gt;
&lt;p&gt;I would be curious to see if taking these new word vectors and applying them to a downstream task like parsing, but letting them change during training, would be beneficial.
The general idea of a sparse auto-encoder also seems cool and may have other applications.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1711.08792&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv171108792S,
  author        = {{Subramanian}, A. and {Pruthi}, D. and {Jhamtani}, H. and {Berg-Kirkpatrick}, T. and {Hovy}, E.},
  title         = {SPINE: SParse Interpretable Neural Embeddings},
  journal       = {ArXiv e-prints},
  archivePrefix = {arXiv},
  eprint        = {1711.08792},
  primaryClass  = {cs.CL},
  year          = {2017},
  month         = {November},
  url           = {https://arxiv.org/abs/1711.08792},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Ordinal Common-sense Inference (Zhang et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-27_commonsense/</link>
      <pubDate>Mon, 27 Nov 2017 11:21:45 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-27_commonsense/</guid>
      <description>&lt;p&gt;When people read a sentence they form an entire world around it, making inferences about unwritten properties based on their prior knowledge.
If we want NLP systems to do the same, we need data to train and test this common sense aspect of language understanding.&lt;/p&gt;
&lt;p&gt;This paper is about a new dataset of automatically generated sentence pairs with human ratings.
The ratings indicate that given the first sentence, the second sentence is either very likely, likely, plausible, technically possible, or impossible.
These ratings are crowdsourced, using the median of three ratings per example.
The pay rates are fairly low, at $3.45 / hour (1.99c / example and 20.71 seconds / example), though it&amp;rsquo;s possible that the time is being skewed by outliers, and it&amp;rsquo;s unclear exactly how pay was determined (does this include Amazon&amp;rsquo;s cut? Why is it an average cost per example, rather than just the cost?).&lt;/p&gt;
&lt;p&gt;The main contribution is the novel way of generating the sentences.
For each prompt sentence, an argument is chosen, and then a hypothesis is generated in one of three ways (all trained with Gigaword).
(1) A sequence-to-sequence model takes the full sentence as input and generates a sentence.
(2) The same as (1), but with only the argument provided.
(3) A sentence is sampled from templates generated by abstraction of sentences in the training data.
Together these produce a diverse set of examples that get a range of ratings, with only &amp;lsquo;likely&amp;rsquo; being somewhat rarer.
They also labeled some pairs from SNLI and COPA, to enable analysis of how this task compares.&lt;/p&gt;
&lt;p&gt;They also provide a set of baselines for the new task.
Using the baselines, they show that the generated sentences are somewhat more difficult than the pairs from existing datasets.
The standard metrics proposed are MSE and Spearman&amp;rsquo;s Rho (both necessary because otherwise always guessing the middle would get an MSE better than any of the proposed baselines).
Interestingly, regression does quite a bit better than a set of one-vs-all SVMs on MSE, and also slightly better on rho (I&amp;rsquo;m surprised because while there is an ordinal scale, it doesn&amp;rsquo;t feel like it should have a strong continuous interpretation).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1082&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1082,
	author = {Zhang, Sheng  and Rudinger, Rachel  and Duh, Kevin  and Van Durme, Benjamin },
	title = {Ordinal Common-sense Inference},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	keywords = {},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1082},
	pages = {379--395}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Error-repair Dependency Parsing for Ungrammatical Texts (Sakaguchi et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-22_errorrepairparsing/</link>
      <pubDate>Wed, 22 Nov 2017 15:48:53 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-22_errorrepairparsing/</guid>
      <description>&lt;p&gt;This work presents a system that parses sentences and identifies grammatical errors simultaneously.
It&amp;rsquo;s an intuitive combination - a syntactic model should assign higher probability to a parse for a fixed version of a sentence than the one with a mistake.&lt;/p&gt;
&lt;p&gt;They build on an incremental &amp;lsquo;easy-first&amp;rsquo; dependency parsing approach.
Easy-First parsing starts with the set of words in the sentence and allows an edge to be created between any adjacent pair of words.
Once an edge is created, the child is hidden beneath its parent, so now the parent is effectively adjacent to a word slightly further away.
Then the process repeats, until there is only one word left (the root of the sentence).
In a way it is like following a dynamic program, but with only a single state that ties together multiple cells.&lt;/p&gt;
&lt;p&gt;The change in this paper is the addition of actions that insert a word, delete a word, or alter a word.
To make it work, there are constraints to avoid cycles of repeated actions (e.g. insert-delete-insert-delete&amp;hellip;), and on the sets of allowed word substitutions.
To produce additional training data, a tool is used to inject errors into grammatical text.
On error detection, this approach does lead to improvements, though it changes a relatively small number of the sentences.
On dependency parsing it is (unsurprisingly) worse than a baseline system on grammatical text.
It does perform better on ungrammatical text, though the data is generated using the same process as the training data, creating a bias in the system&amp;rsquo;s favour.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-2030&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{sakaguchi-post-vandurme:2017:Short,
  author    = {Sakaguchi, Keisuke  and  Post, Matt  and  Van Durme, Benjamin},
  title     = {Error-repair Dependency Parsing for Ungrammatical Texts},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {189--195},
  url       = {https://aclanthology.org/P17-2030}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-21_multiinputattention/</link>
      <pubDate>Tue, 21 Nov 2017 16:42:19 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-21_multiinputattention/</guid>
      <description>&lt;p&gt;Attention, a weighted average over vectors with weights determined based on context (usually decoder state), has proven effective in many NLP tasks.
There are several variants, and this paper adds new types that address the question of how to apply attention to different sources at the same time, such as text and an image.&lt;/p&gt;
&lt;p&gt;They consider three general versions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Concatenation, just do attention separately then concatenate the vectors from the input sources&lt;/li&gt;
&lt;li&gt;Flat, do the weighted average over all of the inputs&lt;/li&gt;
&lt;li&gt;Hierarchical, do attention separately, but then combine the vectors with another phase of attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They also explore two variations that are orthogonal to the list above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first step and the last step in attention both involve the input vectors being multiplied by a weight matrix. Should that matrix be shared for the two steps, or different? (the first informs the decision of what to give high weight in the average, the second determines what is being averaged over)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;sentinel gates&lt;/em&gt;, a modification to the way the inputs and context vector are combined that allow one or the other to be ignored.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They consider two tasks, (1) translation when both an image and source sentence are given, (2) post-editing a translated sentence with the original source given.
The results show fairly clear trends, though the systems are not great compared to baselines (worse than a text only baseline for the first, and only slightly better than a direct MT system for the second).
The trends are that hierarchical is best, the sentinel doesn&amp;rsquo;t help, and it is better to not share weights (though I wonder if that would be true when controlling for the total number of parameters).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-2031&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{libovicky-helcl:2017:Short,
  author    = {Libovick\&#39;{y}, Jind\v{r}ich  and  Helcl, Jind\v{r}ich},
  title     = {Attention Strategies for Multi-Source Sequence-to-Sequence Learning},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {196--202},
  url       = {https://aclanthology.org/P17-2031}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Robust Incremental Neural Semantic Graph Parsing (Buys et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-20_mrsparser/</link>
      <pubDate>Mon, 20 Nov 2017 10:13:12 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-20_mrsparser/</guid>
      <description>&lt;p&gt;Like the 
&lt;a href=&#34;https://www.jkk.name/post/2017-11-16_ucca/&#34;&gt;UCCA parser&lt;/a&gt;, this paper explores a transition-based neural model for semantic parsing, but for Minimal Recursion Semantics instead of Universal Conceptual Cognitive Annotation.
Comparing MRS and UCCA, every word gets a non-terminal symbol in MRS, plus additional non-terminals for phenomena like quantification, while UCCA only introduces them for special cases like linking to a coordination.
Both have discontinuous graph structures, creating a challenge for most parsers.&lt;/p&gt;
&lt;p&gt;The UCCA and MRS parsers extend the basic shift-reduce transitions in different ways.
Here, crossing edges can be added with a transition that forms edges between the front of the buffer and a word anywhere in the stack, while the UCCA parser used swapping and a additional reduce actions for graph edges.
The models are similar, both using a form of stack-RNN, but with different structures (partly as a result of the different transition schemes).
The results in this case are not state-of-the-art, though this task has received more attention, and the data is slightly biased (the parser that does better, ACE, is based on the grammar that was used to determine which sentences to include).
However, the system can also be applied to AMR, and does fairly well, better than other neural AMR parsers at the time (and more recent ideas for improvements are large orthogonal).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1112&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{buys-blunsom:2017:Long,
  author    = {Buys, Jan  and  Blunsom, Phil},
  title     = {Robust Incremental Neural Semantic Graph Parsing},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1215--1226},
  url       = {https://aclanthology.org/P17-1112}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Two-Stage Parsing Method for Text-Level Discourse Analysis (Wang et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-17_twostagediscourseparsing/</link>
      <pubDate>Fri, 17 Nov 2017 18:40:20 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-17_twostagediscourseparsing/</guid>
      <description>&lt;p&gt;Discourse parsing for Rhetorical Structure Theory is difficult partly because it involves a range of relation types at different scales (within and between sentences) and partly because there is relatively little annotated data available.
To deal with the limited data, this paper breaks the task into two parts: (1) identify relations, (2) assign labels.
Their system is state-of-the-art, and an ablation shows that the division of tasks helps performance.
They also divide up the labeling step to have different classifiers for within sentences, between sentences in the same paragraph, and between paragraphs, which also helps a little.&lt;/p&gt;
&lt;p&gt;I find the second improvement surprising, since an expanded feature set for a single classifier would be able to emulate their multi-classifier model, while having the advantage of sharing information between classes.
The first improvement is more intuitive (a denser space makes for an easier problem), though I wonder whether this will be one point on the back-and-forth that usually occurs between sequential and joint models (with joint models usually winning in the end).
This paper also continues the trend of transition-based inference applying effectively to tasks, which makes sense if our models are getting good enough that search errors are not a major issue.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-2029&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{wang-li-wang:2017:Short,
  author    = {Wang, Yizhong  and  Li, Sujian  and  Wang, Houfeng},
  title     = {A Two-Stage Parsing Method for Text-Level Discourse Analysis},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {184--188},
  url       = {https://aclanthology.org/P17-2029}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Transition-Based Directed Acyclic Graph Parser for UCCA (Hershcovich et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-16_ucca/</link>
      <pubDate>Thu, 16 Nov 2017 17:24:59 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-16_ucca/</guid>
      <description>&lt;p&gt;Over the last few years interest has risen in parsing structures other than projective trees (including my dissertation!).
There are now a range of different datasets with annotations for syntactic and/or semantic structure that include discontinuous constituents and graphs.
This paper looks at UCCA, a proposed formalism that is somewhat similar to SRL, with non-terminals included to allow for easier handling of cases like coordination.&lt;/p&gt;
&lt;p&gt;The parser is a transition based, with a transition system that covers all the structural phenomena in UCCA: non-terminals, discontinuous spans, and multiple parents.
The key to consistent multiple parents is distinguishing the addition of edges that are the primary parent (to prevent multiple being added).
To get discontinuity, they use a swap operation.
They consider a range of models, including both linear and neural network examples.&lt;/p&gt;
&lt;p&gt;The dataset is relatively small, with only 4,268 training sentences, and the task is hard, so performance is relatively low (50 - 75 for primary edges, 20-50 for others).
The neural model consistently beats the linear ones, particularly for the non-primary edges.
Comparing to other standard parsers (retrained on this data), the ability to generate the full space of structures makes a big difference.&lt;/p&gt;
&lt;p&gt;It would be interesting to see coverage of this data for one-endpoint crossing graphs.
If it is high, then my own parser could be applied fairly directly!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1104&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hershcovich-abend-rappoport:2017:Long,
  author    = {Hershcovich, Daniel  and  Abend, Omri  and  Rappoport, Ari},
  title     = {A Transition-Based Directed Acyclic Graph Parser for UCCA},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1127--1138},
  url       = {https://aclanthology.org/P17-1104}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-15_entityvectors/</link>
      <pubDate>Wed, 15 Nov 2017 18:01:27 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-15_entityvectors/</guid>
      <description>&lt;p&gt;Since word2vec was released there have been a series of X2vec papers, though none have had the success of word vectors.
In this case the idea is to represent entities and chunks of text (words, sentences, paragraphs).&lt;/p&gt;
&lt;p&gt;Entities are represented with vectors.
To get the vector for a chunk of text, they:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sum word vectors for the text.&lt;/li&gt;
&lt;li&gt;Rescale to be of unit length.&lt;/li&gt;
&lt;li&gt;Multiply by a weight matrix and add a bias.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then to learn these, negative log likelihood is used, where the probability is defined as a softmax over the dot product between entity and text vectors.
The data is a portion of Wikipedia annotated with entities as indicated by links (plus they say the entity the page is about is implicitly part of every sentence).&lt;/p&gt;
&lt;p&gt;With these new vectors in hand, they try textual similarity, with strong results.
They also build a very simple entity linking system, a feed-forward network with these representations plus a few other features, and beat all prior work.
Similarly
They apply the same modeling approach to Quizball QA, also with strong results.&lt;/p&gt;
&lt;p&gt;The simplicity and effectiveness of the model really is impressive.
Some qualitative examples are included, but hard to find trends in.
It does seem like a more reasonable vector learning approach than skip-thought and other similar approaches that rely only on text context - the entities provide something different, but clearly closely related.
That said, I feel like more ablation is needed to see what role each of these pieces is playing (are they learning better vectors, or using them in a way that is more effective? Or both?).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1065&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1065,
	author = {Yamada, Ikuya  and Shindo, Hiroyuki  and Takeda, Hideaki  and Takefuji, Yoshiyasu },
	title = {Learning Distributed Representations of Texts and Entities from Knowledge Base},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1065},
	pages = {397--411}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>In-Order Transition-based Constituent Parsing (Liu et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-14_inorderparsing/</link>
      <pubDate>Tue, 14 Nov 2017 14:10:39 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-14_inorderparsing/</guid>
      <description>&lt;p&gt;Shift-reduce constituency parsing incrementally builds the parse either bottom-up or top-down.
The difference is whether a non-terminal is placed on the stack before or after the words that it spans.
This corresponds to two forms of depth-first traversal of the tree: pre-order or post-order.&lt;/p&gt;
&lt;p&gt;The idea in this paper is to do an in-order traversal, which in a binary tree means traversing the left child of a node, then the node, then its right child.
In this context that means putting the non-terminal symbol on the stack after the first word it spans, but before the rest.
The model follows the stack-LSTM approach of Dyer et al., with non-terminals always fed into the LSTM first during composition, regardless of where it was inserted into the stack.&lt;/p&gt;
&lt;p&gt;This leads to a 0.5 F1 gain on standard parsing metrics, with no hyperparameter tuning.
High-level error analysis seems to show it just does better everywhere.
I wonder whether further gains could be realised with a label-sensitive ordering.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1199,
	author = {Liu, Jiangming  and Zhang, Yue },
	title = {In-Order Transition-based Constituent Parsing},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1199},
	pages = {413--424}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog (Kottur et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-13_languagegame/</link>
      <pubDate>Mon, 13 Nov 2017 09:47:08 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-13_languagegame/</guid>
      <description>&lt;p&gt;In reference games, two players communicate in a shared world with the goal of one learning what the other is referring to.
Their small scale and clear success criteria make them a convenient testbed for dialogue agents, going back decades, with recent work focusing on neural approaches.
This paper considers a simple game and constrains models in various ways to improve performance and see how their communication varies, a line of work also appearing in recent papers by Jacob Andreas (
&lt;a href=&#34;https://aclanthology.org/P17-1022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACL 2017&lt;/a&gt;, 
&lt;a href=&#34;https://aclanthology.org/D17-1311.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EMNLP 2017&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The game in this case is to find out two properties of an object, where there are three possible properties, each with four possible values.
Given enough flexibility, models will explicitly encode every possible structure of the world as a separate symbol, which does not generalise well.
Limiting the vocabulary to one symbol per property and one per value helps, but in this particular game there are only 3 possible questions, and over two turns of dialogue the 12 value words are sufficient to encode the space.
Limiting even further, to 4 words for values and providing each turn in isolation to the answerer does lead to some compositionality, but clearly not full compositionality as they still make errors on unseen combinations of the inputs.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a short paper, so they can only do so much, but some experiments I am curious about are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decrease the questioner vocabulary to 2. This avoids the problem that the questioner can express the task in one step by saying what is not needed. It&amp;rsquo;s still doable, by defining an order for questions, e.g. ask about attribute A vs. B first, then in the second step ask about either C or the other option from the first step. This is a little weird as symbols need to mean different things at different time steps, but would be interesting.&lt;/li&gt;
&lt;li&gt;Increase the number of attributes to 4. This also avoids the task expression problem, by forcing there to be compositionality on the questioner side (watching the video of the talk, someone asked this in the question time, and they didn&amp;rsquo;t know).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kottur-EtAl:2017:EMNLP2017,
  author    = {Kottur, Satwik  and  Moura, Jos\&#39;{e}  and  Lee, Stefan  and  Batra, Dhruv},
  title     = {Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2952--2957},
  url       = {https://aclanthology.org/D17-1320}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-10_kginlstm/</link>
      <pubDate>Fri, 10 Nov 2017 15:37:15 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-10_kginlstm/</guid>
      <description>&lt;p&gt;Structured information sources have been effectively used for entity detection and typing in linear models with the information encoded as binary features.
This paper looks at how to integrate vector representations of structured information into an LSTM.
The solution is an additional processing step during output generation, in which the vectors for relevant entities in the structured data are combined with the standard LSTM output (note, they do not affect the cell itself, so the information is not passed on through the recurrence).&lt;/p&gt;
&lt;p&gt;In this case the structured information is a set of tuples forming a graph of relations between entities, from either NELL or WordNet.
The actual encoding of entities is an application of prior work; vectors representing tuples are trained with the objective that the score for any tuple is higher than made-up tuples (where the score is $v_a M_r v_b$ for entities $a$ and $b$ in relation $r$).
The set of relevant entities for a particular word in the sentence is obtained by string matching, and then attention is used to combine them.
There is also a kind of gating mechanism to choose how big a role the entities play in the prediction, using a combination of the input, hidden state, and cell state.&lt;/p&gt;
&lt;p&gt;The results are interesting not only because this method helps, but because of how well the standard LSTM does on this task, matching or exceeding prior results.
This is even more impressive given how small ACE is (if I remember correctly).
The other key observations are that having a sequence level loss (using a CRF) helps, and NELL and WordNet seem to be providing different types of information (as using both leads to further improvements).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1132&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{yang-mitchell:2017:Long,
  author    = {Yang, Bishan  and  Mitchell, Tom},
  title     = {Leveraging Knowledge Bases in LSTMs for Improving Machine Reading},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1436--1446},
  url       = {https://aclanthology.org/P17-1132}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-09_framesdataset/</link>
      <pubDate>Thu, 09 Nov 2017 19:47:08 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-09_framesdataset/</guid>
      <description>&lt;p&gt;Another paper about a dataset of dialogues, but this time with structure.
Like the paper from yesterday, the aim is a dataset of task-oriented conversations, but with more complexity than prior work.
The difference is that this work includes a structured representation of the state of the conversation: frames.&lt;/p&gt;
&lt;p&gt;A frame is essentially a tuple describing a query, e.g. (Destination: Sydney, Origin: Ann Arbor, price: 1500 USD).
There are multiple frames in a dialogue (a departure from DSTC tasks), and utterances are labeled with dialogue acts that capture modifications to the frames as well as references to them.
This structure sounds fairly general, though the focus here was on vacation planning, where the user is buying a package.
The setup doesn&amp;rsquo;t maximise the potential complexity though, as there are a small number of set packages available, rather than the complex tradeoffs of flight+hotel combinations that exist in practise.
Looking at the example dialogues in the paper, it has complete sentences of some complexity.
One thing I&amp;rsquo;m still curious about is disagreements between annotators, as for the complete task the score was 0.62 +/- 5 (with dialogue acts being trickier than slot values, and no scores for frame references on their own).&lt;/p&gt;
&lt;p&gt;Comparing to the Stanford dataset this is smaller (11k vs. 1.4k), but has more turns per dialogue (11 vs. 15) and probably longer turns too, judging by the examples.
The tasks are completely different, but both come with small tables of information that are private to the two participants and required for almost every turn in the conversation.
Evaluating on both could be a great way to show the flexibility of a dialogue system, but the lack of frames for the Stanford data and the difficulty of running a human evaluation for this data limits the feasible types of multi-domain experiments.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/W17-5526&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{elasri-EtAl:2017:W17-55,
  author    = {El Asri, Layla  and  Schulz, Hannes  and  Sharma, Shikhar  and  Zumer, Jeremie  and  Harris, Justin  and  Fine, Emery  and  Mehrotra, Rahul  and  Suleman, Kaheer},
  title     = {Frames: a corpus for adding memory to goal-oriented dialogue systems},
  booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  month     = {August},
  year      = {2017},
  address   = {Saarbrucken, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {207--219},
  url       = {https://aclanthology.org/W17-5526}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-08_graphdialogue/</link>
      <pubDate>Wed, 08 Nov 2017 18:46:04 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-08_graphdialogue/</guid>
      <description>&lt;p&gt;Task-oriented dialogue systems are often focused on a very narrow task, to the point where the state can be described completely with a tuple (e.g. preferences for a restaurant).
This paper sets up a more challenging task with more complex language use, while still having a specific goal and directly relevant structured information.
They collected 11,000 dialogues, where two people have private lists of friends and are trying to identify which friend they have in common.
While this is a lot of data, the mechanical turk workers are clearly moving fast, with dialogues taking 1.5 minutes on average, and in 18% of cases they get the friend wrong.&lt;/p&gt;
&lt;p&gt;The algorithmic contribution is that the lists of people are represented as a graph, where nodes are properties like company and hobby.
The graph is used to generate vectors for each person by running a form of message passing over its structure.
During generation, the LSTM uses attention over these vectors to inform the output choice.&lt;/p&gt;
&lt;p&gt;A few interesting things in the output:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are cases where the output is incorrect, as in, says a fact about the structured information / knowledge base that is false.&lt;/li&gt;
&lt;li&gt;Evaluation is tricky, and over the metrics they consider sometimes this wins, but sometimes the baseline system (rules) does better. In particular, success on bot-bot evaluation doesn&amp;rsquo;t seem to clearly transfer to bot-human experiments.&lt;/li&gt;
&lt;li&gt;The utterances are very fluent, but that may be because it&amp;rsquo;s essentially copying from the training data. It looks like there is diversity in the dataset, but a lot of utterances do fit a template of &amp;ldquo;I have X who Y&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1162&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{he-EtAl:2017:Long4,
  author    = {He, He  and  Balakrishnan, Anusha  and  Eric, Mihail  and  Liang, Percy},
  title     = {Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1766--1776},
  abstract  = {We study a \emph{symmetric collaborative dialogue} setting
	in which two agents, each with private knowledge,
	must strategically communicate to achieve a common goal.
	The open-ended dialogue state in this setting poses new challenges for existing
	dialogue systems.
	We collected a dataset of 11K human-human dialogues,
	which exhibits interesting lexical, semantic, and strategic elements.
	To model
	both structured knowledge and unstructured language,
	we propose a neural model with dynamic knowledge graph embeddings
	that evolve as the dialogue progresses.
	Automatic and human evaluations show that our model is both more effective
	at achieving the goal and more human-like than baseline neural and rule-based
	models.},
  url       = {https://aclanthology.org/P17-1162}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-07_spineparsinglstm/</link>
      <pubDate>Tue, 07 Nov 2017 20:42:45 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-07_spineparsinglstm/</guid>
      <description>&lt;p&gt;This paper brings together work on neural dependency parsing with the idea of non-terminal spines as a way to represent constituency structure.
Within the transition parsing inference process they can naturally fit the generation of a new spines by gradually building up the spine, which makes for a very elegant inference process.&lt;/p&gt;
&lt;p&gt;Surprisingly, it doesn&amp;rsquo;t seem to matter what head choices are used to generate the spines (they tried leftmost word, rightmost word, and two standard schemes).
This contrasts with my own observations that the choice of head had a big impact (0.5 F) on accuracy.
I think the incrementally-built spines are the key difference.
Decisions about higher up in the spine are difficult to make when looking at a single word, but with the incremental construction there is information about a larger context.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/W17-6316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{ballesteros-carreras:2017:IWPT,
  author    = {Ballesteros, Miguel  and  Carreras, Xavier},
  title     = {Arc-Standard Spinal Parsing with Stack-LSTMs},
  booktitle = {Proceedings of the 15th International Conference on Parsing Technologies},
  month     = {September},
  year      = {2017},
  address   = {Pisa, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {115--121},
  url       = {https://aclanthology.org/W17-6316}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)</title>
      <link>https://www.jkk.name/post/2017-11-06_literarycharacters/</link>
      <pubDate>Mon, 06 Nov 2017 20:16:28 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-06_literarycharacters/</guid>
      <description>&lt;p&gt;NLP tools seem like a natural fit for literary analysis, but the domain shift from news text is large enough to degrade performance to the point where tools are not useful.
Here the specific question is how many characters are there in novels?
NER + coreference would seem to be enough, but an off-the-shelf system fares poorly (and I doubt improvements in the last few years would change that story).&lt;/p&gt;
&lt;p&gt;The solution is to craft a kind of coreference system focused on getting all of the characters, but not necessarily every mention.
The most interesting new piece is how they identify rare characters: identify arguments of verbs that usually take people.
With this tool in hand they analyse patterns of character use over time to test hypotheses from literary analysis.&lt;/p&gt;
&lt;p&gt;Another key piece of this work was a tool to annotate a collection of books with character occurrences.
CHARLES, their tool, is built on top of 
&lt;a href=&#34;http://brat.nlplab.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;brat&lt;/a&gt;, adding features to help multiple annotators coordinate labels (specifically handling the case of new character identification, which modifies the set of linkable entities).&lt;/p&gt;
&lt;p&gt;Finally, they released the character lists identified for the novels considered (
&lt;a href=&#34;https://aclanthology.org/attachments/D/D15/D15-1088.Attachment.zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).
It would be interesting to modify a coreference resolution system to process these books, taking advantage of that information!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D15-1088&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2016/pdf/1130_Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annotation Tool Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{vala-EtAl:2015:EMNLP,
  author    = {Vala, Hardik  and  Jurgens, David  and  Piper, Andrew  and  Ruths, Derek},
  title     = {Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {769--774},
  url       = {https://aclanthology.org/D15-1088}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-03_discourserelations/</link>
      <pubDate>Fri, 03 Nov 2017 15:40:32 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-11-03_discourserelations/</guid>
      <description>&lt;p&gt;Automatically generating high quality meeting notes and summaries would be awesome, but involves solving many challenges.
Here, they assume speech recognition is already done and we also know the structure over utterances indicating which previous utterance each is a response to.
The task is to label each of those utterance-utterance pairs with a type (e.g. elaboration) and to select the key phrase of each utterance.&lt;/p&gt;
&lt;p&gt;Two datasets are used, the AMI and ICSO meeting corpora, which have all of the required information.
The new idea here is to jointly model the choice of link label and the key phrase, which is intuitive.
To show the value of joint modeling they run a version of the system with the same linear model, but with independent inference, which performs quite a bit worse.&lt;/p&gt;
&lt;p&gt;One neat follow up is that by combining the key phrases into a list you get a form of summary.
According to automatic metrics it is quite a bit better than running the summarisation system they compare to, though it&amp;rsquo;s still a long way from a human summary.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1090&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{qin-wang-kim:2017:Long,
  author    = {Qin, Kechen  and  Wang, Lu  and  Kim, Joseph},
  title     = {Joint Modeling of Content and Discourse Relations in Dialogues},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {974--984},
  url       = {https://aclanthology.org/P17-1090}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Shazeer et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-01_mixtureofexperts/</link>
      <pubDate>Wed, 01 Nov 2017 21:57:27 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-11-01_mixtureofexperts/</guid>
      <description>&lt;p&gt;Mixture of experts can be seen as an ensemble approach in which we assume that each of our models is effective under different circumstances and so we combine them by switching between which we use to make our decision.
From this perspective the idea can be applied to any set of models, but here the idea is to train (1) the expert models, (2) our method of choosing between them, and (3) a set of common model components, all at the same time.&lt;/p&gt;
&lt;p&gt;The particular set up here is that they modify a series of LSTM layers, adding a new layer in between each pair of LSTMs.
The new layer has a set of small feed-forward networks (the experts) and an even simpler network that chooses which expert to use.
One big benefit of this is that a lot of computation can be avoided when we know some of the small feed-forward components are going to be ignored.
As a result, they can scale up to massive networks while still having reasonable runtimes.&lt;/p&gt;
&lt;p&gt;Some key things to make this all work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enough machines to train it! Also, there is a careful mixture of data and model parallelism during training.&lt;/li&gt;
&lt;li&gt;Some noise in the expert selection process&lt;/li&gt;
&lt;li&gt;A loss that directly encourages the use of multiple experts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One thing mentioned in passing is how this relates to a form of dropout (which can be viewed as training a set of overlapping experts, kind of).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=B1ckMDqlg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{45929,
	title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author    = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	year  = {2017},
  booktitle = {ICLR},
	URL = {https://openreview.net/pdf?id=B1ckMDqlg},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)</title>
      <link>https://www.jkk.name/post/2017-10-31_realtimecaptioning/</link>
      <pubDate>Tue, 31 Oct 2017 13:23:13 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-31_realtimecaptioning/</guid>
      <description>&lt;p&gt;For any given task, automatic systems are fast, while annotation is accurate.
This work is about bridging that gap to provide a way for a team of annotators to produce real-time high quality labels.
The specific application is speech transcription, in which automatic systems are not accurate, while average people are slow (experts can transcribe in real time, but are very expensive).&lt;/p&gt;
&lt;p&gt;The solution is to carefully break up the task and combine annotations back together.
To get it to work well there are a range of subtle design decisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;People hear the entire audio stream, but with their section at normal volume and the rest quieter. This allows them to focus their effort while still understanding the context.&lt;/li&gt;
&lt;li&gt;The alignment process combines annotations with guidance from a language model and a model of typos based on keyboard layout.&lt;/li&gt;
&lt;li&gt;Words are locked in shortly after being typed, to encourage workers to go on rather than revising their own errors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Follow up work added several more ideas to improve performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time warping, slowing down to half speed for their section, then going to 1.5x for the rest.&lt;/li&gt;
&lt;li&gt;Use ASR as well, either as another worker (with very uncorrelated errors), or as a starting point for human editing (or vice versa).&lt;/li&gt;
&lt;li&gt;Use A* search rather than a greedy algorithm for the alignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Performance does not reach the level of a professional, but is far better than ASR.
From the paper it&amp;rsquo;s tricky to see a final cost, but it is certainly far lower than the professional.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://doi.acm.org/10.1145/2380116.2380122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Lasecki:2012:RCG:2380116.2380122,
 author = {Lasecki, Walter and Miller, Christopher and Sadilek, Adam and Abumoussa, Andrew and Borrello, Donato and Kushalnagar, Raja and Bigham, Jeffrey},
 title = {Real-time Captioning by Groups of Non-experts},
 booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
 series = {UIST &#39;12},
 year = {2012},
 isbn = {978-1-4503-1580-7},
 location = {Cambridge, Massachusetts, USA},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2380116.2380122},
 doi = {10.1145/2380116.2380122},
 acmid = {2380122},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {captioning, crowdsourcing, deaf, hard of hearing, real-time, text alignment, transcription},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-10-30_neuralsequence/</link>
      <pubDate>Mon, 30 Oct 2017 13:28:30 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-30_neuralsequence/</guid>
      <description>&lt;p&gt;Language is bursty, with rare words occurring in clumps, the simplest example being an unusual name that occurs a lot in one news article, but not in other articles.
This paper is about how to modify a neural language model to take this into consideration, by adapting the model over time.&lt;/p&gt;
&lt;p&gt;The main idea is to have one model of overall word usage (global) and a separate model that shifts over time to take into consideration the current text (local).
The idea of adapting is not new (as the paper makes clear), but the key here is an update rule that is a modified form of RMSprop, combining the local and global models.
It also seems like performing the updates after every 5 words is important, balancing frequency with informativeness (though no ablation of frequencies is presented).
Conveniently, this is orthogonal to many other ideas and can essentially be stapled on top of a range of sequential architectures, consistently leading to improvements.&lt;/p&gt;
&lt;p&gt;One question left open is how this would work in generation.
The paper describes how it could be applied and could provide improvements, but it also seems likely to risk the repetitive outputs seen in many dialogue systems.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1709.07432.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv170907432K,
   author = {{Krause}, B. and {Kahembwe}, E. and {Murray}, I. and {Renals}, S.},
    title = &amp;quot;{Dynamic Evaluation of Neural Sequence Models}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1709.07432},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
     year = 2017,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170907432K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Searching for Activation Functions (Ramachandran et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-10-27_swishactivation/</link>
      <pubDate>Fri, 27 Oct 2017 11:22:45 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-27_swishactivation/</guid>
      <description>&lt;h2 id=&#34;update&#34;&gt;Update&lt;/h2&gt;
&lt;p&gt;After prior work came to light that uses the same non-linearity, this paper was updated to focus more on the search process used to investigate the space.
One new takeaway for me was how diverse the effective activation functions were (see figures in the new version of the paper).&lt;/p&gt;
&lt;h2 id=&#34;original-post&#34;&gt;Original Post&lt;/h2&gt;
&lt;p&gt;Non-linear functions are the key to the representation power of neural networks.
Many different ones have been proposed, though it is difficult to make theoretical claims of their properties and so the choice of which to use is generally empirical.
This paper proposes a new non-linearity, $\text{swish}(x) = x \cdot \text{sigmoid}(x)$.&lt;/p&gt;
&lt;p&gt;Interestingly, it was chosen by a combination of exhaustive search and search with reinforcement learning across a range of functions, evaluating on CIFAR-10 with a small model.
ReLU variants were consistently second-best to swish variants, and generally the more complicated functions performed worse.
They do mention two functions that performed well, but didn&amp;rsquo;t generalise: $\text{cos}(x) - x$ and $\text{max}(x, \text{tanh}(x))$, which look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/activation-functions.png&#34; alt=&#34;Four activation functions&#34;&gt;&lt;/p&gt;
&lt;p&gt;In a range of experiments in vision and machine translation swish does at least as well or slightly better than the alternatives.
It also seems more robust to network depth and to work across different network structures.
As for why it works so well, there are two main ideas: (1) it adds smoothness to the ReLU, (2) it has some sensitivity to negative inputs.
Both of these seem particularly important at the start of training.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1710.05941.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv171005941R,
   author = {{Ramachandran}, P. and {Zoph}, B. and {Le}, Q.~V.},
    title = &amp;quot;{Swish: a Self-Gated Activation Function}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1710.05941},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171005941R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)</title>
      <link>https://www.jkk.name/post/2017-10-26_multimodalwordembeddings/</link>
      <pubDate>Thu, 26 Oct 2017 20:47:12 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-26_multimodalwordembeddings/</guid>
      <description>&lt;p&gt;Word2Vec and other approaches provide a single vector representing a word&amp;rsquo;s meaning, giving words spatially defined relationships capturing relatedness.
A natural extension is to consider regions in that space and allow some words to take up larger or smaller regions.
Another natural idea is to allow a single word to have multiple representations, to capture the different senses.
This paper considers both of those ideas, using multiple gaussian distributions per word.&lt;/p&gt;
&lt;p&gt;Using gaussians has the nice property that there is a closed form for calculating the amount of overlap between them, which is used as a measure of similarity.
Following ideas from word2vec, during learning the aim is to increase similarity between words that occur together and decrease it between random pairs that do not occur together.
Once the word representations are learned, KL divergence is used for similarity, along with the standard approaches that only look at the gaussian centres.&lt;/p&gt;
&lt;p&gt;In practise, two spherical distributions per word is sufficient.
Performance is better than word2vec and several other approaches for multi-sense word embeddings.
There was one puzzling line about the model suffering larger variance problems, but it was not quantified.&lt;/p&gt;
&lt;p&gt;It would be very interesting to inject some knowledge, such as from WordNet, to guide the number of gaussians per word, rather than giving them all N.
The paper also doesn&amp;rsquo;t get into details about the learned space, for example, are the two senses often far apart or close together? (in the latter case it is learning a slightly non-linear spatial representation).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1151&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{athiwaratkun-wilson:2017:Long,
  author    = {Athiwaratkun, Ben  and  Wilson, Andrew},
  title     = {Multimodal Word Distributions},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1645--1656},
  url       = {https://aclanthology.org/P17-1151}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice (Mi and Huang, 2015)</title>
      <link>https://www.jkk.name/post/2017-10-25_shiftreducedp/</link>
      <pubDate>Wed, 25 Oct 2017 14:44:13 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-25_shiftreducedp/</guid>
      <description>&lt;p&gt;This paper is a follow-up to yesterday&amp;rsquo;s, where the approach is implemented and evaluated on English and Chinese, with very strong results.
The novel contribution is the idea of introducing alternating steps in the dynamic program to do unary steps (not a novel idea in general, but novel in its application to the dynamic programming version of shift-reduce parsing).&lt;/p&gt;
&lt;p&gt;What I found interesting here were the clear benefits of the dynamic program (DP) version.
One way of viewing this is that the DP gives a more intelligent type of beam, avoiding the issue where the beam is filled with minor variations on a theme.
Results are given for various beam sizes in both approaches, but it would be interesting to see a graph where the x-axis is number of items built.
I suspect in that situation, the gap would be smaller.
On speed, there is the nice theoretical bound of $O(n)$ for this approach, but that obscures a grammar constant related to the item structure.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/N15-1108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{mi-huang:2015:NAACL-HLT,
  author    = {Mi, Haitao  and  Huang, Liang},
  title     = {Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {May--June},
  year      = {2015},
  address   = {Denver, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {1030--1035},
  url       = {https://aclanthology.org/N15-1108}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)</title>
      <link>https://www.jkk.name/post/2017-10-24_dynamictransition/</link>
      <pubDate>Tue, 24 Oct 2017 13:06:04 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-24_dynamictransition/</guid>
      <description>&lt;p&gt;This paper from 2011 explores the relationship between transition based parsing and dynamic programming based parsing.
They show how to convert common dependency parsing systems (Arc-Standard and Arc-Eager) into dynamic programs, and how doing the reverse on a dynamic program gives the Arc-Hybrid approach (which has since been used in many places, and is now joined by additional systems like Arc-Swift).&lt;/p&gt;
&lt;p&gt;The benefit of this transformation is that we can find exact answers without massive beams.
The drawback is that the feature set is restricted.
This paper is theoretical, so it doesn&amp;rsquo;t give a direct measure of this tradeoff, though 
&lt;a href=&#34;https://aclanthology.org/D/D13/D13-1071.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;follow up work&lt;/a&gt; shows that avoiding search errors is indeed beneficial.&lt;/p&gt;
&lt;p&gt;With all of the positive results using neural networks for multi-task learning, one thought this work leads to is whether we could treat different inference methods as different tasks.
In other words, have a single model encoding the input, then have multiple inference algorithms with different extensions of that model, all trained simultaneously.
The variation in available context for the different algorithms may force generality in the core representation shared across them.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P/P11/P11-1068.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kuhlmann-gomezrodriguez-satta:2011:ACL-HLT2011,
  author    = {Kuhlmann, Marco  and  G\&#39;{o}mez-Rodr\&#39;{i}guez, Carlos  and  Satta, Giorgio},
  title     = {Dynamic Programming Algorithms for Transition-Based Dependency Parsers},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {673--682},
  url       = {https://aclanthology.org/P11-1068}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Mastering the game of Go without human knowledge (Silver et al., Nature 2017)</title>
      <link>https://www.jkk.name/post/2017-10-23_alphagozero/</link>
      <pubDate>Mon, 23 Oct 2017 21:12:57 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-23_alphagozero/</guid>
      <description>&lt;p&gt;This paper is an extension of the original AlphaGo work on using reinforcement learning to build a Go-player.
Interestingly, the changes have simplified the overall model, as well as enabling it to do even better than the previous model, but now without any supervised training.&lt;/p&gt;
&lt;p&gt;One key change is that there is a single core neural network learning to represent the game state.
On top of that there are either a set of layers that produce an evaluation of the quality of a position, or there are a set of layers that place a distribution over moves.
This ties in nicely to a lot of work happening at the moment on multi-task learning in NLP and elsewhere.&lt;/p&gt;
&lt;p&gt;Getting into the details, they use monte-carlo tree search to choose actions during training, then update the model to better match the outcomes observed.
Starting from a completely random initialisation, the argument for why this works is that at every point in self-play the MCTS informed outcomes are just slightly better than the current model.
That edge is enough to provide a useful signal, without being such a drastic shift because in self-play the two sides are closely matched.
Interestingly, while the unsupervised model is worse at predicting what expert human players will do in a game, it is still better at predicting which player will win.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaGoZero,
  author = {Silver, David  and  Schrittwieser, Julian  and  Simonyan, Karen  and  Antonoglou, Ioannis  and  Huang, Aja  and  Guez, Arthur  and  Hubert, Thomas  and  Baker, Lucas  and  Lai, Matthew  and  Bolton, Adrian  and  Chen, Yutian  and  Lillicrap, Timothy  and  Hui, Fan  and  Sifre, Laurent  and  van den Driessche, George  and  Graepel, Thore  and  Hassabis, Demis},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  year = {2017},
  volume = {550},
  issue = {7676},
  pages = {354-359},
  publisher = {Macmillan Publishers Limited, part of Springer Nature},
  doi = {10.1038/nature24270},
  url = {http://www.nature.com/nature/journal/v550/n7676/abs/nature24270.html#supplementary-information},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Attention Is All You Need (Vaswani et al., ArXiv 2017)</title>
      <link>https://www.jkk.name/post/2017-10-20_onlyattention/</link>
      <pubDate>Fri, 20 Oct 2017 15:25:23 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-20_onlyattention/</guid>
      <description>&lt;p&gt;Recurrent neural networks like LSTMs and GRUs have limited scope for parallelisation because each step depends on the one before it.
This architecture also means that many steps of computation separate two words that are far apart, making it difficult to capture long-distance relations.
A range of approaches have been used to try to address these issues, such as convolutional structures and other forms of recurrence (e.g. QRNNs).
The idea in this work is to use attention, applied multiple times, to get a network that is fast while still capturing positional information.&lt;/p&gt;
&lt;p&gt;To explain the structure I put together the figure below, which captures the network structure with a few simplifications:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/google-attention.png&#34; alt=&#34;Google Attention Network&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are a few ideas being brought together here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Positional encoding&lt;/em&gt;, which is a vector of the same length as the word representation, but that depends only on the position in the input. Here they use $f(pos, dim) = sin(pos / 10000^{2 dim / d_w})$ for even dimensions and the cosine equivalent for odd dimensions (where $d_w$ is the number of dimensions.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multi-head attention&lt;/em&gt;, where rather than running attention once on the full vector, multiple linear transforms are applied to get smaller vectors.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Scaled dot product attention&lt;/em&gt;, the equation is shown in the figure, the key new idea is to rescale by the square root of the dimensionality so that larger vectors don&amp;rsquo;t produce excessively sharp distributions after the softmax is applied. The more general form of this described in the paper has keys ($K$), queries ($Q$) and values ($V$), but the network uses the same vector for the key and value. I show the query with a solid line and the values/keys with dotted lines. The matrix $V$ is formed by using the $v$ vectors as rows, while $Q$ is formed by duplicating $q$ in every row. Note, on the left hand side attention is over all input words, while on the right it is only over the words produced so far (ie. content to the left).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Layer normalisation&lt;/em&gt;, a way to rescale weights to keep vector outputs in a nice range, from 
&lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ba, Kiros and Hinton (ArXiv 2016)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Other details, (1) When the outputs are words, the vectors used to initially represent each input word are also used to represent the outputs and in the final linear transformation (though with some rescaling). (2) They use a formula I haven&amp;rsquo;t seen before to adjust the learning rate during training, (3) dropout in several places and label smoothing are used for regularization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simplifications in the figure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For multi-head attention I only show two transforms, while in practise they used 8.&lt;/li&gt;
&lt;li&gt;The shaded regions are duplicated 6 times to create a stack, with the output of one region acting as the input to the next copy of it. The links from left to right are always from the top of the input stack.&lt;/li&gt;
&lt;li&gt;The musical repeat signs indicate that the structure is essentially the same. On the output side this isn&amp;rsquo;t quite true since the attention boxes only take inputs to their left (since output to the right doesn&amp;rsquo;t exist when they are being calculated).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In terms of experiments, it works at least as well if not better than prior approaches, and is a lot faster for machine translation (no speed numbers are given for parsing).
There is also some nice analysis of what it ends out using the attention mechanism to focus on for each word.
It seems like it can provide a way to effectively disambiguate the sense of a word based on its context.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Google also has some blog posts up

&lt;a href=&#34;https://research.googleblog.com/2017/08/transformer-novel-neural-network.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;about the paper&lt;/a&gt;
and

&lt;a href=&#34;https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;about the library&lt;/a&gt;
they released.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{arxiv:1706.03762,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title     = {Attention Is All You Need},
  journal   = {ArXiv},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</title>
      <link>https://www.jkk.name/post/2017-10-19_mace/</link>
      <pubDate>Thu, 19 Oct 2017 17:08:50 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-19_mace/</guid>
      <description>&lt;p&gt;The standard way to get high quality annotations is to get labels from multiple people and take a majority vote.
Getting multiple annotations costs more, and the quality of annotators can vary considerably (with spamming at one extreme).
One way to avoid the quality issue is to restrict who can do the task (must have done X previous tasks with an accept rate of Y), but that limits the pool of available workers.
Another approach is to try to estimate the quality of annotator work using a statistical model.&lt;/p&gt;
&lt;p&gt;Here a generative model is used, with the following structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T_i$, the true label, sampled with a uniform prior over labels&lt;/li&gt;
&lt;li&gt;$S_{ij}$, a binary variable indicating if the person is spamming or not, sampled as a Bernoulli variable with a Beta prior&lt;/li&gt;
&lt;li&gt;$A_{ij}$, the annotator&amp;rsquo;s decision, if they are spamming it is sampled from a multinomial with parameters specific to them (with a Dirichlet prior), otherwise it is the true label&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$A$ is observed, but $T$ and $S$ are not, so they use expectation maximization to get both model parameters and variable values.
To deal with nonconvexity they use 100 random restarts, deciding which is best based on how well the model describes the data.
Note - this model (and the code) was the basis of the error detection paper I 
&lt;a href=&#34;https://www.jkk.name/post/2017-10-13_errordetection/&#34;&gt;wrote about&lt;/a&gt; recently.&lt;/p&gt;
&lt;p&gt;For predicting annotator quality the model is consistently effective across three datasets, though the Beta and Dirichlet priors are key for one (where annotator agreement was high on average).
For determining the correct answer it is slightly better than majority vote, though the gains are small.
The real advantage comes in deciding whether to discard data, where the choice of what to discard can be guided by the estimate of quality (this is what the error detection paper was doing).
A range of synthetic experiments also show positive results, though their design shares the assumptions about behaviour that are baked into the model.&lt;/p&gt;
&lt;p&gt;I found a few results particularly interesting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As the number of annotators is decreased, the benefit of this approach over majority vote grows to be quite substantial (the main experiments are for data with 10 annotators).&lt;/li&gt;
&lt;li&gt;If you do use majority vote, use an odd number of annotators. Switching to an even number mainly seems to create ties. The right number is also very data dependent.&lt;/li&gt;
&lt;li&gt;Providing gold information as supervision within EM doesn&amp;rsquo;t help much unless it is quite substantial (20%+ of the data)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/N13-1132.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hovy-EtAl:2013:NAACL-HLT,
  author    = {Hovy, Dirk  and  Berg-Kirkpatrick, Taylor  and  Vaswani, Ashish  and  Hovy, Eduard},
  title     = {Learning Whom to Trust with MACE},
  booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2013},
  address   = {Atlanta, Georgia},
  publisher = {Association for Computational Linguistics},
  pages     = {1120--1130},
  url       = {https://aclanthology.org/N13-1132}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)</title>
      <link>https://www.jkk.name/post/2017-10-18_neuralamr/</link>
      <pubDate>Wed, 18 Oct 2017 21:31:05 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-18_neuralamr/</guid>
      <description>&lt;p&gt;This is another paper concerned with the challenge of sparsity in AMR parsing, specifically that there are an enormous number of output symbols in the parse trees and most are seen infrequently.
The system they develop is based on the encoder-decoder with attention approach, which has previously done poorly for AMR, partially because of sparsity.&lt;/p&gt;
&lt;p&gt;Their solution is to merge certain types of symbols into groups (dates, named entities, rare verbs, constants, etc) and have a standard way to map from the surface form to the output symbol.
This is an alternative to the approach from the paper I 
&lt;a href=&#34;https://www.jkk.name/post/2017-10-12_amralignment/&#34;&gt;wrote about&lt;/a&gt; last week.
They also introduce a completely separate idea, which is a different way to take an AMR graph and turn it into a linear sequence.
This change is necessary to make the output follow the form their model generates - a sequence (though there has been work on tree based LSTMs on the output side, so AMR could be directly generated, and I believe there has been some work on applying that to AMR).&lt;/p&gt;
&lt;p&gt;Together these changes do substantially improve performance over previous encoder-decoder based work for AMR.
However, there is still a substantial gap between the system and state-of-the-art, presumably because of the additional resources that other systems indirectly use by running external systems for NER, dependency parsing, etc.
Given the recent success of multi-task learning with neural nets, it would be interesting to see if those resources could be used here to further boost performance.
It may also be productive to combine these ideas with the graph abstraction ideas from AMR alignment paper.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/E/E17/E17-1035.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{peng-EtAl:2017:EACLlong1,
  author    = {Peng, Xiaochang  and  Wang, Chuan  and  Gildea, Daniel  and  Xue, Nianwen},
  title     = {Addressing the Data Sparsity Issue in Neural AMR Parsing},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  month     = {April},
  year      = {2017},
  address   = {Valencia, Spain},
  publisher = {Association for Computational Linguistics},
  pages     = {366--375},
  url       = {https://aclanthology.org/E17-1035}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)</title>
      <link>https://www.jkk.name/post/2017-10-17_nedisambiguation/</link>
      <pubDate>Tue, 17 Oct 2017 20:33:58 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-17_nedisambiguation/</guid>
      <description>&lt;p&gt;Several NLP tasks aim to identify information regarding entities, such as when two sections of text are referring to the same thing, or which thing out of a large set (e.g. things in Wikipedia) a piece of text is about.
This paper focuses on a subset of entity linking, trying to determine which entity out of a set of candidates is the correct one (in a way a kind of reranker for entity linking).&lt;/p&gt;
&lt;p&gt;The task is based on a really cool dataset from Google+UMass, which collected text that was hyperlinked to wikipedia articles.
The idea is that the text (&lt;em&gt;mention&lt;/em&gt;) is probably a reference to the thing the article describes, so it is an easy way to get entity linked data for free.
Here, the data is filtered to mentions that aren&amp;rsquo;t too rare (more than 10 occurrences) and where the mention isn&amp;rsquo;t used to refer to too many different entities (the two most common entities account for over 10% of occurrences).
Then, the set of things that this mention is used to refer to somewhere are treated as a list of candidates, and the task is to choose which one is correct in a given context.&lt;/p&gt;
&lt;p&gt;The model is of the common style at the moment:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The context is processed using a recurrent neural network to produce a set of vectors&lt;/li&gt;
&lt;li&gt;Attention is used to produce vectors that combine the context with a candidate entity&lt;/li&gt;
&lt;li&gt;A feedforward neural network produces a score that is maxed over to get a final decision&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On the wikilinks based dataset this performs quite a bit better than other models, but it is behind on the smaller manually curated datasets used elsewhere (YAGO and PPRforNED, which link entities in the CoNLL 2003 shared task).
Interestingly, augmenting the training data for YAGO with data from wikilinks does improve performance.
For future users of the wikilinks data there is also some nice analysis at the end of remaining challenges, which are spit between mistakes in the data (unsurprising given the approximate collection process), answers that are too general or specific, tricky cases, and the long tail (which would be even longer without the filtering used in these experiments).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/K/K17/K17-1008.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{eshel-EtAl:2017:CoNLL,
  author    = {Eshel, Yotam  and  Cohen, Noam  and  Radinsky, Kira  and  Markovitch, Shaul  and  Yamada, Ikuya  and  Levy, Omer},
  title     = {Named Entity Disambiguation for Noisy Text},
  booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)},
  month     = {August},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {58--68},
  url       = {https://aclanthology.org/K17-1008}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)</title>
      <link>https://www.jkk.name/post/2017-10-16_forumrnn/</link>
      <pubDate>Mon, 16 Oct 2017 20:55:07 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-16_forumrnn/</guid>
      <description>&lt;p&gt;Attention - a weighted average over a set of vectors representing context - has consistently produced positive results.
Here we see an example of how it can be applied in the case of modeling a threaded discussion.&lt;/p&gt;
&lt;p&gt;Attention is applied in two ways.
First, over a fixed set of vectors.
This is intended to provide a mechanism to choose between several different sub-models contained within a single model.
Put differently, the vectors provide a set of latent representations that capture each of the different types of posts in the subreddit.
Second, attention over the current utterance is used in the process of predicting responses (at training time only).
This provides an additional source of input to the model, by forcing it to explain the response utterances using the same representations as a source of information.&lt;/p&gt;
&lt;p&gt;The application is a new task, using values assigned to posts = upvotes - downvotes (i.e. Reddit karma).
Predicting the specific value is hard, so the task is split into 7 binary decisions about whether a post has a score higher or lower than some value.
On this task the new approach provides consistent gains, though overall performance remains low (53 - 56%).
Confusingly though, one of the figures (number 4) seems to suggest that it was a single multi-way decision, not a set of binary decisions.
I&amp;rsquo;m also curious about the data, in particular what the distribution of scores is.
The paper mentions it is Zipfian, but surely it would be something double-sided with a massive peak at 0 and a rapid drop in either direction?&lt;/p&gt;
&lt;p&gt;Overall, this is further evidence of the versatility of the idea of attention!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1242.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{cheng-fang-ostendorf:2017:EMNLP2017,
  author    = {Cheng, Hao  and  Fang, Hao  and  Ostendorf, Mari},
  title     = {A Factored Neural Network Model for Characterizing Online Discussions in Vector Space},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2286--2296},
  url       = {https://aclanthology.org/D17-1242}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)</title>
      <link>https://www.jkk.name/post/2017-10-13_errordetection/</link>
      <pubDate>Fri, 13 Oct 2017 13:32:19 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-13_errordetection/</guid>
      <description>&lt;p&gt;Active learning doesn&amp;rsquo;t seem to get much attention in NLP, probably because of fear that developing data based on the errors of one model will introduce a particular sampling bias.
This paper is a nice example of a problem it can be applied to that doesn&amp;rsquo;t raise that issue: detecting all the errors in a system&amp;rsquo;s output.&lt;/p&gt;
&lt;p&gt;The scenario is that you have a bunch of models for doing a task (e.g. POS tagging) and a new dataset with no labeled data, which you would like to label.
Having a person label the data would take a long time and doesn&amp;rsquo;t take advantage of these systems.
At the same time, we can&amp;rsquo;t just run the systems and use their output because they aren&amp;rsquo;t perfect, particularly out of domain.
We could run the systems and check their output, which could be faster than annotating directly, but would still take a long time.
If we don&amp;rsquo;t mind having some errors, we can check just some output, but how do we decide what to check?&lt;/p&gt;
&lt;p&gt;This paper applies the generative model from 
&lt;a href=&#34;https://aclanthology.org/N13-1132&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MACE&lt;/a&gt; to build a generative model of system outputs.
The model is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each example, sample the true label with a uniform prior&lt;/li&gt;
&lt;li&gt;Then, for each classifier, sample from a Bernoulli distribution to decide if they are good or not&lt;/li&gt;
&lt;li&gt;A good classifier returns the true label, a not good classifier samples from a multinomial over the options&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since we don&amp;rsquo;t know the parameters of the model, or the true labels, use expectation maximisation to learn.&lt;/p&gt;
&lt;p&gt;This work takes that model, trains it and uses it to identify the sample that is most uncertain.
A person annotates it, the correct label replaces one of the system predictions, and EM is run again.
This is repeated until either there appear to be no more errors, or annotators run out of time.&lt;/p&gt;
&lt;p&gt;How well does it work?
The main metric is precision: how many of the instances asked for annotation actually have errors.
For POS tagging on WSJ text, the taggers initially get 2.5% of words wrong.
To get that down to 1.1% the precision is 33%, and to get it to 0.65% precision is 17.6%.
On an out of domain dataset, the error rate is 10% initially, and is down to 5% with a precision of 50%.
Put differently, in a dataset of 25,000 tokens, with 2,500 errors, after checking 2,500 tokens, there are only 1,250 errors (another 2,500 checks brings it down to 730).
It also works well for NER, and consistently does better than the alternative they compare to (consider the taggers a committee and find the examples with highest entropy, i.e. greatest disagreement).&lt;/p&gt;
&lt;p&gt;This seems like a natural fit for 
&lt;a href=&#34;https://prodi.gy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prodigy&lt;/a&gt; and something that could be broadly useful.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P/P17/P17-1107.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{rehbein-ruppenhofer:2017:Long,
  author    = {Rehbein, Ines  and  Ruppenhofer, Josef},
  title     = {Detecting annotation noise in automatically labelled data},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1160--1170},
  url       = {https://aclanthology.org/P17-1107}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)</title>
      <link>https://www.jkk.name/post/2017-10-12_amralignment/</link>
      <pubDate>Thu, 12 Oct 2017 19:52:34 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-12_amralignment/</guid>
      <description>&lt;p&gt;Abstract Meaning Representation (AMR) structures represent sentence meaning with labeled nodes (concepts) that are related to the words in the sentence, but not explicitly linked to them.
This is a problem for most parsing algorithms, which need a way to efficiently decompose the structure in order to learn how to generate it.
In dependency parsing there are no abstract nodes to generate, in constituency parsing there is a very small set of node types, and for CCG, TAG, etc the labels come from a constrained space.
The solution for many AMR parsers is to have a process for generating the concepts as a first step towards parsing, and to automatically align the training data to guide this concept generation stage.&lt;/p&gt;
&lt;p&gt;The first idea in this paper is about the set of AMR concepts.
Some concepts are easy to link, as the concept clearly maps to a single word in the sentence.
Around a quarter of concepts have a more complex relation, where a set of concepts link to a set of words, for example, named entities.
The idea for these is to identify common subgraphs by abstracting some lexical items.
For example, a teacher and a worker both get mapped to a person concept that is the ARG0 of the lexical item (teach, or work in this case).
This can allow for the generation of entirely novel concepts (e.g. &amp;ldquo;concept&amp;rdquo;-er), giving a 0.6 boost to recall for CAMR simply by making these additional concepts available.
Using a bidirectional LSTM with a character CNN to generate features on likely concepts, there is a gain of 1.0 F1 for the parser.&lt;/p&gt;
&lt;p&gt;The second idea is to improve the alignments used to train concept generation by taking into consideration the graph structure.
To use an aligner developed for machine translation the graph needs to be turned into a linear sequence, but that can lead to strange jumps.
The idea here is to take that into consideration by modifying the calculation of the cost of distortion (i.e. jumping) to be reshaped based on the graph structure.
For optimal alignment quality they consider aligning in either direction, directly changing the distance metric in the English-AMR direction, and just rescaling it to be less sensitive when appropriate for AMR-English.
This is definitely higher precision than prior approaches, but lower recall.
It&amp;rsquo;s hard to tell whether this helps, since the evaluation doesn&amp;rsquo;t separate it out from the first idea (results in section 5.3 are not on the same dataset as 5.1).&lt;/p&gt;
&lt;p&gt;Given how separate this is from CAMR, it would be interesting to see if it helps other systems similarly.
With concept identification at 83 F there is still plenty of scope for improvement, though there is no analysis of which types of concepts remain the most problematic.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D/D17/D17-1130.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{wang-xue:2017:EMNLP2017,
  author    = {Wang, Chuan  and  Xue, Nianwen},
  title     = {Getting the Most out of AMR Parsing},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {1268--1279},
  url       = {https://aclanthology.org/D17-1130},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)</title>
      <link>https://www.jkk.name/post/2017-10-11_multimt/</link>
      <pubDate>Wed, 11 Oct 2017 17:29:04 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-11_multimt/</guid>
      <description>&lt;p&gt;This paper is a detailed analysis of a surprisingly effective simple idea: train a machine translation system with sentence pairs from multiple languages, adjusting the input to have an extra token at the end that says what the target language is.
To deal with class imbalance, data is oversampled to have all language pairs be equally represented (though even without that, it works fairly well).&lt;/p&gt;
&lt;p&gt;The biggest advantage of this approach is that a single model can handle translation between many pairs, rather than needing $O(n^2)$ models for $n$ languages.
The performance is slightly lower on average, but the single model can manage with far fewer parameters.
In one example, twelve models are combined into a single model with as many parameters as one of the twelve, and the results are lower by just 0.76 BLEU on average.
Another advantage of the model is the ability to handle code-switched language, though they didn&amp;rsquo;t have evaluation datasets to get an quantitative measure of accuracy.&lt;/p&gt;
&lt;p&gt;Having this model also opens up the possibility of translating between pairs of languages with no parallel training data (A -&amp;gt; B).
As long as there is data (A -&amp;gt; C) and (D -&amp;gt; B), sentences from A can be fed in with B as the target language.
For closely related languages this works very well, and in particular, better than going via another language such that there is data for the two language pairs.
For example, going from Portuguese to Spanish with the multilingual model scores 24.75, whereas going via English scores 21.62 and a model with explicit training data gets 31.50.
Going between less related languages is less successful, with direct Spanish to Japanese scoring 9.14, and going via English scoring 18.00.
One thing I wish the paper had is more exploration of this result - what does it get right when scoring 9.14?
For the time being at least, going via a third language still seems necessary, and presumably the best language to use is whichever one the performance is highest on.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/Q/Q17/Q17-1024.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1611.04558.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv version&lt;/a&gt; which appears to be the same aside from one extra figure of the model architecture.&lt;/p&gt;
&lt;p&gt;As an aside, it is interesting to see the timeline for this paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;November 2016, Submission to ArXiv and in the TACL submission batch&lt;/li&gt;
&lt;li&gt;March 2017, TACL revision batch&lt;/li&gt;
&lt;li&gt;October 2017, TACL published&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1081,
	author    = {Johnson, Melvin  and Schuster, Mike  and Le, Quoc  and Krikun, Maxim  and Wu, Yonghui  and Chen, Zhifeng  and Thorat, Nikhil  and ViÃ©gas, Fernanda  and Wattenberg, Martin  and Corrado, Greg  and Hughes, Macduff  and Dean, Jeffrey},
	title     = {Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	journal   = {Transactions of the Association for Computational Linguistics},
	volume    = {5},
	year      = {2017},
	issn      = {2307-387X},
	url       = {https://www.transacl.org/ojs/index.php/tacl/article/view/1081},
	pages     = {339--351}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Search-based Neural Structured Learning for Sequential Question Answering (Iyyer et al., ACL 2017)</title>
      <link>https://www.jkk.name/post/2017-10-10_seqqa/</link>
      <pubDate>Tue, 10 Oct 2017 13:43:36 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-10_seqqa/</guid>
      <description>&lt;p&gt;Semantic parsing datasets generally consist of (question, answer) pairs, where each pair is completely independent of the rest (one exception is ATIS, which has multi-turn conversations, though most work doesn&amp;rsquo;t use them).
In reality, we often ask a series of simple questions that together form a complex one, for example &amp;ldquo;What flights are available from Detroit to Sydney? And how much is the price if I don&amp;rsquo;t want to leave before 8am?&amp;rdquo;
This work explores these kinds of sequential questions with a new dataset and algorithm.&lt;/p&gt;
&lt;p&gt;The dataset was formed by asking crowd workers to rephrase questions from the WikiTableQuestions dataset into sequences of shorter questions.
This naturally constrains the types of questions (in particular, they reference a single table only), but covers a range of domains.
With 6,066 question sequences, and on average 2.9 questions / sequence, it&amp;rsquo;s a large dataset by semantic parsing standards.
However, there are no logical forms, only the row, column, or cell(s) that contain the answer.&lt;/p&gt;
&lt;p&gt;To solve the problem, they treat it as choosing a sequence of actions, where each action generate a part of the execution instructions.
The model follows the recent approach of considering the contents of the database as part of the calculation (e.g. by taking the dot product of the vector for a cell and the vector for the question).&lt;/p&gt;
&lt;p&gt;The system has consistently better performance than other QA systems on the new dataset (though no results are shown for the WikiTableQuestions dataset).
At only 12.8% of sequences completely correct, there is plenty of scope for improvement.
Based on the description of the operators there are definitely additional abilities that would be useful, so this model has potential to improve.
That said, it seems difficult to generalise the model to handle more complicated databases with multiple interconnected tables.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P/P17/P17-1167.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{iyyer-yih-chang:2017:Long,
  author    = {Iyyer, Mohit  and  Yih, Wen-tau  and  Chang, Ming-Wei},
  title     = {Search-based Neural Structured Learning for Sequential Question Answering},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1821--1831},
  url       = {https://aclanthology.org/P17-1167}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title> Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)</title>
      <link>https://www.jkk.name/post/2017-10-09_parsing-autoencoder/</link>
      <pubDate>Mon, 09 Oct 2017 14:31:24 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-09_parsing-autoencoder/</guid>
      <description>&lt;p&gt;Semantic parsing datasets are small because they are expensive to produce (logical forms don&amp;rsquo;t occur naturally and writing them down takes time).
The idea here is to do semi-supervised learning by implementing both a parser and a generator, which are trained together as a form of autoencoder where the intermediate representation is natural language.&lt;/p&gt;
&lt;p&gt;The architecture has four LSTMs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bidirectional LSTM over a logical form.&lt;/li&gt;
&lt;li&gt;One directional LSTM attending to the first LSTM&amp;rsquo;s hidden states, generating a sentence.&lt;/li&gt;
&lt;li&gt;Bidirectional LSTM over the sentence generated by the second LSTM.&lt;/li&gt;
&lt;li&gt;One directional LSTM attending to the third LSTM&amp;rsquo;s hidden states, generating a logical form.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Usually a component like the second LSTM would choose the max word at each position (or use beam search), but here they want this whole thing to be differentiable, so the distribution over words is used.
At evaluation time only the second half (3+4) is used, with the test sentence as input.&lt;/p&gt;
&lt;p&gt;With this structure, a loss function is defined that compares the input to (1) and the output of (4), which in both cases is a logical form.
As a result, they don&amp;rsquo;t need (logical form, sentence) pairs to train, they can use automatically generated logical forms.
Of course, with only logical forms it would do something random with the intermediate representation, so some supervised examples are also needed (in which case the two halves are trained independently).&lt;/p&gt;
&lt;p&gt;The results are not state-of-the-art, but good on all three tasks (Geoquery, NLmaps, SAIL), and on two they show am improvement over training (3+4) with only supervised data.
Varying the amount of training data gives a less clear picture.
On Geoquery with 5-25% of the data, this approach clearly helps, particularly if the queries are real rather than generated (which is a realistic scenario), but then there is no improvement for 50% or 75%, and at 100% the improvement is small.
On NLmaps there was no generator, and the differences at different data %s seem like noise.
SAIL has the most clear benefit, though it&amp;rsquo;s a particularly small dataset, consisting of paths in just four maps.&lt;/p&gt;
&lt;p&gt;This is a cool idea that seems effective in certain situations.
The generator is key, and it&amp;rsquo;s possible that performance on GeoQuery would be higher with a more sophisticated one (e.g. a tree structured generator, rather than the ngram model used here).
One idea mentioned in the conclusion is to try reversing the setup (3-4-1-2) and training with natural language examples that have no logical form.
How to tradeoff the different data scenarios seems like an interesting challenge!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D16-1116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kovcisky-EtAl:2016:EMNLP2016,
  author    = {Ko\v{c}isk\&#39;{y}, Tom\&#39;{a}\v{s}  and  Melis, G\&#39;{a}bor  and  Grefenstette, Edward  and  Dyer, Chris  and  Ling, Wang  and  Blunsom, Phil  and  Hermann, Karl Moritz},
  title     = {Semantic Parsing with Semi-Supervised Sequential Autoencoders},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  pages     = {1078--1087},
  url       = {https://aclanthology.org/D16-1116}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)</title>
      <link>https://www.jkk.name/post/2017-10-06-madlibs/</link>
      <pubDate>Fri, 06 Oct 2017 13:31:43 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-06-madlibs/</guid>
      <description>&lt;p&gt;Humor is an incredibly difficult problem, as this paper makes clear in its background section.
Most work has considered very specific types of jokes (e.g. &amp;ldquo;that&amp;rsquo;s what she said&amp;rdquo;, or pairs of words that sound similar to form riddles).
This work contributes (1) a new task, (2) an evaluation method, and (3) an example system.&lt;/p&gt;
&lt;p&gt;The task is Mad Libs, where a story has some words removed and people choose new words to make the story funny.
If you are familiar with the normal version, one key difference is that here people have access to the complete story when they are choosing their words.
A set of 40 &amp;lsquo;stories&amp;rsquo; were written based on Simple Wikipedia articles, and workers on Mechanical Turk wrote words to fill them, with filtering based on judging by other workers.&lt;/p&gt;
&lt;p&gt;The evaluation method involved recruiting a set of judges on Mechanical Turk and asking a series of questions to measure humour for a given response.
As well as judging the overall story, they were asked to select which words contributed the most.
By aggregating these selections as votes, each word was scored as funny or not.&lt;/p&gt;
&lt;p&gt;The system is a linear classifier with a range of features, including scores from a language model.
On its own, it performs very poorly, but using it as a filter to restrict the space of words a person can choose from actually leads to better performance than people on their own.
Of course, it&amp;rsquo;s difficult to analyse the source of improvement;
The authors theorise that it is because it prevents people from selecting words that only they would see is funny.
Another interpretation is that the constraint gives them a smaller space to think about and so they can find more interesting plays on words.&lt;/p&gt;
&lt;p&gt;Finally, as a non-expert in this area, this paper had some nice discussion of the tradeoffs between different ways of generating humour (incongruous vs. coherent content strategies).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hossain-EtAl:2017:EMNLP2017,
  author    = {Hossain, Nabil  and  Krumm, John  and  Vanderwende, Lucy  and  Horvitz, Eric  and  Kautz, Henry},
  title     = {Filling the Blanks (hint: plural noun) for Mad Libs Humor},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {649--658},
  url       = {https://aclanthology.org/D17-1068},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)</title>
      <link>https://www.jkk.name/post/2017-10-05-deftnn/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-10-05-deftnn/</guid>
      <description>&lt;p&gt;This paper proposes two techniques for speeding up neural network execution on GPUs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reduce computation when doing matrix-multiply by removing rows.&lt;/li&gt;
&lt;li&gt;Reduce communication on the GPU by halving the number of bits used to represent numbers.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Either of these gives a speed up of ~1.5x and together they give ~2x, across a range of different computer vision tasks+models.&lt;/p&gt;
&lt;h2 id=&#34;core-ideas-in-detail&#34;&gt;Core ideas in detail&lt;/h2&gt;
&lt;p&gt;The first idea, reducing work by eliminating parts of the computation, has been considered before.
In the past, however, the focus was on saving memory in models, and so the most common strategy was to move to a sparse matrix where weights close to zero are dropped.
Here the focus is on speed and they show that while the sparse approach saves memory it can end up being slower because of hardware behaviour.
Instead, they eliminate entire rows of the matrix, which means there is less computation, but it remains dense (and therefore fast).
Rows are identified by measuring correlation between outputs and greedily eliminating rows that correlate highly with the rest of the output.&lt;/p&gt;
&lt;p&gt;The natural question to ask is whether this hurts performance.
First, they do two things to avoid problems, (1) a scale factor is used to make sure the outputs are of the same range that they would have been with the full matrix, and (2) they restart training to fine-tune the network once pruning is set up.
With high enough pruning accuracy does fall, but speed ups can be gained before that is a problem (the exact point depends on the task).&lt;/p&gt;
&lt;p&gt;The second idea relates to numerical representation, and is motivated by measurements of where the bottlenecks are in communication.
Many AI researchers have tried switching to 16 bit representations to save space and time, but here they develop a different floating point encoding that gives more bits to the exponent, and fewer to the mantissa.&lt;/p&gt;
&lt;h2 id=&#34;thoughts&#34;&gt;Thoughts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It would be interesting to see the interaction of this work with the investigation of networks without non-linear functions that can still learn non-linear behaviour because of numerical approximations.&lt;/li&gt;
&lt;li&gt;In the context of language, the weight reduction approach would be interesting to analyse. Specifically, what do we lose in our word vectors depending on the task?&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve always had some interest in making things faster. It would be interesting to know where the remaining bottlenecks are (after applying these changes).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Hill:MICRO:2017,
  author    = {Hill, Parker and Jain, Animesh and Hill1, Mason and Zamirai, Babak and Hsu, Chang-Hong and Laurenzano, Michael A. and Mahlke, Scott and Tang, Lingjia and Mars, Jason},
  title = {DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission},
  booktitle = {The 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  year = {2017},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Ordering Pizza for an Event with Vegetarians</title>
      <link>https://www.jkk.name/post/pizza/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 -0500</pubDate>
      <guid>https://www.jkk.name/post/pizza/</guid>
      <description>&lt;h2 id=&#34;how-much-vegetarian-pizza-should-i-order&#34;&gt;How much vegetarian pizza should I order?&lt;/h2&gt;
&lt;p&gt;This question frequently comes up in the world of free food at university events. In my experience (as someone who does not eat meat pizzas), often not enough is ordered. Let&amp;rsquo;s try to come up with a model to tell us how much to order. Set it up like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are $N$ people.&lt;/li&gt;
&lt;li&gt;There are $P$ pizzas.&lt;/li&gt;
&lt;li&gt;The fraction of people who are vegetarian is $V$.&lt;/li&gt;
&lt;li&gt;Assume everyone eats the same amount of pizza, and all the pizza is eaten (ie. each person eats $\frac{P}{N}$).&lt;/li&gt;
&lt;li&gt;Assume people randomly sample from the available pizzas, subject to the constraint that some eat only vegetarian pizzas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, let the fraction of vegetarian pizzas we get be $k$, and we can write down the number of vegetarian pizzas in two ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How many we order: $P * k$&lt;/li&gt;
&lt;li&gt;How many are eaten: (pizzas eaten by vegetarians) + (vegetarian pizzas eaten by others) = $\frac{P}{N} * (N * V) + \frac{P}{N} * (N * (1 - V)) * k$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since all the pizza we order is eaten, these are equal.
$N$ and $P$ are both positive numbers, so we can safely cancel the $N$s and divide through by $P$, giving:&lt;/p&gt;
&lt;p&gt;$V + (1 - V) * k = k$&lt;/p&gt;
&lt;p&gt;To satisfy this equation, $k = 1$. Therefore all the pizza should be vegetarian :)&lt;/p&gt;
&lt;p&gt;Of course, these assumptions aren&amp;rsquo;t quite right (for example, not everyone samples randomly from the available pizza), so here are some more useful suggestions too:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do order more than the proportion of vegetarians.&lt;/li&gt;
&lt;li&gt;Place the vegetarian pizza at the end of the line of pizzas, or in a separate location with clear signage discouraging non-vegetarians from eating it.&lt;/li&gt;
&lt;li&gt;Order a diverse set of popular meat pizzas (people tend to want variety, so this encourages them to try more meat pizzas).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For other peoples&amp;rsquo; thoughts on this question see

&lt;a href=&#34;http://www.seriouseats.com/2014/07/etiquette-ordering-pizza-for-a-group-manner-matters.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Serious Eats&lt;/a&gt; and

&lt;a href=&#34;https://www.quora.com/What-are-the-best-pizza-toppings-to-get-for-a-big-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quora&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation</title>
      <link>https://www.jkk.name/publication/emnlp17forums/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp17forums/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IE/NER from Cybercriminal Forums</title>
      <link>https://www.jkk.name/data/cybercrime-forums/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/cybercrime-forums/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Crowdsourced Paraphrases</title>
      <link>https://www.jkk.name/data/paraphrasing-sample/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/paraphrasing-sample/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection</title>
      <link>https://www.jkk.name/publication/acl17paraphrase/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl17paraphrase/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tools for Automated Analysis of Cybercriminal Markets</title>
      <link>https://www.jkk.name/publication/www17forums/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/www17forums/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One-Endpoint Crossing Graph Parser</title>
      <link>https://www.jkk.name/software/1ec-parsing/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/software/1ec-parsing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Parsing with Traces: An O($n^4$) Algorithm and a Structural Representation</title>
      <link>https://www.jkk.name/publication/tacl17parsing/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/tacl17parsing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spine and Arc version of the Penn Treebank</title>
      <link>https://www.jkk.name/data/shp-ptb/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/shp-ptb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Algorithms for Identifying Syntactic Errors and Parsing with Graph Structured Output</title>
      <link>https://www.jkk.name/publication/thesis16parsing/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/thesis16parsing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Empirical Analysis of Optimization for Max-Margin NLP</title>
      <link>https://www.jkk.name/publication/emnlp15learn/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp15learn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coreference Error Analysis</title>
      <link>https://www.jkk.name/software/coreference-analysis/</link>
      <pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/software/coreference-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Error-Driven Analysis of Challenges in Coreference Resolution</title>
      <link>https://www.jkk.name/publication/emnlp13analysis/</link>
      <pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp13analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Empirical Examination of Challenges in Chinese Parsing</title>
      <link>https://www.jkk.name/publication/acl13analysis/</link>
      <pubDate>Thu, 01 Aug 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl13analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High-velocity Clouds in the Galactic All Sky Survey. I. Catalog</title>
      <link>https://www.jkk.name/publication/astro13clouds/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/astro13clouds/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CCG to PST</title>
      <link>https://www.jkk.name/software/ccg2pst/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/software/ccg2pst/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Parse Error Analysis</title>
      <link>https://www.jkk.name/software/parsing-analysis/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/software/parsing-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</title>
      <link>https://www.jkk.name/publication/emnlp12analysis/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/emnlp12analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Conversion of CCG Derivations to Phrase Structure Trees</title>
      <link>https://www.jkk.name/publication/acl12conversion/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl12conversion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mention Detection: Heuristics for the OntoNotes annotations</title>
      <link>https://www.jkk.name/publication/conll11coreference/</link>
      <pubDate>Wed, 01 Jun 2011 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/conll11coreference/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spatiotemporal Hierarchy of Relaxation Events, Dynamical Heterogeneities, and Structural Reorganization in a Supercooled Liquid</title>
      <link>https://www.jkk.name/publication/prl10chemistry/</link>
      <pubDate>Wed, 01 Sep 2010 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/prl10chemistry/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Morphological Analysis Can Improve a CCG Parser for English</title>
      <link>https://www.jkk.name/publication/coling10morph/</link>
      <pubDate>Sun, 01 Aug 2010 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/coling10morph/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adaptive CCG Supertagging Model</title>
      <link>https://www.jkk.name/data/ccg-model/</link>
      <pubDate>Thu, 01 Jul 2010 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/ccg-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Faster Parsing by Supertagger Adaptation</title>
      <link>https://www.jkk.name/publication/acl10adapt/</link>
      <pubDate>Thu, 01 Jul 2010 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/acl10adapt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Faster parsing and supertagging model estimation</title>
      <link>https://www.jkk.name/publication/alta09tagging/</link>
      <pubDate>Tue, 01 Dec 2009 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/alta09tagging/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adaptive Supertagging for Faster Parsing</title>
      <link>https://www.jkk.name/publication/thesis09adapt/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/thesis09adapt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Large-Scale Syntactic Processing: Parsing the Web</title>
      <link>https://www.jkk.name/publication/report09jhu/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/report09jhu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Classification of Verb Particle Constructions with the Google Web1T Corpus</title>
      <link>https://www.jkk.name/publication/alta08vpc/</link>
      <pubDate>Mon, 01 Dec 2008 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/alta08vpc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The densest packing of AB binary hard-sphere homogeneous compounds across all size ratios</title>
      <link>https://www.jkk.name/publication/chem08packing/</link>
      <pubDate>Fri, 01 Aug 2008 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/publication/chem08packing/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
