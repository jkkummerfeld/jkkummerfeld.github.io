<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jonathan K. Kummerfeld</title>
    <link>http://www.jkk.name/</link>
    <description>Jonathan K. Kummerfeld</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jonathan K. Kummerfeld</copyright>
    <lastBuildDate>Wed, 27 Sep 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://www.jkk.name/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      
      <title>
        "Outlier Detection for Improved Data Quality and Diversity in Dialog Systems",
        
          Stefan Larson, Anish Mahendran, Andrew Lee, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Parker Hill, Michael Laurenzano, Johann Hauswald, Lingjia Tang and Jason Mars
        
        (NAACL,
        2019)
      </title>
      
      <link>http://www.jkk.name/publication/naacl19outliers/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/naacl19outliers/</guid>
      
        <description>
          
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Look Who&#39;s Talking: Inferring Speaker Attributes from Personal Longitudinal Dialog",
        
          Charles Welch, Veronica Perez-Rosas, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, and Rada Mihalcea
        
        (CICLing,
        2019)
      </title>
      
      <link>http://www.jkk.name/publication/cicling19personal/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/cicling19personal/</guid>
      
        <description>
          
        </description>
      

    </item>
    
    <item>
      
      <title>
        "DSTC7 Task 1: Noetic End-to-End Response Selection",
        
          Chulaka Gunasekara, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Lazaros Polymenakos, and Walter S. Lasecki
        
        (DSTC,
        2019)
      </title>
      
      <link>http://www.jkk.name/publication/dstc19task1/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/dstc19task1/</guid>
      
        <description>
          Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Dialog System Technology Challenge 7",
        
          Koichiro Yoshino, Chiori Hori, Julien Perez, Luis Fernando D&#39;Haro, Lazaros Polymenakos, Chulaka Gunasekara, Walter S. Lasecki, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, Xiang Gao, Huda Alamari, Tim K. Marks, Devi Parikh and Dhruv Batra
        
        (ConvAI Workshop,
        2018)
      </title>
      
      <link>http://www.jkk.name/publication/ws18dstc/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/ws18dstc/</guid>
      
        <description>
          This paper introduces the Seventh Dialog System Technology Challenges (DSTC), which use shared datasets to explore the problem of building dialog systems. Recently, end-to-end dialog modeling approaches have been applied to various dialog tasks. The seventh DSTC (DSTC7) focuses on developing technologies related to end-to-end dialog systems for (1) sentence selection, (2) sentence generation and (3) audio visual scene aware dialog. This paper summarizes the overall setup and results of DSTC7, including detailed descriptions of the different tracks and provided datasets. We also describe overall trends in the submitted systems and the key results. Each track introduced new datasets and participants achieved impressive results using state-of-the-art end-to-end technologies.
        </description>
      

    </item>
    
    <item>
      
        <title>PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution (Chen et al., 2018)</title>
      
      <link>http://www.jkk.name/post/2018-11-08_corefdata/</link>
      <pubDate>Thu, 08 Nov 2018 11:29:32 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2018-11-08_corefdata/</guid>
      
        <description>

&lt;p&gt;The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset).
Some of these are discussed in my &lt;a href=&#34;http://jkk.name/publication/conll11coreference/&#34; target=&#34;_blank&#34;&gt;CoNLL Shared Task submission paper&lt;/a&gt;, the biggest being the choice to not annotate mentions that are not coreferent.
This paper describes a new dataset that has a different set of compromises, specifically:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A broader definition of coreference (e.g. appositives are coreferent)&lt;/li&gt;
&lt;li&gt;All mentions annotated&lt;/li&gt;
&lt;li&gt;Different annotation methods for different subsets of the data (training data is double annotated and then adjudicated, while the development and test data is triple annotated, all pairs of annotations are adjudicated, then the outcomes are merged by voting)&lt;/li&gt;
&lt;li&gt;A variety of genres, but generally simpler language&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The dataset is 10x the size of OntoNotes and freely available, which is fantastic.
The source text is 2/3rds the RACE dataset (English reading comprehension exams from China), and 1/3rd scraped websites.
Measurements of annotator agreement suggest the annotations are not as consistent as OntoNotes, but still good enough to be a useful resource.
I do disagree with one aspect of the paper&amp;rsquo;s analysis - the results show a substantial gain in performance when providing gold mentions, suggesting to me that it remains an important challenge in coreference resolution.
I&amp;rsquo;m also curious whether my &lt;a href=&#34;http://jkk.name/publication/emnlp13analysis/&#34; target=&#34;_blank&#34;&gt;coreference analysis tool&lt;/a&gt; would find different patterns in errors on this dataset compared to OntoNotes.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/D18-1016&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://preschool-lab.github.io/PreCo/&#34; target=&#34;_blank&#34;&gt;Data&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Chen:EMNLP:2018,
  author    = {Chen, Hong and Fan, Zhenhua and Lu, Hao and Yuille, Alan and Rong, Shu},
  title     = {PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  pages     = {172--181},
  location  = {Brussels, Belgium},
  url       = {http://aclweb.org/anthology/D18-1016},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
      <title>
        "Analyzing Assumptions in Conversation Disentanglement Research Through the Lens of a New Dataset and Model",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Sai R. Gouravajhala, Joseph Peper, Vignesh Athreya, Chulaka Gunasekara, Jatin Ganhotra, Siva Sankalp Patel, Lazaros Polymenakos, Walter S. Lasecki
        
        (ArXiv e-prints,
        2018)
      </title>
      
      <link>http://www.jkk.name/publication/arxiv18disentangle/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/arxiv18disentangle/</guid>
      
        <description>
          Disentangling conversations mixed together in a single stream of messages is a difficult task with no large annotated datasets. We created a new dataset that is 25 times the size of any previous publicly available resource, has samples of conversation from 152 points in time across a decade, and is annotated with both threads and a within-thread reply-structure graph. We also developed a new neural network model, which extracts conversation threads substantially more accurately than prior work. Using our annotated data and our model we tested assumptions in prior work, revealing major issues in heuristically constructed resources, and identifying how small datasets have biased our understanding of multi-party multi-conversation chat.
        </description>
      

    </item>
    
    <item>
      
        <title>Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu et al., 2018)</title>
      
      <link>http://www.jkk.name/post/2018-09-04_featureengineering/</link>
      <pubDate>Tue, 04 Sep 2018 10:37:23 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2018-09-04_featureengineering/</guid>
      
        <description>

&lt;p&gt;A common argument in favour of neural networks is that they do not require &amp;lsquo;feature engineering&amp;rsquo;, manually defining functions that produce useful representations of the input data (e.g. a function that checks if a word is in a list of cities and returns 1 or 0).
This paper argues that there is in fact still value in such functions.&lt;/p&gt;

&lt;p&gt;The task is named entity recognition and the model is a CRF with a bidirectional LSTM using character and word embeddings.
The functions in this case are (1) part of speech tags, (2) word shapes, and (3) gazetteers.
Importantly, as well as receiving these as inputs, the model has to predict them as outputs (in both cases using predictions, not gold values).
The improvement on the test set is substantial, ~0.8 F1.
Ablation indicates that POS tags and word shape are particularly important, and having both the input and output is important.
Interestingly, the shift on the development set is more marginal, ~0.3 F1, and the ablation doesn&amp;rsquo;t show as clear trends.&lt;/p&gt;

&lt;p&gt;Overall, my takeaway is that these kinds of features (which are not very hard to define) are worth the effort.
However, there are a few more values I would have liked to see:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Multi-task learning (they kind of get at this with one ablation, but it is on non-gold output)&lt;/li&gt;
&lt;li&gt;Cross-validation results (given the difference between dev and test)&lt;/li&gt;
&lt;li&gt;ELMo (the paper argues that it is orthogonal, which is reasonable, but I&amp;rsquo;m still curious)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1808.09075&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Wu:2018:EMNLP,
  author    = {Minghao Wu, Fei Liu and Trevor Cohn},
  title     = {Evaluating the Utility of Hand-crafted Features in Sequence Labelling},
  booktitle = {EMNLP},
  year      = {2018},
  url       = {https://arxiv.org/abs/1808.09075},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Neural POS tagging</title>
      
      <link>http://www.jkk.name/software/neural-tagger/</link>
      <pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/neural-tagger/</guid>
      
        <description>Implementations of a POS tagger in DyNet, PyTorch, and Tensorflow, visualised to show the overall picture and make comparisons easy. <a href="https://jkk.name/neural-tagger-tutorial/" target="_blank">https://jkk.name/neural-tagger-tutorial/</a></description>
      

    </item>
    
    <item>
      
        <title>Text to SQL datasets</title>
      
      <link>http://www.jkk.name/data/text-to-sql/</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/text-to-sql/</guid>
      
        <description>A collection of datasets containing questions in English paired with SQL queries for a provided database. Our version homogenises the style of the SQL and corrects errors in previous versions of the data. <a href="https://jkk.name/text2sql-data" target="_blank">https://jkk.name/text2sql-data</a></description>
      

    </item>
    
    <item>
      
        <title>Text to SQL Baseline</title>
      
      <link>http://www.jkk.name/software/text2sql-baseline/</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/text2sql-baseline/</guid>
      
        <description>A simple LSTM-based model that uses templates and slot-filing to map questions to SQL queries. <a href="https://jkk.name/text2sql-data/systems/baseline-template/" target="_blank">https://jkk.name/text2sql-data/systems/baseline-template/</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Improving Text-to-SQL Evaluation Methodology",
        
          Catherine Finegan-Dollak\*, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld\*&lt;/span&gt;, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev
        
        (ACL,
        2018)
      </title>
      
      <link>http://www.jkk.name/publication/acl18sql/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl18sql/</guid>
      
        <description>
          To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.
        </description>
      

    </item>
    
    <item>
      
        <title>Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples (Vidur Joshi et al., 2018)</title>
      
      <link>http://www.jkk.name/post/2018-06-12_parseradaptation/</link>
      <pubDate>Tue, 12 Jun 2018 20:33:00 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2018-06-12_parseradaptation/</guid>
      
        <description>

&lt;p&gt;Virtually all systems trained using data have trouble when applied to datasets that differ even slightly - even switching from Wall Street Journal text to New York Times text can hurt parsing performance slightly.
Extensive work has explored how to adapt to new domains (including &lt;a href=&#34;http://jkk.name/publication/acl10adapt/&#34; target=&#34;_blank&#34;&gt;one of my own&lt;/a&gt;), but generally these approaches only made up a fraction of the gap in performance.&lt;/p&gt;

&lt;p&gt;This paper shows two interesting new approaches to this issue:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use &lt;a href=&#34;https://allennlp.org/elmo&#34; target=&#34;_blank&#34;&gt;ELMo&lt;/a&gt;, a type of word representation trained on massive amounts of text.&lt;/li&gt;
&lt;li&gt;Train a span-based parser with partial annotations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first is straightforward, and further demonstrates the effectiveness of ELMo.
To give a sense of how much this helps, the Charniak parser goes from 92 on the WSJ to 85 on the Brown corpus, while this model goes from 94 to 90.
The second idea takes advantage of &lt;a href=&#34;https://aclanthology.info/papers/P17-1076/p17-1076&#34; target=&#34;_blank&#34;&gt;a recent parsing model&lt;/a&gt; with a simple approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Independently assign a score to every span of a sentence, indicating whether it is part of the parse.&lt;/li&gt;
&lt;li&gt;Find the maximum scoring set of spans using a dynamic program.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The structure of the scoring step allows for a convenient form of partial annotations.
Simply label the tricky spans in a sentence (e.g. to indicate where a prepositional phrase attaches / does not attach).
During training on partially annotated sentences, only the labeled spans are used to update the model.
This gives dramatic gains across multiple datasets.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.06556&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Joshi:2018:ACL,
  author    = {Vidur Joshi, Matthew Peters, Mark Hopkins},
  title     = {Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples},
  booktitle = {ACL},
  year      = {2018},
  url       = {https://arxiv.org/abs/1805.06556},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
      <title>
        "Factors Influencing the Surprising Instability of Word Embeddings",
        
          Laura Wendlandt, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Rada Mihalcea
        
        (NAACL,
        2018)
      </title>
      
      <link>http://www.jkk.name/publication/naacl18embeddings/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/naacl18embeddings/</guid>
      
        <description>
          Despite the recent popularity of word embedding methods, there is only a small body of work exploring the limitations of these representations. In this paper, we consider one aspect of embedding spaces, namely their stability. We show that even relatively high frequency words (100-200 occurrences) are often unstable. We provide empirical evidence for how various factors contribute to the stability of word embeddings, and we analyze the effects of stability on downstream tasks.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Data Collection for Dialogue System: A Startup Perspective",
        
          Yiping Kang, Yunqi Zhang, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Parker Hill, Johann Hauswald, Michael A. Laurenzano, Lingjia Tang, Jason Mars
        
        (NAACL (industry),
        2018)
      </title>
      
      <link>http://www.jkk.name/publication/naacl18data/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/naacl18data/</guid>
      
        <description>
          Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities. Crowdsourcing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs. Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis, which may not be indicative of impact on trained model performance. In this paper, we present a study of crowdsourcing methods for a user intent classification task in one of our deployed dialogue systems. Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs. Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Effective Crowdsourcing for a New Type of Summarization Task",
        
          Youxuan Jiang, Catherine Finegan-Dollak, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Walter Lasecki
        
        (NAACL (short),
        2018)
      </title>
      
      <link>http://www.jkk.name/publication/naacl18summary/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/naacl18summary/</guid>
      
        <description>
          Most summarization research focuses on summarizing the entire given text, but in practice readers are often interested in only one aspect of the document or conversation. We propose &#39;targeted summarization&#39; as an umbrella category for summarization tasks that intentionally consider only parts of the input data. This covers query-based summarization, update summarization, and a new task we propose where the goal is to summarize a particular aspect of a document. However, collecting data for this new task is hard because directly asking annotators (e.g., crowd workers) to write summaries leads to data with low accuracy when there are a large number of facts to include.  We introduce a novel crowdsourcing workflow, Pin-Refine, that allows us to collect highquality summaries for our task, a necessary step for the development of automatic systems.
        </description>
      

    </item>
    
    <item>
      
        <title>The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models (Weber et al., 2018)</title>
      
      <link>http://www.jkk.name/post/2018-05-08_seq2seqsensitivity/</link>
      <pubDate>Tue, 08 May 2018 09:00:31 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2018-05-08_seq2seqsensitivity/</guid>
      
        <description>

&lt;p&gt;We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models usually end up doing reasonably well.
This paper asks an important question - are those metrics measuring generalisability effectively?
In particular, if we sample our test set from a slightly different distribution of data, do models still work well?&lt;/p&gt;

&lt;p&gt;As a controlled set up they form a simple dataset as follows for each sentence:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Go through the sentence left to right&lt;/li&gt;
&lt;li&gt;For each word generate three words in the output, where the output words are randomly sampled from a small vocabulary that is unique to each input word&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is clearly learnable and it seems reasonable that a sequence-to-sequence neural model with attention should be able to learn it.
Experiments show they do, getting close to 100% on a test set sampled the same way as the training set (input length 5-10, no symbol used twice).
However, if the test set is slightly different, with sequences of length 11-15, then results vary from 0% to 98% depending on the random seed in training (other variations also lead to large variations).
What this means is that sometimes the model is not learning to generalise.
They also show that the models that do generalise can only do so in one way (e.g. remain effective when length varies, or remain effective when symbols are used more than once in the input).&lt;/p&gt;

&lt;p&gt;A few takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Make sure your training and testing data are sampled from the distribution you are interested in&lt;/li&gt;
&lt;li&gt;More study of training data order and weight initialisation is needed (these are the two factors impacted by the random seed)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Incidentally, I am a co-author on an ACL paper that points out a similar issue for mapping text questions to SQL queries.
If we restrict the test set to be novel queries (i.e. the model has to generalise) performance falls through the floor.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.01445&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{Weber:2018:GenDeep,
   author = {Noah Weber, Leena Shekhar, Niranjan Balasubramanian},
    title = {The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models},
  journal = {Workshop on New Forms of Generalization in Deep Learning and NLP (NAACL 2018)},
     year = {2018},
      url = {https://arxiv.org/abs/1805.01445},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
      <title>
        "World Knowledge for Abstract Meaning Representation Parsing",
        
          Charles Welch, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Song Feng, Rada Mihalcea
        
        (LREC,
        2018)
      </title>
      
      <link>http://www.jkk.name/publication/lrec18amr/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/lrec18amr/</guid>
      
        <description>
          In this paper we explore the role played by world knowledge in semantic parsing. We look at the types of errors that currently exist in a state-of-the-art Abstract Meaning Representation (AMR) parser, and explore the problem of how to integrate world knowledge to reduce these errors. We look at three types of knowledge from (1) WordNet hypernyms and super senses, (2) Wikipedia entity links, and (3) retraining a named entity recognizer to identify concepts in AMR. The retrained entity recognizer is not perfect and cannot recognize all concepts in AMR and we examine the limitations of the named entity features using a set of oracles. The oracles show how performance increases if it can recognize different subsets of AMR concepts. These results show improvement on multiple fine-grained metrics, including a 6% increase in named entity F-score, and provide insight into the potential of world knowledge for future work in Abstract Meaning Representation parsing.
        </description>
      

    </item>
    
    <item>
      
        <title>An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)</title>
      
      <link>http://www.jkk.name/post/2018-04-16_lm_analysis/</link>
      <pubDate>Mon, 16 Apr 2018 20:55:22 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2018-04-16_lm_analysis/</guid>
      
        <description>

&lt;p&gt;Assigning a probability distribution over the next word or character in a sequence (language modeling) is a useful component of many systems, such as speech recognition and translation.
Recently neural networks have come to dominate in performance, with a range of clever innovations in network structure.
This paper is not about new models, but rather explores the current evaluation and how well carefully tuned baseline models can do.&lt;/p&gt;

&lt;p&gt;The key observations for me were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are issues with the PTB dataset for character-level evaluation - it removes all punctuation, makes numbers &amp;lsquo;N&amp;rsquo;, and removes rare words (i.e. it is a character-level version of the token-level task).
Given that the original Penn Treebank exists, I would have been interested to see a comparison with the PTB without any simplification.
The other dataset, enwik8, makes sense as a testing ground for compression algorithms, but is a little odd for modeling language, since it is the first 100 million bytes of a Wikipedia XML dump.
The paper does have another dataset, WikiText, which sounds good, but then there is no character-level evaluation!&lt;/li&gt;
&lt;li&gt;The LSTM is able to achieve ~SotA results for character-level modeling.
The key seems to be careful design of the softmax that produces the final probability distribution:
(1) rare words are clustered and represented by a single value in the distribution calculation, and
(2) word vectors are shared between input and output.&lt;/li&gt;
&lt;li&gt;Dropout matters more than the network design, and multiple forms of dropout should be tuned jointly.
This comes from analysis of a set of models trained with random variation in hyperparameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1803.08240&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{2018arXiv180308240M,
   author = {{Merity}, S. and {Shirish Keskar}, N. and {Socher}, R.},
    title = {An Analysis of Neural Language Modeling at Multiple Scales},
  journal = {ArXiv e-prints},
     year = {2018},
      url = {https://arxiv.org/abs/1803.08240},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Provenance for Natural Language Queries (Deutch et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2018-03-08_sql/</link>
      <pubDate>Thu, 08 Mar 2018 20:11:48 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2018-03-08_sql/</guid>
      
        <description>

&lt;p&gt;Being able to query a database in natural language could help make data accessible to more people.
Systems that do this have to solve two challenges: (1) understanding the query and (2) expressing the response in a way the user will understand.
Recently there have been papers in the NLP community on the first challenge, but this paper comes from the DB community and considers the second.&lt;/p&gt;

&lt;p&gt;The approach assumes we have a syntactic parse of the query and an alignment between the parse and the SQL query it corresponds to (they rely on prior work for this query interpretation piece).
Given that, the new idea in this paper is to take the database results and use the alignment to insert values for each field into the original parse, and from there into the original question.
To avoid extremely long sentences (when there are multiple result rows) they define a procedure to identify ways to summarise results.&lt;/p&gt;

&lt;p&gt;However, I&amp;rsquo;m not convinced by the evaluation.
The dataset they use was collected by (1) enumerating the 196 types of queries people could ask using the Microsoft Academic Search service, and (2) a person manually writing a question for each query.
As a result, the questions feel very formulaic and also only cover cases that we already have a user-friendly interface for, making it unclear how well this will generalise to more natural data.
Still, this work explores an interesting problem and it&amp;rsquo;s cool to see a direct use of syntactic parsing!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.vldb.org/pvldb/vol10/p577-deutch.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{Deutch:2017,
  author = {Deutch, Daniel and Frost, Nave and Gilad, Amir},
  title = {Provenance for Natural Language Queries},
  journal = {Proceedings of the VLDB Endowment},
  volume = {10},
  number = {5},
  month = {Jan},
  year = {2017},
  issn = {2150-8097},
  pages = {577--588},
  doi = {10.14778/3055540.3055550},
  url = {http://www.vldb.org/pvldb/vol10/p577-deutch.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning (Tsvetkov et al., 2016)</title>
      
      <link>http://www.jkk.name/post/2018-03-05_curriculum/</link>
      <pubDate>Mon, 05 Mar 2018 21:09:58 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2018-03-05_curriculum/</guid>
      
        <description>

&lt;p&gt;Usually when we learn, we have a curriculum designed to incrementally build understanding.
It seems reasonable that the same idea could be useful for machine learning, and indeed there is a large body of work on the topic.
This paper explores the specific question of whether a curriculum can help develop task-specific word vectors, and whether we can determine an effective curriculum automatically.&lt;/p&gt;

&lt;p&gt;They define a linear model with a range of features that characterise a paragraph of text, such as the number of distinct words, the number of prepositional phrases, and the average number of syllables per word.
Paragraphs are sorted by the model and used to train word vectors with word2vec.
These word vectors are then used as part of a model for a target task, giving a score that indicates the quality of the curriculum.
Based on this score the weights for the model are updated, using a form of Bayesian optimisation.&lt;/p&gt;

&lt;p&gt;One really nice aspect of this paper is the range of tasks considered: sentiment analysis, NER, POS tagging, and parsing.
Learning a curriculum does improve performance slightly, and which features are important varies across the tasks (indicating the importance of task-specific curriculums).
However, the models are somewhat restricted (as shown by the low absolute performance) because they do not change the word vectors during training.
For most of this paper that&amp;rsquo;s a reasonable decision, as it allows a clearer learning signal, but it would have been interesting to also see the impact on the normal training scenario and a state-of-the-art model.
In my experience (and in our soon-to-appear NAACL paper) we find that variations in word vectors can disappear during training for a downstream task.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/P16-1013&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{tsvetkov-EtAl:2016:P16-1,
  author    = {Tsvetkov, Yulia  and  Faruqui, Manaal  and  Ling, Wang  and  MacWhinney, Brian  and  Dyer, Chris},
  title     = {Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {130--139},
  url       = {http://www.aclweb.org/anthology/P16-1013}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (Wieting et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2018-01-31_sentencerepfromparaphrases/</link>
      <pubDate>Wed, 31 Jan 2018 19:25:36 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2018-01-31_sentencerepfromparaphrases/</guid>
      
        <description>

&lt;p&gt;It would be convenient to have a way to represent sentences in a vector space, similar to the way vectors are frequently used to represent input words for a task.
Quite a few sentence embeddings methods have been proposed, but none have really caught on.
Building on prior work by the same authors, the approach here is to define a neural network that maps a sentence to a vector, then train it with a loss function that measures similarity between the vectors for paraphrases.&lt;/p&gt;

&lt;p&gt;This paper scales up the approach, using millions of paraphrases, and explores a range of models.
To get the paraphrases they use translation (start with a sentence, translate it to another language and back, then assume the translation is a paraphrase).
For negative examples they use the sentence that the model currently thinks is most similar other than the correct one (choosing this from a large enough set is key).&lt;/p&gt;

&lt;p&gt;The best model is very simple - concatenate together the average of word vectors and the average of character trigram vectors.
That consistently beats prior work, including convolutional models, and LSTMs.
In a way, this is nice as it is a simple way to get a sentence representation!
On the other hand, this can&amp;rsquo;t possibly capture the semantics of a sentence fully since it doesn&amp;rsquo;t take word order into consideration at all.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.05732&#34; target=&#34;_blank&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv171105732W,
  author        = {{Wieting}, J. and {Gimpel}, K.},
  title         = {Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations},
  journal       = {ArXiv e-prints},
  archivePrefix = {arXiv},
  eprint        = {1711.05732},
  primaryClass  = {cs.CL},
  year          = {2017},
  month         = {November},
  url           = {https://arxiv.org/abs/1711.05732},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</title>
      
      <link>http://www.jkk.name/post/2018-01-28_crowdassistant/</link>
      <pubDate>Sun, 28 Jan 2018 16:01:20 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2018-01-28_crowdassistant/</guid>
      
        <description>

&lt;p&gt;There is a lot of interest in dialogue agents, but a lot of work sits at one of two extremes: either chit-chat agents that just chat, or task-oriented agents that aim to call a specific API for the user.
This work is about trying to integrate a range of systems from both categories, to get something more general purpose as a result.&lt;/p&gt;

&lt;p&gt;The core approach is a hybrid system that switches between different agents behind the scenes (an approach taken by a number of Alexa Prize teams).
The innovation here is that crowd workers will help with the decision (both suggesting things to say and voting on which response to use), and their votes will be used to learn a model to (partially) replace the people over time.&lt;/p&gt;

&lt;p&gt;Unfortunately, the improvement from a learned model of votes is only small (saves only 14% of the crowd effort), and the automated responses are rarely chosen (12% of the time).
That said, it seems like an interesting design with a lot of subtle decisions that require more exploration - the sets of agents (4-6 here, mostly narrow types), the voting scheme (only 1 or 2 votes needed here), choosing which agent responses to show (here, the proportion of previously accepted messages from this agent), and so on.
That choice of which responses to show is particularly tricky, as with this scheme a very domain specific agent might get voted down too much initially and never be chosen when the appropriate time comes.
One potentially interesting alternative would be to let the crowd workers choose which agent&amp;rsquo;s response to see, and possibly even post-edit slightly.&lt;/p&gt;

&lt;p&gt;Note - This post is the first of a (hopefully) regular series again.
However, rather than keeping it weekday-ly, I plan to do three times a week, at least until the ACL deadline.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{blah,
  title = {Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time},
  author = {Ting-Hao (Kenneth) Huang, Joseph Chee Chang, and Jeffrey P. Bigham},
  booktitle = {CHI},
  year = {2018},
  url = {https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings (Yang et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-12-12_multidomainwordvector/</link>
      <pubDate>Tue, 12 Dec 2017 20:25:40 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-12-12_multidomainwordvector/</guid>
      
        <description>

&lt;p&gt;To construct word vectors from multi-domain data, use a separate vector for each domain and add a loss term to encourage them to agree.
Here the loss is an l2 norm, weighted by a factor that depends on the frequency of the words in the two domains.
The factor is the harmonic mean of the normalised frequency in each domain (so the lower frequency dominates the factor, pulling it lower).
Across a range of tasks this consistently performs better than other approaches.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1312&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{yang-lu-zheng:2017:EMNLP2017,
  author    = {Yang, Wei  and  Lu, Wei  and  Zheng, Vincent},
  title     = {A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2898--2904},
  url       = {https://www.aclweb.org/anthology/D17-1312}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>The strange geometry of skip-gram with negative sampling (Mimno et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-12-12_wordvectorgeometry/</link>
      <pubDate>Tue, 12 Dec 2017 20:15:34 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-12-12_wordvectorgeometry/</guid>
      
        <description>

&lt;p&gt;It turns out that if the vectors learned by word2vec are projected into a plane they all point in the same direction.
Also, the context vectors (which are part of the algorithm, but not retained afterwards) point the other way.
When visualising with t-SNE this effect is not visible because of the way the space is warped to optimise the t-SNE objective.&lt;/p&gt;

&lt;p&gt;This is surprising, and may seem problematic since it doesn&amp;rsquo;t fit our goals for what these vectors should be capturing.
However, it doesn&amp;rsquo;t seem to impact downstream tasks, for example, GloVe does not have this property, and doesn&amp;rsquo;t seem to derive a great benefit from it.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1308&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{mimno-thompson:2017:EMNLP2017,
  author    = {Mimno, David  and  Thompson, Laure},
  title     = {The strange geometry of skip-gram with negative sampling},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2873--2878},
  url       = {https://www.aclweb.org/anthology/D17-1308}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Sequence Effects in Crowdsourced Annotations (Mathur et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-12-08_crowdbias/</link>
      <pubDate>Fri, 08 Dec 2017 19:49:09 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-12-08_crowdbias/</guid>
      
        <description>

&lt;p&gt;Getting high quality annotations from crowdsourcing requires careful design.
This paper looks at how one annotation a worker does can influence their next annotation, for example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When scoring translations, a good example may make the next one look worse in comparison&lt;/li&gt;
&lt;li&gt;For labeling tasks, we may expect a long sequence of the same label to be rare (the gambler&amp;rsquo;s fallacy)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To investigate this they fit a linear model with inputs (previous label, gold label, random noise) and see what the coefficients are.
Across multiple tasks, there is a non-zero correlation with the previous label.
Interestingly, there also seems to be a learning effect for good workers, where over time they become calibrated and show less sequence bias.
Fortunately, there is a simple solution - for each worker, give every annotator their documents in a different random order!
With that change, averaging over annotations should avoid this bias.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1306&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{mathur-baldwin-cohn:2017:EMNLP2017,
  author    = {Mathur, Nitika  and  Baldwin, Timothy  and  Cohn, Trevor},
  title     = {Sequence Effects in Crowdsourced Annotations},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2860--2865},
  url       = {https://www.aclweb.org/anthology/D17-1306}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-12-07_rarewordvectors/</link>
      <pubDate>Thu, 07 Dec 2017 20:45:39 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-12-07_rarewordvectors/</guid>
      
        <description>

&lt;p&gt;Word vectors are great for common words, but what about rare words?
People can have a fairly good understanding of a word given only a few instances, but it&amp;rsquo;s fairly standard to turn all words with a frequency of less than 5 into UNK when learning word vectors.&lt;/p&gt;

&lt;p&gt;One simple approach is to add up word vectors from the context of the rare word and use that as the representation.
This paper proposes using a tweaked version of word2vec: keep vectors for frequent words fixed, increase the learning rate, use a fixed width context window, initialise with the additive approach, and only subsample by discarding frequent words.
All of those make sense, though I am curious whether it would be better to just decrease subsampling or disable it entirely.&lt;/p&gt;

&lt;p&gt;The results are mixed, with the improvement over the additive approach data dependent.
That might partly reflect the tasks though - something downstream like POS tagging would have been interesting, particularly since the LSTM may already be capturing contextual information that covers what the additive approach has, but not what this adds.
Ultimately this is not a solution to this problem, but it&amp;rsquo;s an idea to keep in mind.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1030&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{herbelot-baroni:2017:EMNLP2017,
  author    = {Herbelot, Aur\&#39;{e}lie  and  Baroni, Marco},
  title     = {High-risk learning: acquiring new word vectors from tiny data},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {304--309},
  url       = {https://www.aclweb.org/anthology/D17-1030}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Revisiting Selectional Preferences for Coreference Resolution (Heinzerling et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-12-06_coreferencearguments/</link>
      <pubDate>Wed, 06 Dec 2017 19:05:20 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-12-06_coreferencearguments/</guid>
      
        <description>

&lt;p&gt;Selectional preferences in this context are about how some verbs are more likely to take certain types of arguments (e.g. people laugh, computers do not).
Many papers have added features or structures to coreference systems aiming to get at this kind of information.
This paper presents another way of doing it and experiments that probe how useful it is (punchline: not very).&lt;/p&gt;

&lt;p&gt;Their approach is to parse a large amount of text, producing noun-verb pairs.
They learn vector representations of the relations and try to create a single space containing both entities and relations (e.g. Michigan gets a vector, as does attended@dobj).
The goal is that entities end up in locations similar to the locations of relations they are selected for.&lt;/p&gt;

&lt;p&gt;For results, first it seems like these vector similarities do not correlate particularly strongly with being coreferent.
It could be that the feature on its own isn&amp;rsquo;t enough, or this representation might not be capturing it effectively.
Adding this to the Stanford coreference system they are able to get slight gains, though the improvement might not be statistically significant.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not sure exactly how to do this, but it would be neat if a vector at some point of the model could be modified to remove any correlation with these features, and see what that does to performance.
If performance remains high, then this actually is an uninformative feature, but if it drops that suggests the model is already learning it.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1138&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{heinzerling-moosavi-strube:2017:EMNLP2017,
  author    = {Heinzerling, Benjamin  and  Moosavi, Nafise Sadat  and  Strube, Michael},
  title     = {Revisiting Selectional Preferences for Coreference Resolution},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {1332--1339},
  url       = {https://www.aclweb.org/anthology/D17-1138}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Neural Semantic Parsing over Multiple Knowledge-bases (Herzig et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-12-05_multidomainparsing/</link>
      <pubDate>Tue, 05 Dec 2017 19:28:33 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-12-05_multidomainparsing/</guid>
      
        <description>

&lt;p&gt;One reason learning for semantic parsing is difficult is that the datasets are generally small.
Assuming some words behave similarly across domains, multi-domain parsing should improve performance by providing more data, which is essentially what this paper finds.
They consider several configurations, all based on a sequence to sequence LSTM:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Train a separate model for every domain.&lt;/li&gt;
&lt;li&gt;Use a single model. They do three subtypes here, (a) that&amp;rsquo;s it, (b) add an LSTM input at each step with the domain, &amp;copy; give the domain as a token at the start.&lt;/li&gt;
&lt;li&gt;Use a single encoder model, but a different decoder for each domain.&lt;/li&gt;
&lt;li&gt;Combine (1) and (3), have two encoders, one that is domain specific and one that is trained on all domains.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The results show that any of these does better than (1), with (2b) doing best.
There also seems to be three sections: first the independent models (1), then the models with multiple decoders (3 and 4), then the variants of (2).
A natural thing to try would be a version of (4) with a single decoder, in which case the thing that is shared is the output space representation (rather than the input space as the motivation for the paper frames it).
From the paper it sounds like very little hyperparameter tuning was tried, which is a shame because it makes it less clear how definitive the results are.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-2098&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{herzig-berant:2017:Short,
  author    = {Herzig, Jonathan  and  Berant, Jonathan},
  title     = {Neural Semantic Parsing over Multiple Knowledge-bases},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {623--628},
  url       = {http://aclweb.org/anthology/P17-2098}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-12-05_explainingpredictions/</link>
      <pubDate>Tue, 05 Dec 2017 15:40:45 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-12-05_explainingpredictions/</guid>
      
        <description>

&lt;p&gt;Interpreting the behaviour of statistical models in NLP has been hard for a long time, but it has gotten even harder with nonlinear models.
The simplest method so far in NLP has been to look at the attention distributions in sequence to sequence models, but that doesn&amp;rsquo;t provide everything we need and obviously only applies when the model has attention.
For looking at the dynamics of the hidden state in an LSTM the Harvard NLP group built a cool &lt;a href=&#34;http://lstm.seas.harvard.edu/&#34; target=&#34;_blank&#34;&gt;visualisation&lt;/a&gt;, but what about structured outputs?&lt;/p&gt;

&lt;p&gt;This paper considers sequence to sequence models and determines which parts of the input were most important for determining each part of the output.
The steps are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use a variational autoencoder to get perturbed versions of the input&lt;/li&gt;
&lt;li&gt;Use logistic regression to get scores for every output symbol indicating how sensitive it is to variations in parts of the input&lt;/li&gt;
&lt;li&gt;Create a bipartite graph between inputs and outputs, then find high weight components in the graph&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These components serve as the representation of which parts of the input determine which parts of the output.
Experiments show results that match with past observations and intuitions, which is good for supporting the effectiveness of the method, but it&amp;rsquo;s a shame this didn&amp;rsquo;t uncover any exciting new patterns.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1042&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{alvarezmelis-jaakkola:2017:EMNLP2017,
  author    = {Alvarez-Melis, David  and  Jaakkola, Tommi},
  title     = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {412--421},
  url       = {https://www.aclweb.org/anthology/D17-1042}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>A Local Detection Approach for Named Entity Recognition and Mention Detection (Xu et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-12-01_nonsequencener/</link>
      <pubDate>Fri, 01 Dec 2017 15:28:59 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-12-01_nonsequencener/</guid>
      
        <description>

&lt;p&gt;The classic NER system is a model that has a lot of curated features, like lists of people, and does inference by choosing the top scoring tag sequence for the whole sentence, using Viterbi decoding.
The neural version swaps the curated features for word vectors, and viterbi inference for an LSTM (maybe with beam search).
This paper makes the argument that in reality people are very good at identifying an entity in isolation, so why do global decoding for the best tag sequence?&lt;/p&gt;

&lt;p&gt;Given that perspective, they make a model that scores every span of the sentence independently using a feedforward network.
To get an input representing context, they use a weighted sum of word embeddings, where the weights decay exponentially further from the span (FOFE = Fixed-size Ordinally Forgetting Encoding).
The authors point out that this gives a fixed length encoding that could be reversed to recover the original sequence (assuming arbitrary precision floating point numbers).
Thinking about the calculation though, a word ten positions away is having its vector scaled down by a factor of a thousand, so it probably has negligible impact on the decision.
They also apply this idea to the characters of the span itself in both directions.&lt;/p&gt;

&lt;p&gt;One tradeoff with the independent classification idea is that it can select overlapping spans.
This is a benefit in one sense, because it naturally handles nested entities (e.g. &amp;ldquo;[Member of the Order of [Australia]]&amp;ldquo;), but for partially overlapping spans we have to decide which to keep.
Their solution is to sort by model score and keep the higher scoring option.&lt;/p&gt;

&lt;p&gt;The experiments show this is comparable with previous work using LSTMs.
There were a few things I found interesting in the results:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The FOFE encoding for characters is far worse than a CNN encoding when on their own, but give similar gains when combined with word level features. Since the FOFE essentially ignores the centre of long spans, this suggests they are both learning some representation of prefixes and suffixes.&lt;/li&gt;
&lt;li&gt;They don&amp;rsquo;t try it, but this model seems very amenable to gazetteers, which may be a way to further boost performance.&lt;/li&gt;
&lt;li&gt;They have an in-house dataset of 10,000 manually labeled documents (!), but it only gives a 3% gain on the KBP evaluation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1114&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{xu-jiang-watcharawittayakul:2017:Long,
  author    = {Xu, Mingbin  and  Jiang, Hui  and  Watcharawittayakul, Sedtawut},
  title     = {A Local Detection Approach for Named Entity Recognition and Mention Detection},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1237--1247},
  url       = {http://aclweb.org/anthology/P17-1114}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme (Zheng et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-30_taggingrelations/</link>
      <pubDate>Thu, 30 Nov 2017 20:01:41 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-30_taggingrelations/</guid>
      
        <description>

&lt;p&gt;This paper considers the task of identifying named entities in a sentence and the relations between them.
The contribution is a way of formulating the task as tagging, so a bi-directional LSTM can be applied.&lt;/p&gt;

&lt;p&gt;The tags are like in NER (Begin, Inside, End, Single, Outside), but rather than Person, Location, etc, they label each entity with the relation it is participating in, and whether it is in role one or two for the relation.
Applying a two layer bidirectional LSTM to this set up gets to state-of-the-art precision on news data.
To get SotA F-score they modify the loss to place less weight on Outside tags, which raises recall at the cost of precision.&lt;/p&gt;

&lt;p&gt;One catch with this approach is handling multiple relations of the same type.
The solution here is to link pairs that are closest together (unclear what they do for nesting).
That doesn&amp;rsquo;t handle overlapping relations, which the authors say is particularly common in the BioInfer data (I&amp;rsquo;m curious how much it is hurting here too).
It&amp;rsquo;s unclear how this could be addressed without a radical redesign, since extending the tag scheme could lead to sparsity issues.&lt;/p&gt;

&lt;p&gt;I was not familiar with this data, so I looked back to the original paper the annotated test data came from: &lt;a href=&#34;http://www.aclweb.org/anthology/P11-1055&#34; target=&#34;_blank&#34;&gt;Hoffman et al., (2011)&lt;/a&gt;.
There is no dev set, only a 395 sentence test set, so the standard practise is to use random 10% samples of the test data for development.
Also, if I understand it correctly, the data was annotated by manually confirming the output of systems, which means it will have recall errors.
If interest in this data grows, going back and annotating more seems worthwhile.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1113&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{zheng-EtAl:2017:Long,
  author    = {Zheng, Suncong  and  Wang, Feng  and  Bao, Hongyun  and  Hao, Yuexing  and  Zhou, Peng  and  Xu, Bo},
  title     = {Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1227--1236},
  url       = {http://aclweb.org/anthology/P17-1113}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Abstractive Document Summarization with a Graph-Based Attentional Neural Model (Tan et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-29_abstractivesummarisation/</link>
      <pubDate>Wed, 29 Nov 2017 19:14:05 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-29_abstractivesummarisation/</guid>
      
        <description>

&lt;p&gt;Most effective summarisation systems are extractive, selecting the most important sentences in a document and sticking them together.
Clearly that is not how people write summaries, but creating abstractive summaries means generating fluent language.
At the same time, most datasets are based on news text, where the first few sentences are a strong baseline summary (by design, as journalists need to assume that the reader could stop at any point).
This paper introduces several ideas to get state-of-the-art results on summarisation using an abstractive system.&lt;/p&gt;

&lt;p&gt;There are three core new ideas, one for decoding and two for the model.
The idea in decoding is a beam search in which the score is increased when adding bigrams that occur in the source but are not in the output.
In the model, they propose a new form of attention based on PageRank, similar to previous methods used for ranking sentences in summarisation.
For every pair of sentences plus the current decoder hidden vector, a similarity score is calculated ($h_1 M h_2$), where $M$ is a matrix of parameters.
This produces a matrix of similarities, which they run PageRank on with initialisation set so that all weight starts on the decoder hidden vector.
That produces a score for each input sentence, which is normalised to get attention values.
The second idea is that they don&amp;rsquo;t want to attend to the same sentence multiple times, so before normalising they subtract the previous score for that sentence (with it capped at 0 to avoid negative values).&lt;/p&gt;

&lt;p&gt;Together, these lead to state of the art results, beating both extractive and abstractive systems.
Though in human evaluation using the first three sentences as a summary remains a very strong baseline, only slightly behind this system on informativeness and ahead on coherence and fluency.
Ablation shows that the decoding idea has the biggest impact, but the graph based attention does help.
Interestingly, if the score in decoding is extremely biased to focus on the bigram addition aspect performance only decreases a little.
That may reflect the nature of the metric, which is based on ngram overlap.&lt;/p&gt;

&lt;p&gt;There are also a bunch of little details that may be crucial, like adding markers for entities (which seems like a possible space for a more elegant solution).
I&amp;rsquo;m not sure the beam search scoring idea has applications beyond summarisation, but thee modified attention might!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1108&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{tan-wan-xiao:2017:Long,
  author    = {Tan, Jiwei  and  Wan, Xiaojun  and  Xiao, Jianguo},
  title     = {Abstractive Document Summarization with a Graph-Based Attentional Neural Model},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1171--1181},
  url       = {http://aclweb.org/anthology/P17-1108}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>SPINE: SParse Interpretable Neural Embeddings (Subramanian et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-28_interpretableembeddings/</link>
      <pubDate>Tue, 28 Nov 2017 16:51:09 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-28_interpretableembeddings/</guid>
      
        <description>

&lt;p&gt;The first step in almost any neural network model for language is to look up a vector for each token in the input.
These vectors express relations between the words, but it is difficult to know exactly what relations.
This work proposes a way to modify a vector space of words to have more interpretable dimensions.&lt;/p&gt;

&lt;p&gt;The core idea is actually more general, it is a new loss that encourages sparsity in an auto-encoder.
In this case the model is very simple: input a word vector, apply an affine transformation and a pointwise nonlinearity, producing a hidden vector, then apply another affine transformation to get the output.
The loss is a combination of how well the input and output match (reconstruction loss), plus a function that is minimised when the average activation is below a threshold (average sparsity loss), and the new idea, a loss that is minimised at either 0 or 1 for each hidden value.
To get the hidden values to be bounded between 1 and 0, the nonlinearity used is a modified ReLU that stops increasing after reaching 1.
After training, the hidden values become the new word vectors.&lt;/p&gt;

&lt;p&gt;To evaluate interpretability they consider the top 4 words along each dimension, add a random word, and ask a person to identify the odd word out.
Using either word2vec or GloVe as the initial vectors and applying this method, the results shown a dramatic difference (~25 vs. ~70).
On downstream tasks the story is more mixed.
With 1,000 dimensional vectors, there is usually an improvement for GloVe, but not for word2vec, and the differences are generally small.
Apparently going up to 2,000 further improves interpretability scores, but &amp;lsquo;at a severe cost&amp;rsquo; for the downstream tasks.
Going the other direction, to 500, hurts interpretability, and probably doesn&amp;rsquo;t improve downstream performance (it isn&amp;rsquo;t mentioned).&lt;/p&gt;

&lt;p&gt;I would be curious to see if taking these new word vectors and applying them to a downstream task like parsing, but letting them change during training, would be beneficial.
The general idea of a sparse auto-encoder also seems cool and may have other applications.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.08792&#34; target=&#34;_blank&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv171108792S,
  author        = {{Subramanian}, A. and {Pruthi}, D. and {Jhamtani}, H. and {Berg-Kirkpatrick}, T. and {Hovy}, E.},
  title         = {SPINE: SParse Interpretable Neural Embeddings},
  journal       = {ArXiv e-prints},
  archivePrefix = {arXiv},
  eprint        = {1711.08792},
  primaryClass  = {cs.CL},
  year          = {2017},
  month         = {November},
  url           = {https://arxiv.org/abs/1711.08792},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Ordinal Common-sense Inference (Zhang et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-27_commonsense/</link>
      <pubDate>Mon, 27 Nov 2017 11:21:45 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-27_commonsense/</guid>
      
        <description>

&lt;p&gt;When people read a sentence they form an entire world around it, making inferences about unwritten properties based on their prior knowledge.
If we want NLP systems to do the same, we need data to train and test this common sense aspect of language understanding.&lt;/p&gt;

&lt;p&gt;This paper is about a new dataset of automatically generated sentence pairs with human ratings.
The ratings indicate that given the first sentence, the second sentence is either very likely, likely, plausible, technically possible, or impossible.
These ratings are crowdsourced, using the median of three ratings per example.
The pay rates are fairly low, at $3.45 / hour (1.99c / example and 20.71 seconds / example), though it&amp;rsquo;s possible that the time is being skewed by outliers, and it&amp;rsquo;s unclear exactly how pay was determined (does this include Amazon&amp;rsquo;s cut? Why is it an average cost per example, rather than just the cost?).&lt;/p&gt;

&lt;p&gt;The main contribution is the novel way of generating the sentences.
For each prompt sentence, an argument is chosen, and then a hypothesis is generated in one of three ways (all trained with Gigaword).
(1) A sequence-to-sequence model takes the full sentence as input and generates a sentence.
(2) The same as (1), but with only the argument provided.
(3) A sentence is sampled from templates generated by abstraction of sentences in the training data.
Together these produce a diverse set of examples that get a range of ratings, with only &amp;lsquo;likely&amp;rsquo; being somewhat rarer.
They also labeled some pairs from SNLI and COPA, to enable analysis of how this task compares.&lt;/p&gt;

&lt;p&gt;They also provide a set of baselines for the new task.
Using the baselines, they show that the generated sentences are somewhat more difficult than the pairs from existing datasets.
The standard metrics proposed are MSE and Spearman&amp;rsquo;s Rho (both necessary because otherwise always guessing the middle would get an MSE better than any of the proposed baselines).
Interestingly, regression does quite a bit better than a set of one-vs-all SVMs on MSE, and also slightly better on rho (I&amp;rsquo;m surprised because while there is an ordinal scale, it doesn&amp;rsquo;t feel like it should have a strong continuous interpretation).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1082&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1082,
	author = {Zhang, Sheng  and Rudinger, Rachel  and Duh, Kevin  and Van Durme, Benjamin },
	title = {Ordinal Common-sense Inference},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	keywords = {},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1082},
	pages = {379--395}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Error-repair Dependency Parsing for Ungrammatical Texts (Sakaguchi et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-22_errorrepairparsing/</link>
      <pubDate>Wed, 22 Nov 2017 15:48:53 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-22_errorrepairparsing/</guid>
      
        <description>

&lt;p&gt;This work presents a system that parses sentences and identifies grammatical errors simultaneously.
It&amp;rsquo;s an intuitive combination - a syntactic model should assign higher probability to a parse for a fixed version of a sentence than the one with a mistake.&lt;/p&gt;

&lt;p&gt;They build on an incremental &amp;lsquo;easy-first&amp;rsquo; dependency parsing approach.
Easy-First parsing starts with the set of words in the sentence and allows an edge to be created between any adjacent pair of words.
Once an edge is created, the child is hidden beneath its parent, so now the parent is effectively adjacent to a word slightly further away.
Then the process repeats, until there is only one word left (the root of the sentence).
In a way it is like following a dynamic program, but with only a single state that ties together multiple cells.&lt;/p&gt;

&lt;p&gt;The change in this paper is the addition of actions that insert a word, delete a word, or alter a word.
To make it work, there are constraints to avoid cycles of repeated actions (e.g. insert-delete-insert-delete&amp;hellip;), and on the sets of allowed word substitutions.
To produce additional training data, a tool is used to inject errors into grammatical text.
On error detection, this approach does lead to improvements, though it changes a relatively small number of the sentences.
On dependency parsing it is (unsurprisingly) worse than a baseline system on grammatical text.
It does perform better on ungrammatical text, though the data is generated using the same process as the training data, creating a bias in the system&amp;rsquo;s favour.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-2030&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{sakaguchi-post-vandurme:2017:Short,
  author    = {Sakaguchi, Keisuke  and  Post, Matt  and  Van Durme, Benjamin},
  title     = {Error-repair Dependency Parsing for Ungrammatical Texts},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {189--195},
  url       = {http://aclweb.org/anthology/P17-2030}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-21_multiinputattention/</link>
      <pubDate>Tue, 21 Nov 2017 16:42:19 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-21_multiinputattention/</guid>
      
        <description>

&lt;p&gt;Attention, a weighted average over vectors with weights determined based on context (usually decoder state), has proven effective in many NLP tasks.
There are several variants, and this paper adds new types that address the question of how to apply attention to different sources at the same time, such as text and an image.&lt;/p&gt;

&lt;p&gt;They consider three general versions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Concatenation, just do attention separately then concatenate the vectors from the input sources&lt;/li&gt;
&lt;li&gt;Flat, do the weighted average over all of the inputs&lt;/li&gt;
&lt;li&gt;Hierarchical, do attention separately, but then combine the vectors with another phase of attention&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They also explore two variations that are orthogonal to the list above:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first step and the last step in attention both involve the input vectors being multiplied by a weight matrix. Should that matrix be shared for the two steps, or different? (the first informs the decision of what to give high weight in the average, the second determines what is being averaged over)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;sentinel gates&lt;/em&gt;, a modification to the way the inputs and context vector are combined that allow one or the other to be ignored.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They consider two tasks, (1) translation when both an image and source sentence are given, (2) post-editing a translated sentence with the original source given.
The results show fairly clear trends, though the systems are not great compared to baselines (worse than a text only baseline for the first, and only slightly better than a direct MT system for the second).
The trends are that hierarchical is best, the sentinel doesn&amp;rsquo;t help, and it is better to not share weights (though I wonder if that would be true when controlling for the total number of parameters).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-2031&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{libovicky-helcl:2017:Short,
  author    = {Libovick\&#39;{y}, Jind\v{r}ich  and  Helcl, Jind\v{r}ich},
  title     = {Attention Strategies for Multi-Source Sequence-to-Sequence Learning},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {196--202},
  url       = {http://aclweb.org/anthology/P17-2031}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Robust Incremental Neural Semantic Graph Parsing (Buys et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-20_mrsparser/</link>
      <pubDate>Mon, 20 Nov 2017 10:13:12 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-20_mrsparser/</guid>
      
        <description>

&lt;p&gt;Like the &lt;a href=&#34;http://www.jkk.name/post/2017-11-16_ucca/&#34; target=&#34;_blank&#34;&gt;UCCA parser&lt;/a&gt;, this paper explores a transition-based neural model for semantic parsing, but for Minimal Recursion Semantics instead of Universal Conceptual Cognitive Annotation.
Comparing MRS and UCCA, every word gets a non-terminal symbol in MRS, plus additional non-terminals for phenomena like quantification, while UCCA only introduces them for special cases like linking to a coordination.
Both have discontinuous graph structures, creating a challenge for most parsers.&lt;/p&gt;

&lt;p&gt;The UCCA and MRS parsers extend the basic shift-reduce transitions in different ways.
Here, crossing edges can be added with a transition that forms edges between the front of the buffer and a word anywhere in the stack, while the UCCA parser used swapping and a additional reduce actions for graph edges.
The models are similar, both using a form of stack-RNN, but with different structures (partly as a result of the different transition schemes).
The results in this case are not state-of-the-art, though this task has received more attention, and the data is slightly biased (the parser that does better, ACE, is based on the grammar that was used to determine which sentences to include).
However, the system can also be applied to AMR, and does fairly well, better than other neural AMR parsers at the time (and more recent ideas for improvements are large orthogonal).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1112&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{buys-blunsom:2017:Long,
  author    = {Buys, Jan  and  Blunsom, Phil},
  title     = {Robust Incremental Neural Semantic Graph Parsing},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1215--1226},
  url       = {http://aclweb.org/anthology/P17-1112}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>A Two-Stage Parsing Method for Text-Level Discourse Analysis (Wang et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-17_twostagediscourseparsing/</link>
      <pubDate>Fri, 17 Nov 2017 18:40:20 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-17_twostagediscourseparsing/</guid>
      
        <description>

&lt;p&gt;Discourse parsing for Rhetorical Structure Theory is difficult partly because it involves a range of relation types at different scales (within and between sentences) and partly because there is relatively little annotated data available.
To deal with the limited data, this paper breaks the task into two parts: (1) identify relations, (2) assign labels.
Their system is state-of-the-art, and an ablation shows that the division of tasks helps performance.
They also divide up the labeling step to have different classifiers for within sentences, between sentences in the same paragraph, and between paragraphs, which also helps a little.&lt;/p&gt;

&lt;p&gt;I find the second improvement surprising, since an expanded feature set for a single classifier would be able to emulate their multi-classifier model, while having the advantage of sharing information between classes.
The first improvement is more intuitive (a denser space makes for an easier problem), though I wonder whether this will be one point on the back-and-forth that usually occurs between sequential and joint models (with joint models usually winning in the end).
This paper also continues the trend of transition-based inference applying effectively to tasks, which makes sense if our models are getting good enough that search errors are not a major issue.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-2029&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{wang-li-wang:2017:Short,
  author    = {Wang, Yizhong  and  Li, Sujian  and  Wang, Houfeng},
  title     = {A Two-Stage Parsing Method for Text-Level Discourse Analysis},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {184--188},
  url       = {http://aclweb.org/anthology/P17-2029}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>A Transition-Based Directed Acyclic Graph Parser for UCCA (Hershcovich et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-16_ucca/</link>
      <pubDate>Thu, 16 Nov 2017 17:24:59 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-16_ucca/</guid>
      
        <description>

&lt;p&gt;Over the last few years interest has risen in parsing structures other than projective trees (including my dissertation!).
There are now a range of different datasets with annotations for syntactic and/or semantic structure that include discontinuous constituents and graphs.
This paper looks at UCCA, a proposed formalism that is somewhat similar to SRL, with non-terminals included to allow for easier handling of cases like coordination.&lt;/p&gt;

&lt;p&gt;The parser is a transition based, with a transition system that covers all the structural phenomena in UCCA: non-terminals, discontinuous spans, and multiple parents.
The key to consistent multiple parents is distinguishing the addition of edges that are the primary parent (to prevent multiple being added).
To get discontinuity, they use a swap operation.
They consider a range of models, including both linear and neural network examples.&lt;/p&gt;

&lt;p&gt;The dataset is relatively small, with only 4,268 training sentences, and the task is hard, so performance is relatively low (50 - 75 for primary edges, 20-50 for others).
The neural model consistently beats the linear ones, particularly for the non-primary edges.
Comparing to other standard parsers (retrained on this data), the ability to generate the full space of structures makes a big difference.&lt;/p&gt;

&lt;p&gt;It would be interesting to see coverage of this data for one-endpoint crossing graphs.
If it is high, then my own parser could be applied fairly directly!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1104&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hershcovich-abend-rappoport:2017:Long,
  author    = {Hershcovich, Daniel  and  Abend, Omri  and  Rappoport, Ari},
  title     = {A Transition-Based Directed Acyclic Graph Parser for UCCA},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1127--1138},
  url       = {http://aclweb.org/anthology/P17-1104}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-15_entityvectors/</link>
      <pubDate>Wed, 15 Nov 2017 18:01:27 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-15_entityvectors/</guid>
      
        <description>

&lt;p&gt;Since word2vec was released there have been a series of X2vec papers, though none have had the success of word vectors.
In this case the idea is to represent entities and chunks of text (words, sentences, paragraphs).&lt;/p&gt;

&lt;p&gt;Entities are represented with vectors.
To get the vector for a chunk of text, they:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Sum word vectors for the text.&lt;/li&gt;
&lt;li&gt;Rescale to be of unit length.&lt;/li&gt;
&lt;li&gt;Multiply by a weight matrix and add a bias.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then to learn these, negative log likelihood is used, where the probability is defined as a softmax over the dot product between entity and text vectors.
The data is a portion of Wikipedia annotated with entities as indicated by links (plus they say the entity the page is about is implicitly part of every sentence).&lt;/p&gt;

&lt;p&gt;With these new vectors in hand, they try textual similarity, with strong results.
They also build a very simple entity linking system, a feed-forward network with these representations plus a few other features, and beat all prior work.
Similarly
They apply the same modeling approach to Quizball QA, also with strong results.&lt;/p&gt;

&lt;p&gt;The simplicity and effectiveness of the model really is impressive.
Some qualitative examples are included, but hard to find trends in.
It does seem like a more reasonable vector learning approach than skip-thought and other similar approaches that rely only on text context - the entities provide something different, but clearly closely related.
That said, I feel like more ablation is needed to see what role each of these pieces is playing (are they learning better vectors, or using them in a way that is more effective? Or both?).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1065&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1065,
	author = {Yamada, Ikuya  and Shindo, Hiroyuki  and Takeda, Hideaki  and Takefuji, Yoshiyasu },
	title = {Learning Distributed Representations of Texts and Entities from Knowledge Base},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1065},
	pages = {397--411}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>In-Order Transition-based Constituent Parsing (Liu et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-14_inorderparsing/</link>
      <pubDate>Tue, 14 Nov 2017 14:10:39 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-14_inorderparsing/</guid>
      
        <description>

&lt;p&gt;Shift-reduce constituency parsing incrementally builds the parse either bottom-up or top-down.
The difference is whether a non-terminal is placed on the stack before or after the words that it spans.
This corresponds to two forms of depth-first traversal of the tree: pre-order or post-order.&lt;/p&gt;

&lt;p&gt;The idea in this paper is to do an in-order traversal, which in a binary tree means traversing the left child of a node, then the node, then its right child.
In this context that means putting the non-terminal symbol on the stack after the first word it spans, but before the rest.
The model follows the stack-LSTM approach of Dyer et al., with non-terminals always fed into the LSTM first during composition, regardless of where it was inserted into the stack.&lt;/p&gt;

&lt;p&gt;This leads to a 0.5 F1 gain on standard parsing metrics, with no hyperparameter tuning.
High-level error analysis seems to show it just does better everywhere.
I wonder whether further gains could be realised with a label-sensitive ordering.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1199&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1199,
	author = {Liu, Jiangming  and Zhang, Yue },
	title = {In-Order Transition-based Constituent Parsing},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1199},
	pages = {413--424}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog (Kottur et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-13_languagegame/</link>
      <pubDate>Mon, 13 Nov 2017 09:47:08 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-13_languagegame/</guid>
      
        <description>

&lt;p&gt;In reference games, two players communicate in a shared world with the goal of one learning what the other is referring to.
Their small scale and clear success criteria make them a convenient testbed for dialogue agents, going back decades, with recent work focusing on neural approaches.
This paper considers a simple game and constrains models in various ways to improve performance and see how their communication varies, a line of work also appearing in recent papers by Jacob Andreas (&lt;a href=&#34;http://aclweb.org/anthology/P/P17/P17-1022.pdf&#34; target=&#34;_blank&#34;&gt;ACL 2017&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.info/pdf/D/D17/D17-1310.pdf&#34; target=&#34;_blank&#34;&gt;EMNLP 2017&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The game in this case is to find out two properties of an object, where there are three possible properties, each with four possible values.
Given enough flexibility, models will explicitly encode every possible structure of the world as a separate symbol, which does not generalise well.
Limiting the vocabulary to one symbol per property and one per value helps, but in this particular game there are only 3 possible questions, and over two turns of dialogue the 12 value words are sufficient to encode the space.
Limiting even further, to 4 words for values and providing each turn in isolation to the answerer does lead to some compositionality, but clearly not full compositionality as they still make errors on unseen combinations of the inputs.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a short paper, so they can only do so much, but some experiments I am curious about are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Decrease the questioner vocabulary to 2. This avoids the problem that the questioner can express the task in one step by saying what is not needed. It&amp;rsquo;s still doable, by defining an order for questions, e.g. ask about attribute A vs. B first, then in the second step ask about either C or the other option from the first step. This is a little weird as symbols need to mean different things at different time steps, but would be interesting.&lt;/li&gt;
&lt;li&gt;Increase the number of attributes to 4. This also avoids the task expression problem, by forcing there to be compositionality on the questioner side (watching the video of the talk, someone asked this in the question time, and they didn&amp;rsquo;t know).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1320&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kottur-EtAl:2017:EMNLP2017,
  author    = {Kottur, Satwik  and  Moura, Jos\&#39;{e}  and  Lee, Stefan  and  Batra, Dhruv},
  title     = {Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2952--2957},
  url       = {https://www.aclweb.org/anthology/D17-1320}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-10_kginlstm/</link>
      <pubDate>Fri, 10 Nov 2017 15:37:15 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-10_kginlstm/</guid>
      
        <description>

&lt;p&gt;Structured information sources have been effectively used for entity detection and typing in linear models with the information encoded as binary features.
This paper looks at how to integrate vector representations of structured information into an LSTM.
The solution is an additional processing step during output generation, in which the vectors for relevant entities in the structured data are combined with the standard LSTM output (note, they do not affect the cell itself, so the information is not passed on through the recurrence).&lt;/p&gt;

&lt;p&gt;In this case the structured information is a set of tuples forming a graph of relations between entities, from either NELL or WordNet.
The actual encoding of entities is an application of prior work; vectors representing tuples are trained with the objective that the score for any tuple is higher than made-up tuples (where the score is $v_a M_r v_b$ for entities $a$ and $b$ in relation $r$).
The set of relevant entities for a particular word in the sentence is obtained by string matching, and then attention is used to combine them.
There is also a kind of gating mechanism to choose how big a role the entities play in the prediction, using a combination of the input, hidden state, and cell state.&lt;/p&gt;

&lt;p&gt;The results are interesting not only because this method helps, but because of how well the standard LSTM does on this task, matching or exceeding prior results.
This is even more impressive given how small ACE is (if I remember correctly).
The other key observations are that having a sequence level loss (using a CRF) helps, and NELL and WordNet seem to be providing different types of information (as using both leads to further improvements).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1132&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{yang-mitchell:2017:Long,
  author    = {Yang, Bishan  and  Mitchell, Tom},
  title     = {Leveraging Knowledge Bases in LSTMs for Improving Machine Reading},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1436--1446},
  url       = {http://aclweb.org/anthology/P17-1132}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-09_framesdataset/</link>
      <pubDate>Thu, 09 Nov 2017 19:47:08 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-09_framesdataset/</guid>
      
        <description>

&lt;p&gt;Another paper about a dataset of dialogues, but this time with structure.
Like the paper from yesterday, the aim is a dataset of task-oriented conversations, but with more complexity than prior work.
The difference is that this work includes a structured representation of the state of the conversation: frames.&lt;/p&gt;

&lt;p&gt;A frame is essentially a tuple describing a query, e.g. (Destination: Sydney, Origin: Ann Arbor, price: 1500 USD).
There are multiple frames in a dialogue (a departure from DSTC tasks), and utterances are labeled with dialogue acts that capture modifications to the frames as well as references to them.
This structure sounds fairly general, though the focus here was on vacation planning, where the user is buying a package.
The setup doesn&amp;rsquo;t maximise the potential complexity though, as there are a small number of set packages available, rather than the complex tradeoffs of flight+hotel combinations that exist in practise.
Looking at the example dialogues in the paper, it has complete sentences of some complexity.
One thing I&amp;rsquo;m still curious about is disagreements between annotators, as for the complete task the score was 0.62 +/- 5 (with dialogue acts being trickier than slot values, and no scores for frame references on their own).&lt;/p&gt;

&lt;p&gt;Comparing to the Stanford dataset this is smaller (11k vs. 1.4k), but has more turns per dialogue (11 vs. 15) and probably longer turns too, judging by the examples.
The tasks are completely different, but both come with small tables of information that are private to the two participants and required for almost every turn in the conversation.
Evaluating on both could be a great way to show the flexibility of a dialogue system, but the lack of frames for the Stanford data and the difficulty of running a human evaluation for this data limits the feasible types of multi-domain experiments.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/W17-5526&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{elasri-EtAl:2017:W17-55,
  author    = {El Asri, Layla  and  Schulz, Hannes  and  Sharma, Shikhar  and  Zumer, Jeremie  and  Harris, Justin  and  Fine, Emery  and  Mehrotra, Rahul  and  Suleman, Kaheer},
  title     = {Frames: a corpus for adding memory to goal-oriented dialogue systems},
  booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  month     = {August},
  year      = {2017},
  address   = {Saarbrucken, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {207--219},
  url       = {http://aclweb.org/anthology/W17-5526}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-08_graphdialogue/</link>
      <pubDate>Wed, 08 Nov 2017 18:46:04 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-08_graphdialogue/</guid>
      
        <description>

&lt;p&gt;Task-oriented dialogue systems are often focused on a very narrow task, to the point where the state can be described completely with a tuple (e.g. preferences for a restaurant).
This paper sets up a more challenging task with more complex language use, while still having a specific goal and directly relevant structured information.
They collected 11,000 dialogues, where two people have private lists of friends and are trying to identify which friend they have in common.
While this is a lot of data, the mechanical turk workers are clearly moving fast, with dialogues taking 1.5 minutes on average, and in 18% of cases they get the friend wrong.&lt;/p&gt;

&lt;p&gt;The algorithmic contribution is that the lists of people are represented as a graph, where nodes are properties like company and hobby.
The graph is used to generate vectors for each person by running a form of message passing over its structure.
During generation, the LSTM uses attention over these vectors to inform the output choice.&lt;/p&gt;

&lt;p&gt;A few interesting things in the output:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are cases where the output is incorrect, as in, says a fact about the structured information / knowledge base that is false.&lt;/li&gt;
&lt;li&gt;Evaluation is tricky, and over the metrics they consider sometimes this wins, but sometimes the baseline system (rules) does better. In particular, success on bot-bot evaluation doesn&amp;rsquo;t seem to clearly transfer to bot-human experiments.&lt;/li&gt;
&lt;li&gt;The utterances are very fluent, but that may be because it&amp;rsquo;s essentially copying from the training data. It looks like there is diversity in the dataset, but a lot of utterances do fit a template of &amp;ldquo;I have X who Y&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1162&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{he-EtAl:2017:Long4,
  author    = {He, He  and  Balakrishnan, Anusha  and  Eric, Mihail  and  Liang, Percy},
  title     = {Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1766--1776},
  abstract  = {We study a \emph{symmetric collaborative dialogue} setting
	in which two agents, each with private knowledge,
	must strategically communicate to achieve a common goal.
	The open-ended dialogue state in this setting poses new challenges for existing
	dialogue systems.
	We collected a dataset of 11K human-human dialogues,
	which exhibits interesting lexical, semantic, and strategic elements.
	To model
	both structured knowledge and unstructured language,
	we propose a neural model with dynamic knowledge graph embeddings
	that evolve as the dialogue progresses.
	Automatic and human evaluations show that our model is both more effective
	at achieving the goal and more human-like than baseline neural and rule-based
	models.},
  url       = {http://aclweb.org/anthology/P17-1162}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-07_spineparsinglstm/</link>
      <pubDate>Tue, 07 Nov 2017 20:42:45 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-07_spineparsinglstm/</guid>
      
        <description>

&lt;p&gt;This paper brings together work on neural dependency parsing with the idea of non-terminal spines as a way to represent constituency structure.
Within the transition parsing inference process they can naturally fit the generation of a new spines by gradually building up the spine, which makes for a very elegant inference process.&lt;/p&gt;

&lt;p&gt;Surprisingly, it doesn&amp;rsquo;t seem to matter what head choices are used to generate the spines (they tried leftmost word, rightmost word, and two standard schemes).
This contrasts with my own observations that the choice of head had a big impact (0.5 F) on accuracy.
I think the incrementally-built spines are the key difference.
Decisions about higher up in the spine are difficult to make when looking at a single word, but with the incremental construction there is information about a larger context.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/W17-6316&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{ballesteros-carreras:2017:IWPT,
  author    = {Ballesteros, Miguel  and  Carreras, Xavier},
  title     = {Arc-Standard Spinal Parsing with Stack-LSTMs},
  booktitle = {Proceedings of the 15th International Conference on Parsing Technologies},
  month     = {September},
  year      = {2017},
  address   = {Pisa, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {115--121},
  url       = {http://www.aclweb.org/anthology/W17-6316}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)</title>
      
      <link>http://www.jkk.name/post/2017-11-06_literarycharacters/</link>
      <pubDate>Mon, 06 Nov 2017 20:16:28 -0500</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-06_literarycharacters/</guid>
      
        <description>

&lt;p&gt;NLP tools seem like a natural fit for literary analysis, but the domain shift from news text is large enough to degrade performance to the point where tools are not useful.
Here the specific question is how many characters are there in novels?
NER + coreference would seem to be enough, but an off-the-shelf system fares poorly (and I doubt improvements in the last few years would change that story).&lt;/p&gt;

&lt;p&gt;The solution is to craft a kind of coreference system focused on getting all of the characters, but not necessarily every mention.
The most interesting new piece is how they identify rare characters: identify arguments of verbs that usually take people.
With this tool in hand they analyse patterns of character use over time to test hypotheses from literary analysis.&lt;/p&gt;

&lt;p&gt;Another key piece of this work was a tool to annotate a collection of books with character occurrences.
CHARLES, their tool, is built on top of &lt;a href=&#34;http://brat.nlplab.org/&#34; target=&#34;_blank&#34;&gt;brat&lt;/a&gt;, adding features to help multiple annotators coordinate labels (specifically handling the case of new character identification, which modifies the set of linkable entities).&lt;/p&gt;

&lt;p&gt;Finally, they released the character lists identified for the novels considered (&lt;a href=&#34;http://aclweb.org/anthology/attachments/D/D15/D15-1088.Attachment.zip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).
It would be interesting to modify a coreference resolution system to process these books, taking advantage of that information!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/D15-1088&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2016/pdf/1130_Paper.pdf&#34; target=&#34;_blank&#34;&gt;Annotation Tool Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{vala-EtAl:2015:EMNLP,
  author    = {Vala, Hardik  and  Jurgens, David  and  Piper, Andrew  and  Ruths, Derek},
  title     = {Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {769--774},
  url       = {http://aclweb.org/anthology/D15-1088}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-03_discourserelations/</link>
      <pubDate>Fri, 03 Nov 2017 15:40:32 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-03_discourserelations/</guid>
      
        <description>

&lt;p&gt;Automatically generating high quality meeting notes and summaries would be awesome, but involves solving many challenges.
Here, they assume speech recognition is already done and we also know the structure over utterances indicating which previous utterance each is a response to.
The task is to label each of those utterance-utterance pairs with a type (e.g. elaboration) and to select the key phrase of each utterance.&lt;/p&gt;

&lt;p&gt;Two datasets are used, the AMI and ICSO meeting corpora, which have all of the required information.
The new idea here is to jointly model the choice of link label and the key phrase, which is intuitive.
To show the value of joint modeling they run a version of the system with the same linear model, but with independent inference, which performs quite a bit worse.&lt;/p&gt;

&lt;p&gt;One neat follow up is that by combining the key phrases into a list you get a form of summary.
According to automatic metrics it is quite a bit better than running the summarisation system they compare to, though it&amp;rsquo;s still a long way from a human summary.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1090&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{qin-wang-kim:2017:Long,
  author    = {Qin, Kechen  and  Wang, Lu  and  Kim, Joseph},
  title     = {Joint Modeling of Content and Discourse Relations in Dialogues},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {974--984},
  url       = {http://aclweb.org/anthology/P17-1090}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Shazeer et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-11-01_mixtureofexperts/</link>
      <pubDate>Wed, 01 Nov 2017 21:57:27 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-11-01_mixtureofexperts/</guid>
      
        <description>

&lt;p&gt;Mixture of experts can be seen as an ensemble approach in which we assume that each of our models is effective under different circumstances and so we combine them by switching between which we use to make our decision.
From this perspective the idea can be applied to any set of models, but here the idea is to train (1) the expert models, (2) our method of choosing between them, and (3) a set of common model components, all at the same time.&lt;/p&gt;

&lt;p&gt;The particular set up here is that they modify a series of LSTM layers, adding a new layer in between each pair of LSTMs.
The new layer has a set of small feed-forward networks (the experts) and an even simpler network that chooses which expert to use.
One big benefit of this is that a lot of computation can be avoided when we know some of the small feed-forward components are going to be ignored.
As a result, they can scale up to massive networks while still having reasonable runtimes.&lt;/p&gt;

&lt;p&gt;Some key things to make this all work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Enough machines to train it! Also, there is a careful mixture of data and model parallelism during training.&lt;/li&gt;
&lt;li&gt;Some noise in the expert selection process&lt;/li&gt;
&lt;li&gt;A loss that directly encourages the use of multiple experts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing mentioned in passing is how this relates to a form of dropout (which can be viewed as training a set of overlapping experts, kind of).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://openreview.net/pdf?id=B1ckMDqlg&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{45929,
	title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
	author  = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
	year  = {2017},
  booktitle = {ICLR},
	URL = {https://openreview.net/pdf?id=B1ckMDqlg},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)</title>
      
      <link>http://www.jkk.name/post/2017-10-31_realtimecaptioning/</link>
      <pubDate>Tue, 31 Oct 2017 13:23:13 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-31_realtimecaptioning/</guid>
      
        <description>

&lt;p&gt;For any given task, automatic systems are fast, while annotation is accurate.
This work is about bridging that gap to provide a way for a team of annotators to produce real-time high quality labels.
The specific application is speech transcription, in which automatic systems are not accurate, while average people are slow (experts can transcribe in real time, but are very expensive).&lt;/p&gt;

&lt;p&gt;The solution is to carefully break up the task and combine annotations back together.
To get it to work well there are a range of subtle design decisions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;People hear the entire audio stream, but with their section at normal volume and the rest quieter. This allows them to focus their effort while still understanding the context.&lt;/li&gt;
&lt;li&gt;The alignment process combines annotations with guidance from a language model and a model of typos based on keyboard layout.&lt;/li&gt;
&lt;li&gt;Words are locked in shortly after being typed, to encourage workers to go on rather than revising their own errors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Follow up work added several more ideas to improve performance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Time warping, slowing down to half speed for their section, then going to 1.5x for the rest.&lt;/li&gt;
&lt;li&gt;Use ASR as well, either as another worker (with very uncorrelated errors), or as a starting point for human editing (or vice versa).&lt;/li&gt;
&lt;li&gt;Use A* search rather than a greedy algorithm for the alignment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Performance does not reach the level of a professional, but is far better than ASR.
From the paper it&amp;rsquo;s tricky to see a final cost, but it is certainly far lower than the professional.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://doi.acm.org/10.1145/2380116.2380122&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Lasecki:2012:RCG:2380116.2380122,
 author = {Lasecki, Walter and Miller, Christopher and Sadilek, Adam and Abumoussa, Andrew and Borrello, Donato and Kushalnagar, Raja and Bigham, Jeffrey},
 title = {Real-time Captioning by Groups of Non-experts},
 booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
 series = {UIST &#39;12},
 year = {2012},
 isbn = {978-1-4503-1580-7},
 location = {Cambridge, Massachusetts, USA},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2380116.2380122},
 doi = {10.1145/2380116.2380122},
 acmid = {2380122},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {captioning, crowdsourcing, deaf, hard of hearing, real-time, text alignment, transcription},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-30_neuralsequence/</link>
      <pubDate>Mon, 30 Oct 2017 13:28:30 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-30_neuralsequence/</guid>
      
        <description>

&lt;p&gt;Language is bursty, with rare words occurring in clumps, the simplest example being an unusual name that occurs a lot in one news article, but not in other articles.
This paper is about how to modify a neural language model to take this into consideration, by adapting the model over time.&lt;/p&gt;

&lt;p&gt;The main idea is to have one model of overall word usage (global) and a separate model that shifts over time to take into consideration the current text (local).
The idea of adapting is not new (as the paper makes clear), but the key here is an update rule that is a modified form of RMSprop, combining the local and global models.
It also seems like performing the updates after every 5 words is important, balancing frequency with informativeness (though no ablation of frequencies is presented).
Conveniently, this is orthogonal to many other ideas and can essentially be stapled on top of a range of sequential architectures, consistently leading to improvements.&lt;/p&gt;

&lt;p&gt;One question left open is how this would work in generation.
The paper describes how it could be applied and could provide improvements, but it also seems likely to risk the repetitive outputs seen in many dialogue systems.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.07432.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv170907432K,
   author = {{Krause}, B. and {Kahembwe}, E. and {Murray}, I. and {Renals}, S.},
    title = &amp;quot;{Dynamic Evaluation of Neural Sequence Models}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1709.07432},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
     year = 2017,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170907432K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Searching for Activation Functions (Ramachandran et al., 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-27_swishactivation/</link>
      <pubDate>Fri, 27 Oct 2017 11:22:45 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-27_swishactivation/</guid>
      
        <description>

&lt;h2 id=&#34;update&#34;&gt;Update&lt;/h2&gt;

&lt;p&gt;After prior work came to light that uses the same non-linearity, this paper was updated to focus more on the search process used to investigate the space.
One new takeaway for me was how diverse the effective activation functions were (see figures in the new version of the paper).&lt;/p&gt;

&lt;h2 id=&#34;original-post&#34;&gt;Original Post&lt;/h2&gt;

&lt;p&gt;Non-linear functions are the key to the representation power of neural networks.
Many different ones have been proposed, though it is difficult to make theoretical claims of their properties and so the choice of which to use is generally empirical.
This paper proposes a new non-linearity, $\text{swish}(x) = x \cdot \text{sigmoid}(x)$.&lt;/p&gt;

&lt;p&gt;Interestingly, it was chosen by a combination of exhaustive search and search with reinforcement learning across a range of functions, evaluating on CIFAR-10 with a small model.
ReLU variants were consistently second-best to swish variants, and generally the more complicated functions performed worse.
They do mention two functions that performed well, but didn&amp;rsquo;t generalise: $\text{cos}(x) - x$ and $\text{max}(x, \text{tanh}(x))$, which look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.jkk.name/img/post/activation-functions.png&#34; alt=&#34;Four activation functions&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In a range of experiments in vision and machine translation swish does at least as well or slightly better than the alternatives.
It also seems more robust to network depth and to work across different network structures.
As for why it works so well, there are two main ideas: (1) it adds smoothness to the ReLU, (2) it has some sensitivity to negative inputs.
Both of these seem particularly important at the start of training.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.05941.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv171005941R,
   author = {{Ramachandran}, P. and {Zoph}, B. and {Le}, Q.~V.},
    title = &amp;quot;{Swish: a Self-Gated Activation Function}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1710.05941},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171005941R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-26_multimodalwordembeddings/</link>
      <pubDate>Thu, 26 Oct 2017 20:47:12 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-26_multimodalwordembeddings/</guid>
      
        <description>

&lt;p&gt;Word2Vec and other approaches provide a single vector representing a word&amp;rsquo;s meaning, giving words spatially defined relationships capturing relatedness.
A natural extension is to consider regions in that space and allow some words to take up larger or smaller regions.
Another natural idea is to allow a single word to have multiple representations, to capture the different senses.
This paper considers both of those ideas, using multiple gaussian distributions per word.&lt;/p&gt;

&lt;p&gt;Using gaussians has the nice property that there is a closed form for calculating the amount of overlap between them, which is used as a measure of similarity.
Following ideas from word2vec, during learning the aim is to increase similarity between words that occur together and decrease it between random pairs that do not occur together.
Once the word representations are learned, KL divergence is used for similarity, along with the standard approaches that only look at the gaussian centres.&lt;/p&gt;

&lt;p&gt;In practise, two spherical distributions per word is sufficient.
Performance is better than word2vec and several other approaches for multi-sense word embeddings.
There was one puzzling line about the model suffering larger variance problems, but it was not quantified.&lt;/p&gt;

&lt;p&gt;It would be very interesting to inject some knowledge, such as from WordNet, to guide the number of gaussians per word, rather than giving them all N.
The paper also doesn&amp;rsquo;t get into details about the learned space, for example, are the two senses often far apart or close together? (in the latter case it is learning a slightly non-linear spatial representation).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P17-1151&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{athiwaratkun-wilson:2017:Long,
  author    = {Athiwaratkun, Ben  and  Wilson, Andrew},
  title     = {Multimodal Word Distributions},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1645--1656},
  url       = {http://aclweb.org/anthology/P17-1151}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice (Mi and Huang, 2015)</title>
      
      <link>http://www.jkk.name/post/2017-10-25_shiftreducedp/</link>
      <pubDate>Wed, 25 Oct 2017 14:44:13 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-25_shiftreducedp/</guid>
      
        <description>

&lt;p&gt;This paper is a follow-up to yesterday&amp;rsquo;s, where the approach is implemented and evaluated on English and Chinese, with very strong results.
The novel contribution is the idea of introducing alternating steps in the dynamic program to do unary steps (not a novel idea in general, but novel in its application to the dynamic programming version of shift-reduce parsing).&lt;/p&gt;

&lt;p&gt;What I found interesting here were the clear benefits of the dynamic program (DP) version.
One way of viewing this is that the DP gives a more intelligent type of beam, avoiding the issue where the beam is filled with minor variations on a theme.
Results are given for various beam sizes in both approaches, but it would be interesting to see a graph where the x-axis is number of items built.
I suspect in that situation, the gap would be smaller.
On speed, there is the nice theoretical bound of $O(n)$ for this approach, but that obscures a grammar constant related to the item structure.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/N15-1108&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{mi-huang:2015:NAACL-HLT,
  author    = {Mi, Haitao  and  Huang, Liang},
  title     = {Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {May--June},
  year      = {2015},
  address   = {Denver, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {1030--1035},
  url       = {http://www.aclweb.org/anthology/N15-1108}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)</title>
      
      <link>http://www.jkk.name/post/2017-10-24_dynamictransition/</link>
      <pubDate>Tue, 24 Oct 2017 13:06:04 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-24_dynamictransition/</guid>
      
        <description>

&lt;p&gt;This paper from 2011 explores the relationship between transition based parsing and dynamic programming based parsing.
They show how to convert common dependency parsing systems (Arc-Standard and Arc-Eager) into dynamic programs, and how doing the reverse on a dynamic program gives the Arc-Hybrid approach (which has since been used in many places, and is now joined by additional systems like Arc-Swift).&lt;/p&gt;

&lt;p&gt;The benefit of this transformation is that we can find exact answers without massive beams.
The drawback is that the feature set is restricted.
This paper is theoretical, so it doesn&amp;rsquo;t give a direct measure of this tradeoff, though &lt;a href=&#34;http://www.anthology.aclweb.org/D/D13/D13-1071.pdf&#34; target=&#34;_blank&#34;&gt;follow up work&lt;/a&gt; shows that avoiding search errors is indeed beneficial.&lt;/p&gt;

&lt;p&gt;With all of the positive results using neural networks for multi-task learning, one thought this work leads to is whether we could treat different inference methods as different tasks.
In other words, have a single model encoding the input, then have multiple inference algorithms with different extensions of that model, all trained simultaneously.
The variation in available context for the different algorithms may force generality in the core representation shared across them.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/P/P11/P11-1068.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kuhlmann-gomezrodriguez-satta:2011:ACL-HLT2011,
  author    = {Kuhlmann, Marco  and  G\&#39;{o}mez-Rodr\&#39;{i}guez, Carlos  and  Satta, Giorgio},
  title     = {Dynamic Programming Algorithms for Transition-Based Dependency Parsers},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {673--682},
  url       = {http://www.aclweb.org/anthology/P11-1068}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Mastering the game of Go without human knowledge (Silver et al., Nature 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-23_alphagozero/</link>
      <pubDate>Mon, 23 Oct 2017 21:12:57 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-23_alphagozero/</guid>
      
        <description>

&lt;p&gt;This paper is an extension of the original AlphaGo work on using reinforcement learning to build a Go-player.
Interestingly, the changes have simplified the overall model, as well as enabling it to do even better than the previous model, but now without any supervised training.&lt;/p&gt;

&lt;p&gt;One key change is that there is a single core neural network learning to represent the game state.
On top of that there are either a set of layers that produce an evaluation of the quality of a position, or there are a set of layers that place a distribution over moves.
This ties in nicely to a lot of work happening at the moment on multi-task learning in NLP and elsewhere.&lt;/p&gt;

&lt;p&gt;Getting into the details, they use monte-carlo tree search to choose actions during training, then update the model to better match the outcomes observed.
Starting from a completely random initialisation, the argument for why this works is that at every point in self-play the MCTS informed outcomes are just slightly better than the current model.
That edge is enough to provide a useful signal, without being such a drastic shift because in self-play the two sides are closely matched.
Interestingly, while the unsupervised model is worse at predicting what expert human players will do in a game, it is still better at predicting which player will win.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaGoZero,
  author = {Silver, David  and  Schrittwieser, Julian  and  Simonyan, Karen  and  Antonoglou, Ioannis  and  Huang, Aja  and  Guez, Arthur  and  Hubert, Thomas  and  Baker, Lucas  and  Lai, Matthew  and  Bolton, Adrian  and  Chen, Yutian  and  Lillicrap, Timothy  and  Hui, Fan  and  Sifre, Laurent  and  van den Driessche, George  and  Graepel, Thore  and  Hassabis, Demis},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  year = {2017},
  volume = {550},
  issue = {7676},
  pages = {354-359},
  publisher = {Macmillan Publishers Limited, part of Springer Nature},
  doi = {10.1038/nature24270},
  url = {http://www.nature.com/nature/journal/v550/n7676/abs/nature24270.html#supplementary-information},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Attention Is All You Need (Vaswani et al., ArXiv 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-20_onlyattention/</link>
      <pubDate>Fri, 20 Oct 2017 15:25:23 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-20_onlyattention/</guid>
      
        <description>

&lt;p&gt;Recurrent neural networks like LSTMs and GRUs have limited scope for parallelisation because each step depends on the one before it.
This architecture also means that many steps of computation separate two words that are far apart, making it difficult to capture long-distance relations.
A range of approaches have been used to try to address these issues, such as convolutional structures and other forms of recurrence (e.g. QRNNs).
The idea in this work is to use attention, applied multiple times, to get a network that is fast while still capturing positional information.&lt;/p&gt;

&lt;p&gt;To explain the structure I put together the figure below, which captures the network structure with a few simplifications:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.jkk.name/img/post/google-attention.png&#34; alt=&#34;Google Attention Network&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There a few ideas being brought together here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Positional encoding&lt;/em&gt;, which is a vector of the same length as the word representation, but that depends only on the position in the input. Here they use $f(pos, dim) = sin(pos / 10000^{2 dim / d_w})$ for even dimensions and the cosine equivalent for odd dimensions (where $d_w$ is the number of dimensions.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multi-head attention&lt;/em&gt;, where rather than running attention once on the full vector, multiple linear transforms are applied to get smaller vectors.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Scaled dot product attention&lt;/em&gt;, the equation is shown in the figure, the key new idea is to rescale by the square root of the dimensionality so that larger vectors don&amp;rsquo;t produce excessively sharp distributions after the softmax is applied. The more general form of this described in the paper has keys ($K$), queries ($Q$) and values ($V$), but the network uses the same vector for the key and value. I show the query with a solid line and the values/keys with dotted lines. The matrix $V$ is formed by using the $v$ vectors as rows, while $Q$ is formed by duplicating $q$ in every row. Note, on the left hand side attention is over all input words, while on the right it is only over the words produced so far (ie. content to the left).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Layer normalisation&lt;/em&gt;, a way to rescale weights to keep vector outputs in a nice range, from &lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34; target=&#34;_blank&#34;&gt;Ba, Kiros and Hinton (ArXiv 2016)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Other details, (1) When the outputs are words, the vectors used to initially represent each input word are also used to represent the outputs and in the final linear transformation (though with some rescaling). (2) They use a formula I haven&amp;rsquo;t seen before to adjust the learning rate during training, (3) dropout in several places and label smoothing are used for regularization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simplifications in the figure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For multi-head attention I only show two transforms, while in practise they used 8.&lt;/li&gt;
&lt;li&gt;The shaded regions are duplicated 6 times to create a stack, with the output of one region acting as the input to the next copy of it. The links from left to right are always from the top of the input stack.&lt;/li&gt;
&lt;li&gt;The musical repeat signs indicate that the structure is essentially the same. On the output side this isn&amp;rsquo;t quite true since the attention boxes only take inputs to their left (since output to the right doesn&amp;rsquo;t exist when they are being calculated).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In terms of experiments, it works at least as well if not better than prior approaches, and is a lot faster for machine translation (no speed numbers are given for parsing).
There is also some nice analysis of what it ends out using the attention mechanism to focus on for each word.
It seems like it can provide a way to effectively disambiguate the sense of a word based on its context.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Google also has some blog posts up
&lt;a href=&#34;https://research.googleblog.com/2017/08/transformer-novel-neural-network.html&#34; target=&#34;_blank&#34;&gt;about the paper&lt;/a&gt;
and
&lt;a href=&#34;https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html&#34; target=&#34;_blank&#34;&gt;about the library&lt;/a&gt;
they released.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{arxiv:1706.03762,
  author    = {Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {ArXiv},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</title>
      
      <link>http://www.jkk.name/post/2017-10-19_mace/</link>
      <pubDate>Thu, 19 Oct 2017 17:08:50 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-19_mace/</guid>
      
        <description>

&lt;p&gt;The standard way to get high quality annotations is to get labels from multiple people and take a majority vote.
Getting multiple annotations costs more, and the quality of annotators can vary considerably (with spamming at one extreme).
One way to avoid the quality issue is to restrict who can do the task (must have done X previous tasks with an accept rate of Y), but that limits the pool of available workers.
Another approach is to try to estimate the quality of annotator work using a statistical model.&lt;/p&gt;

&lt;p&gt;Here a generative model is used, with the following structure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$T_i$, the true label, sampled with a uniform prior over labels&lt;/li&gt;
&lt;li&gt;$S_{ij}$, a binary variable indicating if the person is spamming or not, sampled as a Bernoulli variable with a Beta prior&lt;/li&gt;
&lt;li&gt;$A_{ij}$, the annotator&amp;rsquo;s decision, if they are spamming it is sampled from a multinomial with parameters specific to them (with a Dirichlet prior), otherwise it is the true label&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$A$ is observed, but $T$ and $S$ are not, so they use expectation maximization to get both model parameters and variable values.
To deal with nonconvexity they use 100 random restarts, deciding which is best based on how well the model describes the data.
Note - this model (and the code) was the basis of the error detection paper I &lt;a href=&#34;http://www.jkk.name/post/2017-10-13_errordetection/&#34; target=&#34;_blank&#34;&gt;wrote about&lt;/a&gt; recently.&lt;/p&gt;

&lt;p&gt;For predicting annotator quality the model is consistently effective across three datasets, though the Beta and Dirichlet priors are key for one (where annotator agreement was high on average).
For determining the correct answer it is slightly better than majority vote, though the gains are small.
The real advantage comes in deciding whether to discard data, where the choice of what to discard can be guided by the estimate of quality (this is what the error detection paper was doing).
A range of synthetic experiments also show positive results, though their design shares the assumptions about behaviour that are baked into the model.&lt;/p&gt;

&lt;p&gt;I found a few results particularly interesting:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;As the number of annotators is decreased, the benefit of this approach over majority vote grows to be quite substantial (the main experiments are for data with 10 annotators).&lt;/li&gt;
&lt;li&gt;If you do use majority vote, use an odd number of annotators. Switching to an even number mainly seems to create ties. The right number is also very data dependent.&lt;/li&gt;
&lt;li&gt;Providing gold information as supervision within EM doesn&amp;rsquo;t help much unless it is quite substantial (20%+ of the data)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/N13-1132.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hovy-EtAl:2013:NAACL-HLT,
  author    = {Hovy, Dirk  and  Berg-Kirkpatrick, Taylor  and  Vaswani, Ashish  and  Hovy, Eduard},
  title     = {Learning Whom to Trust with MACE},
  booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2013},
  address   = {Atlanta, Georgia},
  publisher = {Association for Computational Linguistics},
  pages     = {1120--1130},
  url       = {http://www.aclweb.org/anthology/N13-1132}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-18_neuralamr/</link>
      <pubDate>Wed, 18 Oct 2017 21:31:05 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-18_neuralamr/</guid>
      
        <description>

&lt;p&gt;This is another paper concerned with the challenge of sparsity in AMR parsing, specifically that there are an enormous number of output symbols in the parse trees and most are seen infrequently.
The system they develop is based on the encoder-decoder with attention approach, which has previously done poorly for AMR, partially because of sparsity.&lt;/p&gt;

&lt;p&gt;Their solution is to merge certain types of symbols into groups (dates, named entities, rare verbs, constants, etc) and have a standard way to map from the surface form to the output symbol.
This is an alternative to the approach from the paper I &lt;a href=&#34;http://www.jkk.name/post/2017-10-12_amralignment/&#34; target=&#34;_blank&#34;&gt;wrote about&lt;/a&gt; last week.
They also introduce a completely separate idea, which is a different way to take an AMR graph and turn it into a linear sequence.
This change is necessary to make the output follow the form their model generates - a sequence (though there has been work on tree based LSTMs on the output side, so AMR could be directly generated, and I believe there has been some work on applying that to AMR).&lt;/p&gt;

&lt;p&gt;Together these changes do substantially improve performance over previous encoder-decoder based work for AMR.
However, there is still a substantial gap between the system and state-of-the-art, presumably because of the additional resources that other systems indirectly use by running external systems for NER, dependency parsing, etc.
Given the recent success of multi-task learning with neural nets, it would be interesting to see if those resources could be used here to further boost performance.
It may also be productive to combine these ideas with the graph abstraction ideas from AMR alignment paper.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/E/E17/E17-1035.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{peng-EtAl:2017:EACLlong1,
  author    = {Peng, Xiaochang  and  Wang, Chuan  and  Gildea, Daniel  and  Xue, Nianwen},
  title     = {Addressing the Data Sparsity Issue in Neural AMR Parsing},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  month     = {April},
  year      = {2017},
  address   = {Valencia, Spain},
  publisher = {Association for Computational Linguistics},
  pages     = {366--375},
  url       = {http://www.aclweb.org/anthology/E17-1035}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-17_nedisambiguation/</link>
      <pubDate>Tue, 17 Oct 2017 20:33:58 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-17_nedisambiguation/</guid>
      
        <description>

&lt;p&gt;Several NLP tasks aim to identify information regarding entities, such as when two sections of text are referring to the same thing, or which thing out of a large set (e.g. things in Wikipedia) a piece of text is about.
This paper focuses on a subset of entity linking, trying to determine which entity out of a set of candidates is the correct one (in a way a kind of reranker for entity linking).&lt;/p&gt;

&lt;p&gt;The task is based on a really cool dataset from Google+UMass, which collected text that was hyperlinked to wikipedia articles.
The idea is that the text (&lt;em&gt;mention&lt;/em&gt;) is probably a reference to the thing the article describes, so it is an easy way to get entity linked data for free.
Here, the data is filtered to mentions that aren&amp;rsquo;t too rare (more than 10 occurrences) and where the mention isn&amp;rsquo;t used to refer to too many different entities (the two most common entities account for over 10% of occurrences).
Then, the set of things that this mention is used to refer to somewhere are treated as a list of candidates, and the task is to choose which one is correct in a given context.&lt;/p&gt;

&lt;p&gt;The model is of the common style at the moment:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The context is processed using a recurrent neural network to produce a set of vectors&lt;/li&gt;
&lt;li&gt;Attention is used to produce vectors that combine the context with a candidate entity&lt;/li&gt;
&lt;li&gt;A feedforward neural network produces a score that is maxed over to get a final decision&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On the wikilinks based dataset this performs quite a bit better than other models, but it is behind on the smaller manually curated datasets used elsewhere (YAGO and PPRforNED, which link entities in the CoNLL 2003 shared task).
Interestingly, augmenting the training data for YAGO with data from wikilinks does improve performance.
For future users of the wikilinks data there is also some nice analysis at the end of remaining challenges, which are spit between mistakes in the data (unsurprising given the approximate collection process), answers that are too general or specific, tricky cases, and the long tail (which would be even longer without the filtering used in these experiments).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/K/K17/K17-1008.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{eshel-EtAl:2017:CoNLL,
  author    = {Eshel, Yotam  and  Cohen, Noam  and  Radinsky, Kira  and  Markovitch, Shaul  and  Yamada, Ikuya  and  Levy, Omer},
  title     = {Named Entity Disambiguation for Noisy Text},
  booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)},
  month     = {August},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {58--68},
  url       = {http://aclweb.org/anthology/K17-1008}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-16_forumrnn/</link>
      <pubDate>Mon, 16 Oct 2017 20:55:07 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-16_forumrnn/</guid>
      
        <description>

&lt;p&gt;Attention - a weighted average over a set of vectors representing context - has consistently produced positive results.
Here we see an example of how it can be applied in the case of modeling a threaded discussion.&lt;/p&gt;

&lt;p&gt;Attention is applied in two ways.
First, over a fixed set of vectors.
This is intended to provide a mechanism to choose between several different sub-models contained within a single model.
Put differently, the vectors provide a set of latent representations that capture each of the different types of posts in the subreddit.
Second, attention over the current utterance is used in the process of predicting responses (at training time only).
This provides an additional source of input to the model, by forcing it to explain the response utterances using the same representations as a source of information.&lt;/p&gt;

&lt;p&gt;The application is a new task, using values assigned to posts = upvotes - downvotes (i.e. Reddit karma).
Predicting the specific value is hard, so the task is split into 7 binary decisions about whether a post has a score higher or lower than some value.
On this task the new approach provides consistent gains, though overall performance remains low (53 - 56%).
Confusingly though, one of the figures (number 4) seems to suggest that it was a single multi-way decision, not a set of binary decisions.
I&amp;rsquo;m also curious about the data, in particular what the distribution of scores is.
The paper mentions it is Zipfian, but surely it would be something double-sided with a massive peak at 0 and a rapid drop in either direction?&lt;/p&gt;

&lt;p&gt;Overall, this is further evidence of the versatility of the idea of attention!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/D17-1242.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{cheng-fang-ostendorf:2017:EMNLP2017,
  author    = {Cheng, Hao  and  Fang, Hao  and  Ostendorf, Mari},
  title     = {A Factored Neural Network Model for Characterizing Online Discussions in Vector Space},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2286--2296},
  url       = {https://www.aclweb.org/anthology/D17-1242}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-13_errordetection/</link>
      <pubDate>Fri, 13 Oct 2017 13:32:19 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-13_errordetection/</guid>
      
        <description>

&lt;p&gt;Active learning doesn&amp;rsquo;t seem to get much attention in NLP, probably because of fear that developing data based on the errors of one model will introduce a particular sampling bias.
This paper is a nice example of a problem it can be applied to that doesn&amp;rsquo;t raise that issue: detecting all the errors in a system&amp;rsquo;s output.&lt;/p&gt;

&lt;p&gt;The scenario is that you have a bunch of models for doing a task (e.g. POS tagging) and a new dataset with no labeled data, which you would like to label.
Having a person label the data would take a long time and doesn&amp;rsquo;t take advantage of these systems.
At the same time, we can&amp;rsquo;t just run the systems and use their output because they aren&amp;rsquo;t perfect, particularly out of domain.
We could run the systems and check their output, which could be faster than annotating directly, but would still take a long time.
If we don&amp;rsquo;t mind having some errors, we can check just some output, but how do we decide what to check?&lt;/p&gt;

&lt;p&gt;This paper applies the generative model from &lt;a href=&#34;http://www.aclweb.org/anthology/N13-1132&#34; target=&#34;_blank&#34;&gt;MACE&lt;/a&gt; to build a generative model of system outputs.
The model is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For each example, sample the true label with a uniform prior&lt;/li&gt;
&lt;li&gt;Then, for each classifier, sample from a Bernoulli distribution to decide if they are good or not&lt;/li&gt;
&lt;li&gt;A good classifier returns the true label, a not good classifier samples from a multinomial over the options&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we don&amp;rsquo;t know the parameters of the model, or the true labels, use expectation maximisation to learn.&lt;/p&gt;

&lt;p&gt;This work takes that model, trains it and uses it to identify the sample that is most uncertain.
A person annotates it, the correct label replaces one of the system predictions, and EM is run again.
This is repeated until either there appear to be no more errors, or annotators run out of time.&lt;/p&gt;

&lt;p&gt;How well does it work?
The main metric is precision: how many of the instances asked for annotation actually have errors.
For POS tagging on WSJ text, the taggers initially get 2.5% of words wrong.
To get that down to 1.1% the precision is 33%, and to get it to 0.65% precision is 17.6%.
On an out of domain dataset, the error rate is 10% initially, and is down to 5% with a precision of 50%.
Put differently, in a dataset of 25,000 tokens, with 2,500 errors, after checking 2,500 tokens, there are only 1,250 errors (another 2,500 checks brings it down to 730).
It also works well for NER, and consistently does better than the alternative they compare to (consider the taggers a committee and find the examples with highest entropy, i.e. greatest disagreement).&lt;/p&gt;

&lt;p&gt;This seems like a natural fit for &lt;a href=&#34;https://prodi.gy/&#34; target=&#34;_blank&#34;&gt;prodigy&lt;/a&gt; and something that could be broadly useful.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/P/P17/P17-1107.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{rehbein-ruppenhofer:2017:Long,
  author    = {Rehbein, Ines  and  Ruppenhofer, Josef},
  title     = {Detecting annotation noise in automatically labelled data},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1160--1170},
  url       = {http://aclweb.org/anthology/P17-1107}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-12_amralignment/</link>
      <pubDate>Thu, 12 Oct 2017 19:52:34 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-12_amralignment/</guid>
      
        <description>

&lt;p&gt;Abstract Meaning Representation (AMR) structures represent sentence meaning with labeled nodes (concepts) that are related to the words in the sentence, but not explicitly linked to them.
This is a problem for most parsing algorithms, which need a way to efficiently decompose the structure in order to learn how to generate it.
In dependency parsing there are no abstract nodes to generate, in constituency parsing there is a very small set of node types, and for CCG, TAG, etc the labels come from a constrained space.
The solution for many AMR parsers is to have a process for generating the concepts as a first step towards parsing, and to automatically align the training data to guide this concept generation stage.&lt;/p&gt;

&lt;p&gt;The first idea in this paper is about the set of AMR concepts.
Some concepts are easy to link, as the concept clearly maps to a single word in the sentence.
Around a quarter of concepts have a more complex relation, where a set of concepts link to a set of words, for example, named entities.
The idea for these is to identify common subgraphs by abstracting some lexical items.
For example, a teacher and a worker both get mapped to a person concept that is the ARG0 of the lexical item (teach, or work in this case).
This can allow for the generation of entirely novel concepts (e.g. &amp;ldquo;concept&amp;rdquo;-er), giving a 0.6 boost to recall for CAMR simply by making these additional concepts available.
Using a bidirectional LSTM with a character CNN to generate features on likely concepts, there is a gain of 1.0 F1 for the parser.&lt;/p&gt;

&lt;p&gt;The second idea is to improve the alignments used to train concept generation by taking into consideration the graph structure.
To use an aligner developed for machine translation the graph needs to be turned into a linear sequence, but that can lead to strange jumps.
The idea here is to take that into consideration by modifying the calculation of the cost of distortion (i.e. jumping) to be reshaped based on the graph structure.
For optimal alignment quality they consider aligning in either direction, directly changing the distance metric in the English-AMR direction, and just rescaling it to be less sensitive when appropriate for AMR-English.
This is definitely higher precision than prior approaches, but lower recall.
It&amp;rsquo;s hard to tell whether this helps, since the evaluation doesn&amp;rsquo;t separate it out from the first idea (results in section 5.3 are not on the same dataset as 5.1).&lt;/p&gt;

&lt;p&gt;Given how separate this is from CAMR, it would be interesting to see if it helps other systems similarly.
With concept identification at 83 F there is still plenty of scope for improvement, though there is no analysis of which types of concepts remain the most problematic.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/D/D17/D17-1130.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{wang-xue:2017:EMNLP2017,
  author    = {Wang, Chuan  and  Xue, Nianwen},
  title     = {Getting the Most out of AMR Parsing},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {1268--1279},
  url       = {https://www.aclweb.org/anthology/D17-1130},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-11_multimt/</link>
      <pubDate>Wed, 11 Oct 2017 17:29:04 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-11_multimt/</guid>
      
        <description>

&lt;p&gt;This paper is a detailed analysis of a surprisingly effective simple idea: train a machine translation system with sentence pairs from multiple languages, adjusting the input to have an extra token at the end that says what the target language is.
To deal with class imbalance, data is oversampled to have all language pairs be equally represented (though even without that, it works fairly well).&lt;/p&gt;

&lt;p&gt;The biggest advantage of this approach is that a single model can handle translation between many pairs, rather than needing $O(n^2)$ models for $n$ languages.
The performance is slightly lower on average, but the single model can manage with far fewer parameters.
In one example, twelve models are combined into a single model with as many parameters as one of the twelve, and the results are lower by just 0.76 BLEU on average.
Another advantage of the model is the ability to handle code-switched language, though they didn&amp;rsquo;t have evaluation datasets to get an quantitative measure of accuracy.&lt;/p&gt;

&lt;p&gt;Having this model also opens up the possibility of translating between pairs of languages with no parallel training data (A -&amp;gt; B).
As long as there is data (A -&amp;gt; C) and (D -&amp;gt; B), sentences from A can be fed in with B as the target language.
For closely related languages this works very well, and in particular, better than going via another language such that there is data for the two language pairs.
For example, going from Portuguese to Spanish with the multilingual model scores 24.75, whereas going via English scores 21.62 and a model with explicit training data gets 31.50.
Going between less related languages is less successful, with direct Spanish to Japanese scoring 9.14, and going via English scoring 18.00.
One thing I wish the paper had is more exploration of this result - what does it get right when scoring 9.14?
For the time being at least, going via a third language still seems necessary, and presumably the best language to use is whichever one the performance is highest on.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/Q/Q17/Q17-1024.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.04558.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv version&lt;/a&gt; which appears to be the same aside from one extra figure of the model architecture.&lt;/p&gt;

&lt;p&gt;As an aside, it is interesting to see the timeline for this paper:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;November 2016, Submission to ArXiv and in the TACL submission batch&lt;/li&gt;
&lt;li&gt;March 2017, TACL revision batch&lt;/li&gt;
&lt;li&gt;October 2017, TACL published&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1081,
	author    = {Johnson, Melvin  and Schuster, Mike  and Le, Quoc  and Krikun, Maxim  and Wu, Yonghui  and Chen, Zhifeng  and Thorat, Nikhil  and Vigas, Fernanda  and Wattenberg, Martin  and Corrado, Greg  and Hughes, Macduff  and Dean, Jeffrey},
	title     = {Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	journal   = {Transactions of the Association for Computational Linguistics},
	volume    = {5},
	year      = {2017},
	issn      = {2307-387X},
	url       = {https://www.transacl.org/ojs/index.php/tacl/article/view/1081},
	pages     = {339--351}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Search-based Neural Structured Learning for Sequential Question Answering (Iyyer et al., ACL 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-10_seqqa/</link>
      <pubDate>Tue, 10 Oct 2017 13:43:36 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-10_seqqa/</guid>
      
        <description>

&lt;p&gt;Semantic parsing datasets generally consist of (question, answer) pairs, where each pair is completely independent of the rest (one exception is ATIS, which has multi-turn conversations, though most work doesn&amp;rsquo;t use them).
In reality, we often ask a series of simple questions that together form a complex one, for example &amp;ldquo;What flights are available from Detroit to Sydney? And how much is the price if I don&amp;rsquo;t want to leave before 8am?&amp;rdquo;
This work explores these kinds of sequential questions with a new dataset and algorithm.&lt;/p&gt;

&lt;p&gt;The dataset was formed by asking crowd workers to rephrase questions from the WikiTableQuestions dataset into sequences of shorter questions.
This naturally constrains the types of questions (in particular, they reference a single table only), but covers a range of domains.
With 6,066 question sequences, and on average 2.9 questions / sequence, it&amp;rsquo;s a large dataset by semantic parsing standards.
However, there are no logical forms, only the row, column, or cell(s) that contain the answer.&lt;/p&gt;

&lt;p&gt;To solve the problem, they treat it as choosing a sequence of actions, where each action generate a part of the execution instructions.
The model follows the recent approach of considering the contents of the database as part of the calculation (e.g. by taking the dot product of the vector for a cell and the vector for the question).&lt;/p&gt;

&lt;p&gt;The system has consistently better performance than other QA systems on the new dataset (though no results are shown for the WikiTableQuestions dataset).
At only 12.8% of sequences completely correct, there is plenty of scope for improvement.
Based on the description of the operators there are definitely additional abilities that would be useful, so this model has potential to improve.
That said, it seems difficult to generalise the model to handle more complicated databases with multiple interconnected tables.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://aclweb.org/anthology/P/P17/P17-1167.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{iyyer-yih-chang:2017:Long,
  author    = {Iyyer, Mohit  and  Yih, Wen-tau  and  Chang, Ming-Wei},
  title     = {Search-based Neural Structured Learning for Sequential Question Answering},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1821--1831},
  url       = {http://aclweb.org/anthology/P17-1167}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title> Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)</title>
      
      <link>http://www.jkk.name/post/2017-10-09_parsing-autoencoder/</link>
      <pubDate>Mon, 09 Oct 2017 14:31:24 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-09_parsing-autoencoder/</guid>
      
        <description>

&lt;p&gt;Semantic parsing datasets are small because they are expensive to produce (logical forms don&amp;rsquo;t occur naturally and writing them down takes time).
The idea here is to do semi-supervised learning by implementing both a parser and a generator, which are trained together as a form of autoencoder where the intermediate representation is natural language.&lt;/p&gt;

&lt;p&gt;The architecture has four LSTMs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Bidirectional LSTM over a logical form.&lt;/li&gt;
&lt;li&gt;One directional LSTM attending to the first LSTM&amp;rsquo;s hidden states, generating a sentence.&lt;/li&gt;
&lt;li&gt;Bidirectional LSTM over the sentence generated by the second LSTM.&lt;/li&gt;
&lt;li&gt;One directional LSTM attending to the third LSTM&amp;rsquo;s hidden states, generating a logical form.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Usually a component like the second LSTM would choose the max word at each position (or use beam search), but here they want this whole thing to be differentiable, so the distribution over words is used.
At evaluation time only the second half (3+4) is used, with the test sentence as input.&lt;/p&gt;

&lt;p&gt;With this structure, a loss function is defined that compares the input to (1) and the output of (4), which in both cases is a logical form.
As a result, they don&amp;rsquo;t need (logical form, sentence) pairs to train, they can use automatically generated logical forms.
Of course, with only logical forms it would do something random with the intermediate representation, so some supervised examples are also needed (in which case the two halves are trained independently).&lt;/p&gt;

&lt;p&gt;The results are not state-of-the-art, but good on all three tasks (Geoquery, NLmaps, SAIL), and on two they show am improvement over training (3+4) with only supervised data.
Varying the amount of training data gives a less clear picture.
On Geoquery with 5-25% of the data, this approach clearly helps, particularly if the queries are real rather than generated (which is a realistic scenario), but then there is no improvement for 50% or 75%, and at 100% the improvement is small.
On NLmaps there was no generator, and the differences at different data %s seem like noise.
SAIL has the most clear benefit, though it&amp;rsquo;s a particularly small dataset, consisting of paths in just four maps.&lt;/p&gt;

&lt;p&gt;This is a cool idea that seems effective in certain situations.
The generator is key, and it&amp;rsquo;s possible that performance on GeoQuery would be higher with a more sophisticated one (e.g. a tree structured generator, rather than the ngram model used here).
One idea mentioned in the conclusion is to try reversing the setup (3-4-1-2) and training with natural language examples that have no logical form.
How to tradeoff the different data scenarios seems like an interesting challenge!&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://aclweb.org/anthology/D16-1116&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kovcisky-EtAl:2016:EMNLP2016,
  author    = {Ko\v{c}isk\&#39;{y}, Tom\&#39;{a}\v{s}  and  Melis, G\&#39;{a}bor  and  Grefenstette, Edward  and  Dyer, Chris  and  Ling, Wang  and  Blunsom, Phil  and  Hermann, Karl Moritz},
  title     = {Semantic Parsing with Semi-Supervised Sequential Autoencoders},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  pages     = {1078--1087},
  url       = {https://aclweb.org/anthology/D16-1116}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-06-madlibs/</link>
      <pubDate>Fri, 06 Oct 2017 13:31:43 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-06-madlibs/</guid>
      
        <description>

&lt;p&gt;Humor is an incredibly difficult problem, as this paper makes clear in its background section.
Most work has considered very specific types of jokes (e.g. &amp;ldquo;that&amp;rsquo;s what she said&amp;rdquo;, or pairs of words that sound similar to form riddles).
This work contributes (1) a new task, (2) an evaluation method, and (3) an example system.&lt;/p&gt;

&lt;p&gt;The task is Mad Libs, where a story has some words removed and people choose new words to make the story funny.
If you are familiar with the normal version, one key difference is that here people have access to the complete story when they are choosing their words.
A set of 40 &amp;lsquo;stories&amp;rsquo; were written based on Simple Wikipedia articles, and workers on Mechanical Turk wrote words to fill them, with filtering based on judging by other workers.&lt;/p&gt;

&lt;p&gt;The evaluation method involved recruiting a set of judges on Mechanical Turk and asking a series of questions to measure humour for a given response.
As well as judging the overall story, they were asked to select which words contributed the most.
By aggregating these selections as votes, each word was scored as funny or not.&lt;/p&gt;

&lt;p&gt;The system is a linear classifier with a range of features, including scores from a language model.
On its own, it performs very poorly, but using it as a filter to restrict the space of words a person can choose from actually leads to better performance than people on their own.
Of course, it&amp;rsquo;s difficult to analyse the source of improvement;
The authors theorise that it is because it prevents people from selecting words that only they would see is funny.
Another interpretation is that the constraint gives them a smaller space to think about and so they can find more interesting plays on words.&lt;/p&gt;

&lt;p&gt;Finally, as a non-expert in this area, this paper had some nice discussion of the tradeoffs between different ways of generating humour (incongruous vs. coherent content strategies).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1068&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hossain-EtAl:2017:EMNLP2017,
  author    = {Hossain, Nabil  and  Krumm, John  and  Vanderwende, Lucy  and  Horvitz, Eric  and  Kautz, Henry},
  title     = {Filling the Blanks (hint: plural noun) for Mad Libs Humor},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {649--658},
  url       = {https://www.aclweb.org/anthology/D17-1068},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)</title>
      
      <link>http://www.jkk.name/post/2017-10-05-deftnn/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-05-deftnn/</guid>
      
        <description>

&lt;p&gt;This paper proposes two techniques for speeding up neural network execution on GPUs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Reduce computation when doing matrix-multiply by removing rows.&lt;/li&gt;
&lt;li&gt;Reduce communication on the GPU by halving the number of bits used to represent numbers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Either of these gives a speed up of ~1.5x and together they give ~2x, across a range of different computer vision tasks+models.&lt;/p&gt;

&lt;h2 id=&#34;core-ideas-in-detail&#34;&gt;Core ideas in detail&lt;/h2&gt;

&lt;p&gt;The first idea, reducing work by eliminating parts of the computation, has been considered before.
In the past, however, the focus was on saving memory in models, and so the most common strategy was to move to a sparse matrix where weights close to zero are dropped.
Here the focus is on speed and they show that while the sparse approach saves memory it can end up being slower because of hardware behaviour.
Instead, they eliminate entire rows of the matrix, which means there is less computation, but it remains dense (and therefore fast).
Rows are identified by measuring correlation between outputs and greedily eliminating rows that correlate highly with the rest of the output.&lt;/p&gt;

&lt;p&gt;The natural question to ask is whether this hurts performance.
First, they do two things to avoid problems, (1) a scale factor is used to make sure the outputs are of the same range that they would have been with the full matrix, and (2) they restart training to fine-tune the network once pruning is set up.
With high enough pruning accuracy does fall, but speed ups can be gained before that is a problem (the exact point depends on the task).&lt;/p&gt;

&lt;p&gt;The second idea relates to numerical representation, and is motivated by measurements of where the bottlenecks are in communication.
Many AI researchers have tried switching to 16 bit representations to save space and time, but here they develop a different floating point encoding that gives more bits to the exponent, and fewer to the mantissa.&lt;/p&gt;

&lt;h2 id=&#34;thoughts&#34;&gt;Thoughts&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;It would be interesting to see the interaction of this work with the investigation of networks without non-linear functions that can still learn non-linear behaviour because of numerical approximations.&lt;/li&gt;
&lt;li&gt;In the context of language, the weight reduction approach would be interesting to analyse. Specifically, what do we lose in our word vectors depending on the task?&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve always had some interest in making things faster. It would be interesting to know where the remaining bottlenecks are (after applying these changes).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Hill:MICRO:2017,
  author = {Parker Hill, Animesh Jain, Mason Hill1, Babak Zamirai, Chang-Hong Hsu, Michael A. Laurenzano, Scott Mahlke, Lingjia Tang and Jason Mars},
  title = {DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission},
  booktitle = {The 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  year = {2017},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      

    </item>
    
    <item>
      
        <title>Ordering Pizza for an Event with Vegetarians</title>
      
      <link>http://www.jkk.name/post/pizza/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/pizza/</guid>
      
        <description>

&lt;h2 id=&#34;how-much-vegetarian-pizza-should-i-order&#34;&gt;How much vegetarian pizza should I order?&lt;/h2&gt;

&lt;p&gt;This question frequently comes up in the world of free food at university events. In my experience (as someone who does not eat meat pizzas), often not enough is ordered. Let&amp;rsquo;s try to come up with a model to tell us how much to order. Set it up like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are $N$ people.&lt;/li&gt;
&lt;li&gt;There are $P$ pizzas.&lt;/li&gt;
&lt;li&gt;The fraction of people who are vegetarian is $V$.&lt;/li&gt;
&lt;li&gt;Assume everyone eats the same amount of pizza, and all the pizza is eaten (ie. each person eats $\frac{P}{N}$).&lt;/li&gt;
&lt;li&gt;Assume people randomly sample from the available pizzas, subject to the constraint that some eat only vegetarian pizzas.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, let the fraction of vegetarian pizzas we get be $k$, and we can write down the number of vegetarian pizzas in two ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How many we order: $P * k$&lt;/li&gt;
&lt;li&gt;How many are eaten: (pizzas eaten by vegetarians) + (vegetarian pizzas eaten by others) = $\frac{P}{N} * (N * V) + \frac{P}{N} * (N * (1 - V)) * k$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since all the pizza we order is eaten, these are equal.
$N$ and $P$ are both positive numbers, so we can safely cancel the $N$s and divide through by $P$, giving:&lt;/p&gt;

&lt;p&gt;$V + (1 - V) * k = k$&lt;/p&gt;

&lt;p&gt;To satisfy this equation, $k = 1$. Therefore all the pizza should be vegetarian :)&lt;/p&gt;

&lt;p&gt;Of course, these assumptions aren&amp;rsquo;t quite right (for example, not everyone samples randomly from the available pizza), so here are some more useful suggestions too:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Do order more than the proportion of vegetarians.&lt;/li&gt;
&lt;li&gt;Place the vegetarian pizza at the end of the line of pizzas, or in a separate location with clear signage discouraging non-vegetarians from eating it.&lt;/li&gt;
&lt;li&gt;Order a diverse set of popular meat pizzas (people tend to want variety, so this encourages them to try more meat pizzas).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For other peoples&amp;rsquo; thoughts on this question see
&lt;a href=&#34;http://www.seriouseats.com/2014/07/etiquette-ordering-pizza-for-a-group-manner-matters.html&#34; target=&#34;_blank&#34;&gt;Serious Eats&lt;/a&gt; and
&lt;a href=&#34;https://www.quora.com/What-are-the-best-pizza-toppings-to-get-for-a-big-group&#34; target=&#34;_blank&#34;&gt;Quora&lt;/a&gt;&lt;/p&gt;
</description>
      

    </item>
    
    <item>
      
        <title></title>
      
      <link>http://www.jkk.name/home/about/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/about/</guid>
      
        <description>&lt;p&gt;My email address is: jkummerf@umich.edu&lt;/p&gt;

&lt;p&gt;I am a Postdoctoral Research Fellow, working at the &lt;a href=&#34;https://www.umich.edu/&#34; target=&#34;_blank&#34;&gt;University of Michigan&lt;/a&gt;, in &lt;a href=&#34;https://www.cse.umich.edu/&#34; target=&#34;_blank&#34;&gt;Computer Science and Engineering&lt;/a&gt;.
My research focus is Natural Language Processing - in particular, I am working on &lt;a href=&#34;https://sapphire.eecs.umich.edu&#34; target=&#34;_blank&#34;&gt;Project Sapphire&lt;/a&gt;, a collaboration with IBM, developing a conversational academic adviser.
I am also affiliated with two research groups: &lt;a href=&#34;http://lit.eecs.umich.edu/&#34; target=&#34;_blank&#34;&gt;Language and Information Technologies&lt;/a&gt;, and &lt;a href=&#34;http://web.eecs.umich.edu/~wlasecki/croma.html&#34; target=&#34;_blank&#34;&gt;Crowds+Machines&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I completed my PhD in the &lt;a href=&#34;http://web.eecs.umich.edu/~wlasecki/croma.html&#34; target=&#34;_blank&#34;&gt;UC Berkeley NLP Group&lt;/a&gt;, advised by &lt;a href=&#34;http://www.cs.berkeley.edu/~klein&#34; target=&#34;_blank&#34;&gt;Dan Klein&lt;/a&gt;, with a thesis on new algorithms related to syntactic parsing: error analysis, formalism conversion, and graph parsing.
I completed my BSc (Adv) with honours and medal in the &lt;a href=&#34;http://www.schwa.org&#34; target=&#34;_blank&#34;&gt;University of Sydney Schwa Lab&lt;/a&gt; advised by &lt;a href=&#34;http://sydney.edu.au/engineering/it/~james&#34; target=&#34;_blank&#34;&gt;James Curran&lt;/a&gt;, with a thesis on an algorithm for faster CCG parsing.
I received my Higher School Certificate at &lt;a href=&#34;http://www.emanuelschool.nsw.edu.au/&#34; target=&#34;_blank&#34;&gt;Emanuel School&lt;/a&gt;, receiving the Premier&amp;rsquo;s Award for my results in English, Mathematics, Physics, and Cosmology.&lt;/p&gt;

&lt;p&gt;My &lt;a href=&#34;http://www.jkk.name/doc/jonathan-kummerfeld-cv.pdf&#34;&gt;CV&lt;/a&gt; is available as a pdf.&lt;/p&gt;
</description>
      

    </item>
    
    <item>
      
      <title>
        "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
        
          Greg Durrett, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Taylor Berg-Kirkpatrick, Rebecca S. Portnoff, Sadia Afroz, Damon McCoy, Kirill Levchenko, Vern Paxson
        
        (EMNLP,
        2017)
      </title>
      
      <link>http://www.jkk.name/publication/emnlp17forums/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/emnlp17forums/</guid>
      
        <description>
          One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects.  We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums.  Each of these forums constitutes its own &#39;fine-grained domain&#39; in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.
        </description>
      

    </item>
    
    <item>
      
        <title>IE/NER from Cybercriminal Forums</title>
      
      <link>http://www.jkk.name/data/cybercrime-forums/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/cybercrime-forums/</guid>
      
        <description>Forum posts with annotations of products. <a href="https://evidencebasedsecurity.org/forums/#data" target="_blank">https://evidencebasedsecurity.org/forums/#data</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection",
        
          Youxuan Jiang, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Walter S. Lasecki
        
        (ACL (short),
        2017)
      </title>
      
      <link>http://www.jkk.name/publication/acl17paraphrase/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl17paraphrase/</guid>
      
        <description>
          Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.
        </description>
      

    </item>
    
    <item>
      
        <title>Crowdsourced Paraphrases</title>
      
      <link>http://www.jkk.name/data/paraphrasing-sample/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/paraphrasing-sample/</guid>
      
        <description>Paraphrases collected while conducting experiments on factors influencing crowd performance. <a href="http://aclweb.org/anthology/attachments/P/P17/P17-2017.Datasets.zip" target="_blank">http://aclweb.org/anthology/attachments/P/P17/P17-2017.Datasets.zip</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Tools for Automated Analysis of Cybercriminal Markets",
        
          Rebecca S. Portnoff, Sadia Afroz, Greg Durrett, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Taylor Berg-Kirkpatrick, Damon McCoy, Kirill Levchenko, Vern Paxson
        
        (WWW,
        2017)
      </title>
      
      <link>http://www.jkk.name/publication/www17forums/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/www17forums/</guid>
      
        <description>
          Underground forums are widely used by criminals to buy and sell a host of stolen items, datasets, resources, and criminal services.  These forums contain important resources for understanding cybercrime.  However, the number of forums, their size, and the domain expertise required to understand the markets makes manual exploration of these forums unscalable. In this work, we propose an automated, top-down approach for analyzing underground forums.  Our approach uses natural language processing and machine learning to automatically generate high-level information about underground forums, first identifying posts related to transactions, and then extracting products and prices. We also demonstrate, via a pair of case studies, how an analyst can use these automated approaches to investigate other categories of products and transactions. We use eight distinct forums to assess our tools: Antichat, Blackhat World, Carders, Darkode, Hack Forums, Hell, L33tCrew and Nulled. Our automated approach is fast and accurate, achieving over 80% accuracy in detecting post category, product, and prices.
        </description>
      

    </item>
    
    <item>
      
        <title>Example Talk</title>
      
      <link>http://www.jkk.name/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/talk/example-talk/</guid>
      
        <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://gcushen.github.io/hugo-academic-demo/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
      

    </item>
    
    <item>
      
        <title>One-Endpoint Crossing Graph Parser</title>
      
      <link>http://www.jkk.name/software/1ec-parsing/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/1ec-parsing/</guid>
      
        <description>A range of tools related to one-endpoint crossing graphs - parsing, format conversion, and evaluation. <a href="https://jkk.name/1ec-graph-parser" target="_blank">https://jkk.name/1ec-graph-parser</a></description>
      

    </item>
    
    <item>
      
        <title>Spine and Arc version of the Penn Treebank</title>
      
      <link>http://www.jkk.name/data/shp-ptb/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/shp-ptb/</guid>
      
        <description>Code to convert the standard Penn Treebank into a version where each word is assigned a spine of non-terminals, and arcs to indicate attachments from one spine to another. <a href="https://jkk.name/1ec-graph-parser/format-conversion" target="_blank">https://jkk.name/1ec-graph-parser/format-conversion</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Parsing with Traces: An O($n^4$) Algorithm and a Structural Representation",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Dan Klein
        
        (TACL,
        2017)
      </title>
      
      <link>http://www.jkk.name/publication/tacl17parsing/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/tacl17parsing/</guid>
      
        <description>
          General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.  We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Algorithms for Identifying Syntactic Errors and Parsing with Graph Structured Output",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;
        
        (EECS Department, University of California, Berkeley,
        2016)
      </title>
      
      <link>http://www.jkk.name/publication/thesis16parsing/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/thesis16parsing/</guid>
      
        <description>
          Representation of syntactic structure is a core area of research in Computational Linguistics, disambiguating distinctions in meaning that are crucial for correct interpretation of language. Development of algorithms and statistical models over the past three decades has led to systems that are accurate enough to be deployed in industry, playing a key role in products such as Google Search and Apple Siri. However, syntactic parsers today are usually constrained to tree representations of language, and performance is interpreted through a single metric that conveys no linguistic information regarding remaining errors.

In this dissertation, we present new algorithms for error analysis and parsing. The heart of our approach to error analysis is the use of structural transformations to identify more meaningful classes of errors, and to enable comparisons across formalisms. For parsing, we combine a novel dynamic program with careful choices in syntactic representation to create an efficient parser that produces graph structured output. Together, these developments allowed us to evaluate the outstanding challenges in parsing and to address a key weakness in current work.

First, we present a search algorithm that, given two structures, finds a sequence of modifications leading from one structure to the other. We applied this algorithm to syntactic error analysis, where one structure is the output of a parser, the other is the correct parse, and each modification corresponds to fixing one error. We constructed a tool based on the algorithm and analyzed variations in behavior between parsers, types of text, and languages. Our observations shine light on several assumptions about syntactic errors, showing some to be true and others to be false. For example, prepositional phrase attachment errors are indeed a major issue, while coordination scope errors do not hurt performance as much as expected.

Next, we describe an algorithm that builds a parse in one syntactic representation to match a parse in another representation. Specifically, we build phrase structure parses from Combinatory Categorial Grammar derivations. Our approach follows the philosophy of CCG, defining specific phrase structures for each lexical category and generic rules for combinatory steps. The new parse is built by following the CCG derivation bottom-up, gradually building the corresponding phrase structure parse. This produced significantly more accurate parses than past work, and enabled us to compare performance of several parsers across formalisms.

Finally, we address a weakness we observed in phrase structure parsers: the exclusion of syntactic trace structures for computational convenience. We present an efficient dynamic programming algorithm that constructs the graph structure that has the highest score under an edge-factored scoring function. We define a parse representation compatible with the algorithm, and show how certain linguistic distinctions dramatically impact coverage. We also show various ways to modify the algorithm to improve performance by exploiting properties of observed linguistic structure. This approach to syntactic parsing is the first to cover virtually all structure encoded in the Penn Treebank.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "An Empirical Analysis of Optimization for Max-Margin NLP",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Taylor Berg-Kirkpatrick, Dan Klein
        
        (EMNLP,
        2015)
      </title>
      
      <link>http://www.jkk.name/publication/emnlp15learn/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/emnlp15learn/</guid>
      
        <description>
          Despite the convexity of structured max-margin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal  optimization methods are often more robust and progress faster than dual methods. This advantage  is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.
        </description>
      

    </item>
    
    <item>
      
        <title>Coreference Error Analysis</title>
      
      <link>http://www.jkk.name/software/coreference-analysis/</link>
      <pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/coreference-analysis/</guid>
      
        <description>A tool for classifying errors in coreference resolution. <a href="https://jkk.name/berkeley-coreference-analyser" target="_blank">https://jkk.name/berkeley-coreference-analyser</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Error-Driven Analysis of Challenges in Coreference Resolution",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Dan Klein
        
        (EMNLP,
        2013)
      </title>
      
      <link>http://www.jkk.name/publication/emnlp13analysis/</link>
      <pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/emnlp13analysis/</guid>
      
        <description>
          Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "An Empirical Examination of Challenges in Chinese Parsing",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Daniel Tse, James R. Curran, Dan Klein
        
        (ACL (short),
        2013)
      </title>
      
      <link>http://www.jkk.name/publication/acl13analysis/</link>
      <pubDate>Thu, 01 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl13analysis/</guid>
      
        <description>
          Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "High-velocity Clouds in the Galactic All Sky Survey. I. Catalog",
        
          Vanessa A. Moss, Naomi M. McClure-Griffiths, Tara Murphy, D. J. Pisano, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, James R. Curran
        
        (The Astrophysical Journal Supplement Series,
        2013)
      </title>
      
      <link>http://www.jkk.name/publication/astro13clouds/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/astro13clouds/</guid>
      
        <description>
          We present a catalogue of high-velocity clouds (HVCs) from the Galactic All Sky Survey (GASS) of southern-sky neutral hydrogen, which has 57 mK sensitivity and 1 km/s velocity resolution and was obtained with the Parkes Telescope. Our catalogue has been derived from the stray-radiation corrected second release of GASS. We describe the data and our method of identifying HVCs and analyse the overall properties of the GASS population. We catalogue a total of 1693 HVCs at declinations &lt; 0 deg, including 1111 positive velocity HVCs and 582 negative velocity HVCs. Our catalogue also includes 295 anomalous velocity clouds (AVCs). The cloud line-widths of our HVC population have a median FWHM of ~19 km/s, which is lower than found in previous surveys. The completeness of our catalogue is above 95% based on comparison with the HIPASS catalogue of HVCs, upon which we improve with an order of magnitude in spectral resolution. We find 758 new HVCs and AVCs with no HIPASS counterpart. The GASS catalogue will shed an unprecedented light on the distribution and kinematic structure of southern-sky HVCs, as well as delve further into the cloud populations that make up the anomalous velocity gas of the Milky Way.
        </description>
      

    </item>
    
    <item>
      
        <title>Parse Error Analysis</title>
      
      <link>http://www.jkk.name/software/parsing-analysis/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/parsing-analysis/</guid>
      
        <description>A tool for classifying mistakes in the output of parsers. <a href="https://jkk.name/berkeley-parser-analyser" target="_blank">https://jkk.name/berkeley-parser-analyser</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, David Hall, James R. Curran, Dan Klein
        
        (EMNLP,
        2012)
      </title>
      
      <link>http://www.jkk.name/publication/emnlp12analysis/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/emnlp12analysis/</guid>
      
        <description>
          Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors.  We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Robust Conversion of CCG Derivations to Phrase Structure Trees",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Dan Klein, James R. Curran
        
        (ACL (short),
        2012)
      </title>
      
      <link>http://www.jkk.name/publication/acl12conversion/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl12conversion/</guid>
      
        <description>
          We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.
        </description>
      

    </item>
    
    <item>
      
        <title>CCG to PST</title>
      
      <link>http://www.jkk.name/software/ccg2pst/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/software/ccg2pst/</guid>
      
        <description>A tool for converting CCG derivations into PTB-style phrase structure trees. <a href="https://jkk.name/berkeley-ccg2pst" target="_blank">https://jkk.name/berkeley-ccg2pst</a></description>
      

    </item>
    
    <item>
      
      <title>
        "Mention Detection: Heuristics for the OntoNotes annotations",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Mohit Bansal, David Burkett, Dan Klein
        
        (CoNLL Shared Task,
        2011)
      </title>
      
      <link>http://www.jkk.name/publication/conll11coreference/</link>
      <pubDate>Wed, 01 Jun 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/conll11coreference/</guid>
      
        <description>
          Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Spatiotemporal Hierarchy of Relaxation Events, Dynamical Heterogeneities, and Structural Reorganization in a Supercooled Liquid",
        
          Raphael Candelier, Asaph Widmer-Cooper, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Olivier Dauchot, Giulio Biroli, Peter Harrowell, David R. Reichman
        
        (Physical Review Letters,
        2010)
      </title>
      
      <link>http://www.jkk.name/publication/prl10chemistry/</link>
      <pubDate>Wed, 01 Sep 2010 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/prl10chemistry/</guid>
      
        <description>
          We identify the pattern of microscopic dynamical relaxation for a two-dimensional glass-forming liquid. On short time scales, bursts of irreversible particle motion, called cage jumps, aggregate into clusters. On larger time scales, clusters aggregate both spatially and temporally into avalanches. This propagation of mobility takes place along the soft regions of the systems, which have been identified by computing isoconfigurational Debye-Waller maps. Our results characterize the way in which dynamical heterogeneity evolves in moderately supercooled liquids and reveal that it is astonishingly similar to the one found for dense glassy granular media.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Morphological Analysis Can Improve a CCG Parser for English",
        
          Matthew Honnibal, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, James R. Curran
        
        (CoLing,
        2010)
      </title>
      
      <link>http://www.jkk.name/publication/coling10morph/</link>
      <pubDate>Sun, 01 Aug 2010 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/coling10morph/</guid>
      
        <description>
          Because English is a low morphology language, current statistical parsers tend to ignore morphology and accept some level of redundancy. This paper investigates how costly such redundancy is for a lexicalised grammar such as CCG.

We use morphological analysis to split verb inflectional suffixes into separate tokens, so that they can receive their own lexical categories. We find that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can benefit from morphological analysis, so long as the analysis is correct.
        </description>
      

    </item>
    
    <item>
      
        <title>Adaptive CCG Supertagging Model</title>
      
      <link>http://www.jkk.name/data/ccg-model/</link>
      <pubDate>Thu, 01 Jul 2010 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/data/ccg-model/</guid>
      
        <description>A model for the C&amp;C supertagger that gives the same results with smaller beam sizes, enabling faster parsing. TODO</description>
      

    </item>
    
    <item>
      
      <title>
        "Faster Parsing by Supertagger Adaptation",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Jessika Roesner, Tim Dawborn, James Haggerty, James R. Curran, Stephen Clark
        
        (ACL,
        2010)
      </title>
      
      <link>http://www.jkk.name/publication/acl10adapt/</link>
      <pubDate>Thu, 01 Jul 2010 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/acl10adapt/</guid>
      
        <description>
          We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Faster parsing and supertagging model estimation",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Jessika Roesner, James R. Curran
        
        (ALTA,
        2009)
      </title>
      
      <link>http://www.jkk.name/publication/alta09tagging/</link>
      <pubDate>Tue, 01 Dec 2009 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/alta09tagging/</guid>
      
        <description>
          Parsers are often the bottleneck for data acquisition, processing text too slowly to be widely applied. One way to improve the efficiency of parsers is to construct more confident statistical models. More training data would enable the use of more sophisticated features and also provide more evidence for current features, but gold standard annotated data is limited and expensive to produce.

We demonstrate faster methods for training a supertagger using hundreds of millions of automatically annotated words, constructing statistical models that further constrain the number of derivations the parser must consider. By introducing new features and using an automatically annotated corpus we are able to double parsing speed on Wikipedia and the Wall Street Journal, and gain accuracy slightly when parsing Section 00 of the Wall Street Journal.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Adaptive Supertagging for Faster Parsing",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;
        
        (The University of Sydney,
        2009)
      </title>
      
      <link>http://www.jkk.name/publication/thesis09adapt/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/thesis09adapt/</guid>
      
        <description>
          Statistical parsers are crucial for tackling the grand challenges of Natural Language Processing. The most effective approaches to these tasks are data driven, but parsers are too slow to be effectively used on large data sets. State-of-the-art parsers generally cannot process more than one sentence a second, and the fastest cannot process more than fifty sentences a second. The situation is even worse when they are applied outside of the domain of their training data. The fastest systems have two components, a parser, which has time complexity O(n3) and a supertagger, which has linear time complexity. By shifting work from the parser to the supertagger we dramatically improve speed.

This work demonstrates several major novel ideas that improve parsing efficiency. The core idea is that the tags chosen by the parser are gold standard data for its supertagger. This leads to the second surprising conceptual development, that decreasing tagging accuracy can improve parsing performance. To demonstrate these ideas required extensive development of the C&amp;C supertagger, including imple- mentation of more efficient estimation algorithms and parallelisation of the training process. This was particularly challenging as the C&amp;C supertagger is a state-of-the-art high performance system designed with a focus on speed rather than flexibility.

I was able to significantly improve performance on the standard evaluation corpus by using the parser to generate extremely large new resources for supertagger training. I have also shown that these methods provide significant benefits on another domain, Wikipedia text, without the cost of generating human annotated data sets. These parsing performance gains occur while supertagging accuracy decreases.

Despite extensive use of supertaggers to improve parsing efficiency there has been no comprehensive study of the interaction between a supertagger and a parser. I present the first systematic exploration of the relationship, show the potential benefits of understanding it, and demonstrate a novel algorithm for optimising the parameters that define it.

I have constructed models that process newspaper text 86% faster than previously, and Wikipedia text 30% faster, without any loss in accuracy and without the aid of extra gold standard resources in either domain. This work will lead directly to improvements in a range of Natural Language Processing tasks by enabling the use of far more parsed data.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Large-Scale Syntactic Processing: Parsing the Web",
        
          Stephen Clark, Ann Copestake, James R. Curran, Yue Zhang, Aurelie Herbelot, James Haggerty, Byung-Gyu Ahn, Curt Van Wyk, Jessika Roesner, &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Tim Dawborn
        
        (Johns Hopkins University,
        2009)
      </title>
      
      <link>http://www.jkk.name/publication/report09jhu/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/report09jhu/</guid>
      
        <description>
          Scalable syntactic processing will underpin the sophisticated language technology needed for next generation information access. Companies are already using nlp tools to create web-scale question answering and &#39;semantic search&#39; engines. Massive amounts of parsed web data will also allow the automatic creation of semantic knowledge resources on an unprecedented scale. The web is a challenging arena for syntactic parsing, because of its scale and variety of styles, genres, and domains.

The goals of our workshop were to scale and adapt an existing wide-coverage parser to Wikipedia text; improve the efficiency of the parser through various methods of chart pruning; use self-training to improve the efficiency and accuracy of the parser; use the parsed wiki data for an innovative form of bootstrapping to make the parser both more efficient and more accurate; and finally use the parsed web data for improved disambiguation of coordination structures, using a variety of syntactic and semantic knowledge sources.

The focus of the research was the C&amp;C parser (Clark and Curran, 2007c), a state-of-the-art statistical parser based on Combinatory Categorial Grammar (ccg). The parser has been evaluated on a number of standard test sets achieving state-of-the-art accuracies. It has also recently been adapted successfully to the biomedical domain (Rimell and Clark, 2009). The parser is surprisingly efficient, given its detailed output, processing tens of sentences per second. For web-scale text processing, we aimed to make the parser an order of magnitude faster still. The C&amp;C parser is one of only very few parsers currently available which has the potential to produce detailed, accurate analyses at the scale we were considering.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "Classification of Verb Particle Constructions with the Google Web1T Corpus",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, James R. Curran
        
        (ALTA,
        2008)
      </title>
      
      <link>http://www.jkk.name/publication/alta08vpc/</link>
      <pubDate>Mon, 01 Dec 2008 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/alta08vpc/</guid>
      
        <description>
          Manually maintaining comprehensive databases of multi-word expressions, for example Verb-Particle Constructions (VPCs), is infeasible. We describe a new classifier for potential VPCs, which uses information in the Google Web1T corpus to perform a simple linguistic constituency test. Specifically, we consider the fronting test, comparing the frequencies of the two possible orderings of the given verb and particle. Using only a small set of queries for each verb-particle pair, the system was able to achieve an F-score of 78.4% in our evaluation while processing thousands of queries a second.
        </description>
      

    </item>
    
    <item>
      
      <title>
        "The densest packing of AB binary hard-sphere homogeneous compounds across all size ratios",
        
          &lt;span style=&#39;text-decoration:underline;&#39;&gt;Jonathan K. Kummerfeld&lt;/span&gt;, Toby S Hudson, Peter Harrowell
        
        (The Journal of Physical Chemistry B,
        2008)
      </title>
      
      <link>http://www.jkk.name/publication/chem08packing/</link>
      <pubDate>Fri, 01 Aug 2008 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/publication/chem08packing/</guid>
      
        <description>
          This paper considers the homogeneous packing of binary hard spheres in an equimolar stoichiometry, and postulates the densest packing at each sphere size ratio. Monte Carlo simulated annealing optimizations are seeded with all known atomic inorganic crystal structures, and the search is performed within the degrees of freedom associated with each homogeneous AB structure type. Structures isopointal to the FeB structure type are found to have the highest packing fraction at all sphere size ratios. The optimized structures match or improve on the best previously demonstrated packings of this type, and show that compound structures can pack more densely than segregated close-packed structures at all radius ratios less than 0.62.
        </description>
      

    </item>
    
    <item>
      
        <title>Publications</title>
      
      <link>http://www.jkk.name/home/publications/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 -0500</pubDate>
      
      <guid>http://www.jkk.name/home/publications/</guid>
      
        <description></description>
      

    </item>
    
    <item>
      
        <title>Data</title>
      
      <link>http://www.jkk.name/home/data/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/data/</guid>
      
        <description></description>
      

    </item>
    
    <item>
      
        <title>Recent Posts</title>
      
      <link>http://www.jkk.name/home/posts/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/posts/</guid>
      
        <description></description>
      

    </item>
    
    <item>
      
        <title>Contact</title>
      
      <link>http://www.jkk.name/home/contact/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/contact/</guid>
      
        <description></description>
      

    </item>
    
    <item>
      
        <title>Software</title>
      
      <link>http://www.jkk.name/home/software/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>http://www.jkk.name/home/software/</guid>
      
        <description></description>
      

    </item>
    
  </channel>
</rss>
