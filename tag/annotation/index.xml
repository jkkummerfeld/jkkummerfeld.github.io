<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>annotation | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/annotation/</link>
      <atom:link href="https://www.jkk.name/tag/annotation/index.xml" rel="self" type="application/rss+xml" />
    <description>annotation</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Sun, 04 Oct 2020 15:10:12 -0500</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>annotation</title>
      <link>https://www.jkk.name/tag/annotation/</link>
    </image>
    
    <item>
      <title>A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-04_crowdsrl/</link>
      <pubDate>Sun, 04 Oct 2020 15:10:12 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-04_crowdsrl/</guid>
      <description>&lt;p&gt;My 
&lt;a href=&#34;https://www.jkk.name/post/2020-09-25_crowdqasrl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions.
This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.&lt;/p&gt;
&lt;p&gt;The core new idea is a filtering process in which workers identify &lt;em&gt;incorrect&lt;/em&gt; answers for a task.
This is the first step of a three stage process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Five workers iteratively filter the options for a label (either for a predicate or argument) until there are only three.&lt;/li&gt;
&lt;li&gt;Five workers select the correct answer.&lt;/li&gt;
&lt;li&gt;If the workers disagree or any of them indicates uncertainty, ask an expert to annotate the example.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To make this work, we use an automatic system for identifying spans for predicates and arguments.
This is better than going straight to the second step because it makes the set of labels less overwhelming, focusing effort on the subtle distinctions between the options.&lt;/p&gt;
&lt;p&gt;This mixture of crowd and expert effort achieves high accuracy (94%) while only having 12% of examples annotated by experts.
The cost is about 52 cents per label for crowd work plus the cost of the expert.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a little tricky to compare the cost with prior work.
Comparing to other work on SRL, we spend more on the crowd, but less on experts.
Whether that trade-off is worth it will depend on the cost of experts.
In practise, our experts are often members of the research team and so their time is a stronger constraint than the crowdsourcing budget.
In that case, our approach comes out ahead as we can get more data annotated per unit of expert effort (by a factor of four).
The QA-SRL work is quite a bit cheaper, at 54 cents per predicate with 2.9 roles on average (which would be ~$2 + expert effort for our approach), but the type of annotations collected are quite different, with ours providing labels from the sense inventory in Propbank.&lt;/p&gt;
&lt;p&gt;I see a range of interesting potential improvements for future work.
First, bringing in methods of worker training in order to improve their accuracy and so reduce the need for duplicate effort.
Second, combining with ideas from other work, such as having a model deciding whether examples are easy or hard and changing how they are processed accordingly, or using QA-SRL annotation to inform the process.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp-findings20srl.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1314632048070594560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My Tweet&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp-findings20srl,
  title     = {A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels},
  author    = {Jiang, Youxuan and Zhu, Huaiyu and Kummerfeld, Jonathan K. and Li, Yunyao and Lasecki, Walter},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  shortvenue = {Findings of EMNLP},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://www.jkk.name/pub/emnlp-findings20srl.pdf},
  abstract  = {Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95\% accuracy for predicate labels and 93\% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56\% to 14\% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Practical Obstacles to Deploying Active Learning (Lowell, et al., EMNLP 2019)</title>
      <link>https://www.jkk.name/post/2020-09-17_activelearningbrittle/</link>
      <pubDate>Thu, 17 Sep 2020 15:08:35 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-17_activelearningbrittle/</guid>
      <description>&lt;p&gt;Training models requires massive amounts of labeled data.
We usually sample data iid from the target domain (e.g. newspapers), but it seems intuitive that this means we wast effort labeling samples that are obvious or easy and so not informative during training.
Active Learning follows that intuition, labeling data incrementally, selecting the next example(s) to label based on what a model considers uncertain.
Lots of work has shown this can be effective for that model, but if the labeled dataset is then used to train another model will it also do well?&lt;/p&gt;
&lt;p&gt;For text classification this paper finds the answer is no: training model X on iid samples is as good or better than training on samples collected while active learning with model Y.
They show this through experiments with four datasets and three models, training on up to 25% of the available data.
For named entity recognition the story is different in my opinion - iid is consistently slightly worse, though the gains from active learning are small in all cases (0 to 0.6 point gain for the better model, 0.4 to 1.7 for the weaker model).
One caveat is that these models are not state-of-the-art.
For CoNLL 2003 NER, many models score 
&lt;a href=&#34;https://nlpprogress.com/english/named_entity_recognition.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;around 93&lt;/a&gt;, but these models are getting 70-90.
On OntoNotes, the best results are close to 90, but these models get 74-85.
This is still an interesting result, but I&amp;rsquo;m left with a few questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This work focused on a low data scenario. What if I have a lot of data? It may be that sampling iid and active learning based samples were similar here because either way the data was capturing the core phenomena. The challenge here is that you can&amp;rsquo;t run this experiment easily with an existing dataset (unless it is truly massive).&lt;/li&gt;
&lt;li&gt;How does the sampled data differ from iid data? Is there a significant shift in the distribution of class types?&lt;/li&gt;
&lt;li&gt;What about using a hybrid approach, with some data sampled iid and other data sampled randomly?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, what I take away from this work is that active learning may not be the right choice for building a small dataset in NLP.
For large datasets, building models, or other tasks and domains the conclusions are less clear, though it is certainly worth being aware of the risk that a dataset made with active learning may not be equally useful to all models.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D19-1003.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Twitter discussion in 
&lt;a href=&#34;https://twitter.com/zacharylipton/status/1019222882482905088&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018&lt;/a&gt;) and 
&lt;a href=&#34;https://twitter.com/zacharylipton/status/1165692913290043398?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{lowell-etal-2019-practical,
    title = &amp;quot;Practical Obstacles to Deploying Active Learning&amp;quot;,
    author = &amp;quot;Lowell, David  and
      Lipton, Zachary C.  and
      Wallace, Byron C.&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)&amp;quot;,
    month = &amp;quot;nov&amp;quot;,
    year = &amp;quot;2019&amp;quot;,
    address = &amp;quot;Hong Kong, China&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/D19-1003&amp;quot;,
    doi = &amp;quot;10.18653/v1/D19-1003&amp;quot;,
    pages = &amp;quot;21--30&amp;quot;,
    abstract = &amp;quot;Active learning (AL) is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. In AL, one iteratively selects training examples for annotation, often those for which the current model is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice, one does not have the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>SLATE: A Super-Lightweight Annotation Tool for Experts</title>
      <link>https://www.jkk.name/software/slate/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/software/slate/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sequence Effects in Crowdsourced Annotations (Mathur et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-08_crowdbias/</link>
      <pubDate>Fri, 08 Dec 2017 19:49:09 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-08_crowdbias/</guid>
      <description>&lt;p&gt;Getting high quality annotations from crowdsourcing requires careful design.
This paper looks at how one annotation a worker does can influence their next annotation, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When scoring translations, a good example may make the next one look worse in comparison&lt;/li&gt;
&lt;li&gt;For labeling tasks, we may expect a long sequence of the same label to be rare (the gambler&amp;rsquo;s fallacy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To investigate this they fit a linear model with inputs (previous label, gold label, random noise) and see what the coefficients are.
Across multiple tasks, there is a non-zero correlation with the previous label.
Interestingly, there also seems to be a learning effect for good workers, where over time they become calibrated and show less sequence bias.
Fortunately, there is a simple solution - for each worker, give every annotator their documents in a different random order!
With that change, averaging over annotations should avoid this bias.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1306&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{mathur-baldwin-cohn:2017:EMNLP2017,
  author    = {Mathur, Nitika  and  Baldwin, Timothy  and  Cohn, Trevor},
  title     = {Sequence Effects in Crowdsourced Annotations},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {2860--2865},
  url       = {https://aclanthology.org/D17-1306}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)</title>
      <link>https://www.jkk.name/post/2017-10-13_errordetection/</link>
      <pubDate>Fri, 13 Oct 2017 13:32:19 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-13_errordetection/</guid>
      <description>&lt;p&gt;Active learning doesn&amp;rsquo;t seem to get much attention in NLP, probably because of fear that developing data based on the errors of one model will introduce a particular sampling bias.
This paper is a nice example of a problem it can be applied to that doesn&amp;rsquo;t raise that issue: detecting all the errors in a system&amp;rsquo;s output.&lt;/p&gt;
&lt;p&gt;The scenario is that you have a bunch of models for doing a task (e.g. POS tagging) and a new dataset with no labeled data, which you would like to label.
Having a person label the data would take a long time and doesn&amp;rsquo;t take advantage of these systems.
At the same time, we can&amp;rsquo;t just run the systems and use their output because they aren&amp;rsquo;t perfect, particularly out of domain.
We could run the systems and check their output, which could be faster than annotating directly, but would still take a long time.
If we don&amp;rsquo;t mind having some errors, we can check just some output, but how do we decide what to check?&lt;/p&gt;
&lt;p&gt;This paper applies the generative model from 
&lt;a href=&#34;https://aclanthology.org/N13-1132&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MACE&lt;/a&gt; to build a generative model of system outputs.
The model is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each example, sample the true label with a uniform prior&lt;/li&gt;
&lt;li&gt;Then, for each classifier, sample from a Bernoulli distribution to decide if they are good or not&lt;/li&gt;
&lt;li&gt;A good classifier returns the true label, a not good classifier samples from a multinomial over the options&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since we don&amp;rsquo;t know the parameters of the model, or the true labels, use expectation maximisation to learn.&lt;/p&gt;
&lt;p&gt;This work takes that model, trains it and uses it to identify the sample that is most uncertain.
A person annotates it, the correct label replaces one of the system predictions, and EM is run again.
This is repeated until either there appear to be no more errors, or annotators run out of time.&lt;/p&gt;
&lt;p&gt;How well does it work?
The main metric is precision: how many of the instances asked for annotation actually have errors.
For POS tagging on WSJ text, the taggers initially get 2.5% of words wrong.
To get that down to 1.1% the precision is 33%, and to get it to 0.65% precision is 17.6%.
On an out of domain dataset, the error rate is 10% initially, and is down to 5% with a precision of 50%.
Put differently, in a dataset of 25,000 tokens, with 2,500 errors, after checking 2,500 tokens, there are only 1,250 errors (another 2,500 checks brings it down to 730).
It also works well for NER, and consistently does better than the alternative they compare to (consider the taggers a committee and find the examples with highest entropy, i.e. greatest disagreement).&lt;/p&gt;
&lt;p&gt;This seems like a natural fit for 
&lt;a href=&#34;https://prodi.gy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prodigy&lt;/a&gt; and something that could be broadly useful.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P/P17/P17-1107.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{rehbein-ruppenhofer:2017:Long,
  author    = {Rehbein, Ines  and  Ruppenhofer, Josef},
  title     = {Detecting annotation noise in automatically labelled data},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1160--1170},
  url       = {https://aclanthology.org/P17-1107}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
