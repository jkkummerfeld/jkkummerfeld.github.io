<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neural-network | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/neural-network/</link>
      <atom:link href="https://www.jkk.name/tag/neural-network/index.xml" rel="self" type="application/rss+xml" />
    <description>neural-network</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Tue, 08 May 2018 09:00:31 -0400</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>neural-network</title>
      <link>https://www.jkk.name/tag/neural-network/</link>
    </image>
    
    <item>
      <title>The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models (Weber et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-05-08_seq2seqsensitivity/</link>
      <pubDate>Tue, 08 May 2018 09:00:31 -0400</pubDate>
      <guid>https://www.jkk.name/post/2018-05-08_seq2seqsensitivity/</guid>
      <description>&lt;p&gt;We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models usually end up doing reasonably well.
This paper asks an important question - are those metrics measuring generalisability effectively?
In particular, if we sample our test set from a slightly different distribution of data, do models still work well?&lt;/p&gt;
&lt;p&gt;As a controlled set up they form a simple dataset as follows for each sentence:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go through the sentence left to right&lt;/li&gt;
&lt;li&gt;For each word generate three words in the output, where the output words are randomly sampled from a small vocabulary that is unique to each input word&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is clearly learnable and it seems reasonable that a sequence-to-sequence neural model with attention should be able to learn it.
Experiments show they do, getting close to 100% on a test set sampled the same way as the training set (input length 5-10, no symbol used twice).
However, if the test set is slightly different, with sequences of length 11-15, then results vary from 0% to 98% depending on the random seed in training (other variations also lead to large variations).
What this means is that sometimes the model is not learning to generalise.
They also show that the models that do generalise can only do so in one way (e.g. remain effective when length varies, or remain effective when symbols are used more than once in the input).&lt;/p&gt;
&lt;p&gt;A few takeaways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure your training and testing data are sampled from the distribution you are interested in&lt;/li&gt;
&lt;li&gt;More study of training data order and weight initialisation is needed (these are the two factors impacted by the random seed)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Incidentally, I am a co-author on an ACL paper that points out a similar issue for mapping text questions to SQL queries.
If we restrict the test set to be novel queries (i.e. the model has to generalise) performance falls through the floor.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1805.01445&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{Weber:2018:GenDeep,
  author    = {Weber, Noah and Shekhar, Leena and Balasubramanian, Niranjan},
    title = {The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models},
  journal = {Workshop on New Forms of Generalization in Deep Learning and NLP (NAACL 2018)},
     year = {2018},
      url = {https://arxiv.org/abs/1805.01445},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-21_multiinputattention/</link>
      <pubDate>Tue, 21 Nov 2017 16:42:19 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-21_multiinputattention/</guid>
      <description>&lt;p&gt;Attention, a weighted average over vectors with weights determined based on context (usually decoder state), has proven effective in many NLP tasks.
There are several variants, and this paper adds new types that address the question of how to apply attention to different sources at the same time, such as text and an image.&lt;/p&gt;
&lt;p&gt;They consider three general versions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Concatenation, just do attention separately then concatenate the vectors from the input sources&lt;/li&gt;
&lt;li&gt;Flat, do the weighted average over all of the inputs&lt;/li&gt;
&lt;li&gt;Hierarchical, do attention separately, but then combine the vectors with another phase of attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They also explore two variations that are orthogonal to the list above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first step and the last step in attention both involve the input vectors being multiplied by a weight matrix. Should that matrix be shared for the two steps, or different? (the first informs the decision of what to give high weight in the average, the second determines what is being averaged over)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;sentinel gates&lt;/em&gt;, a modification to the way the inputs and context vector are combined that allow one or the other to be ignored.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They consider two tasks, (1) translation when both an image and source sentence are given, (2) post-editing a translated sentence with the original source given.
The results show fairly clear trends, though the systems are not great compared to baselines (worse than a text only baseline for the first, and only slightly better than a direct MT system for the second).
The trends are that hierarchical is best, the sentinel doesn&amp;rsquo;t help, and it is better to not share weights (though I wonder if that would be true when controlling for the total number of parameters).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-2031&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{libovicky-helcl:2017:Short,
  author    = {Libovick\&#39;{y}, Jind\v{r}ich  and  Helcl, Jind\v{r}ich},
  title     = {Attention Strategies for Multi-Source Sequence-to-Sequence Learning},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {196--202},
  url       = {https://aclanthology.org/P17-2031}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Shazeer et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-01_mixtureofexperts/</link>
      <pubDate>Wed, 01 Nov 2017 21:57:27 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-11-01_mixtureofexperts/</guid>
      <description>&lt;p&gt;Mixture of experts can be seen as an ensemble approach in which we assume that each of our models is effective under different circumstances and so we combine them by switching between which we use to make our decision.
From this perspective the idea can be applied to any set of models, but here the idea is to train (1) the expert models, (2) our method of choosing between them, and (3) a set of common model components, all at the same time.&lt;/p&gt;
&lt;p&gt;The particular set up here is that they modify a series of LSTM layers, adding a new layer in between each pair of LSTMs.
The new layer has a set of small feed-forward networks (the experts) and an even simpler network that chooses which expert to use.
One big benefit of this is that a lot of computation can be avoided when we know some of the small feed-forward components are going to be ignored.
As a result, they can scale up to massive networks while still having reasonable runtimes.&lt;/p&gt;
&lt;p&gt;Some key things to make this all work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enough machines to train it! Also, there is a careful mixture of data and model parallelism during training.&lt;/li&gt;
&lt;li&gt;Some noise in the expert selection process&lt;/li&gt;
&lt;li&gt;A loss that directly encourages the use of multiple experts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One thing mentioned in passing is how this relates to a form of dropout (which can be viewed as training a set of overlapping experts, kind of).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=B1ckMDqlg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{45929,
	title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author    = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	year  = {2017},
  booktitle = {ICLR},
	URL = {https://openreview.net/pdf?id=B1ckMDqlg},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-10-30_neuralsequence/</link>
      <pubDate>Mon, 30 Oct 2017 13:28:30 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-30_neuralsequence/</guid>
      <description>&lt;p&gt;Language is bursty, with rare words occurring in clumps, the simplest example being an unusual name that occurs a lot in one news article, but not in other articles.
This paper is about how to modify a neural language model to take this into consideration, by adapting the model over time.&lt;/p&gt;
&lt;p&gt;The main idea is to have one model of overall word usage (global) and a separate model that shifts over time to take into consideration the current text (local).
The idea of adapting is not new (as the paper makes clear), but the key here is an update rule that is a modified form of RMSprop, combining the local and global models.
It also seems like performing the updates after every 5 words is important, balancing frequency with informativeness (though no ablation of frequencies is presented).
Conveniently, this is orthogonal to many other ideas and can essentially be stapled on top of a range of sequential architectures, consistently leading to improvements.&lt;/p&gt;
&lt;p&gt;One question left open is how this would work in generation.
The paper describes how it could be applied and could provide improvements, but it also seems likely to risk the repetitive outputs seen in many dialogue systems.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1709.07432.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv170907432K,
   author = {{Krause}, B. and {Kahembwe}, E. and {Murray}, I. and {Renals}, S.},
    title = &amp;quot;{Dynamic Evaluation of Neural Sequence Models}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1709.07432},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
     year = 2017,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170907432K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Searching for Activation Functions (Ramachandran et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-10-27_swishactivation/</link>
      <pubDate>Fri, 27 Oct 2017 11:22:45 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-27_swishactivation/</guid>
      <description>&lt;h2 id=&#34;update&#34;&gt;Update&lt;/h2&gt;
&lt;p&gt;After prior work came to light that uses the same non-linearity, this paper was updated to focus more on the search process used to investigate the space.
One new takeaway for me was how diverse the effective activation functions were (see figures in the new version of the paper).&lt;/p&gt;
&lt;h2 id=&#34;original-post&#34;&gt;Original Post&lt;/h2&gt;
&lt;p&gt;Non-linear functions are the key to the representation power of neural networks.
Many different ones have been proposed, though it is difficult to make theoretical claims of their properties and so the choice of which to use is generally empirical.
This paper proposes a new non-linearity, $\text{swish}(x) = x \cdot \text{sigmoid}(x)$.&lt;/p&gt;
&lt;p&gt;Interestingly, it was chosen by a combination of exhaustive search and search with reinforcement learning across a range of functions, evaluating on CIFAR-10 with a small model.
ReLU variants were consistently second-best to swish variants, and generally the more complicated functions performed worse.
They do mention two functions that performed well, but didn&amp;rsquo;t generalise: $\text{cos}(x) - x$ and $\text{max}(x, \text{tanh}(x))$, which look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/activation-functions.png&#34; alt=&#34;Four activation functions&#34;&gt;&lt;/p&gt;
&lt;p&gt;In a range of experiments in vision and machine translation swish does at least as well or slightly better than the alternatives.
It also seems more robust to network depth and to work across different network structures.
As for why it works so well, there are two main ideas: (1) it adds smoothness to the ReLU, (2) it has some sensitivity to negative inputs.
Both of these seem particularly important at the start of training.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1710.05941.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv171005941R,
   author = {{Ramachandran}, P. and {Zoph}, B. and {Le}, Q.~V.},
    title = &amp;quot;{Swish: a Self-Gated Activation Function}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1710.05941},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171005941R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Attention Is All You Need (Vaswani et al., ArXiv 2017)</title>
      <link>https://www.jkk.name/post/2017-10-20_onlyattention/</link>
      <pubDate>Fri, 20 Oct 2017 15:25:23 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-20_onlyattention/</guid>
      <description>&lt;p&gt;Recurrent neural networks like LSTMs and GRUs have limited scope for parallelisation because each step depends on the one before it.
This architecture also means that many steps of computation separate two words that are far apart, making it difficult to capture long-distance relations.
A range of approaches have been used to try to address these issues, such as convolutional structures and other forms of recurrence (e.g. QRNNs).
The idea in this work is to use attention, applied multiple times, to get a network that is fast while still capturing positional information.&lt;/p&gt;
&lt;p&gt;To explain the structure I put together the figure below, which captures the network structure with a few simplifications:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/google-attention.png&#34; alt=&#34;Google Attention Network&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are a few ideas being brought together here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Positional encoding&lt;/em&gt;, which is a vector of the same length as the word representation, but that depends only on the position in the input. Here they use $f(pos, dim) = sin(pos / 10000^{2 dim / d_w})$ for even dimensions and the cosine equivalent for odd dimensions (where $d_w$ is the number of dimensions.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multi-head attention&lt;/em&gt;, where rather than running attention once on the full vector, multiple linear transforms are applied to get smaller vectors.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Scaled dot product attention&lt;/em&gt;, the equation is shown in the figure, the key new idea is to rescale by the square root of the dimensionality so that larger vectors don&amp;rsquo;t produce excessively sharp distributions after the softmax is applied. The more general form of this described in the paper has keys ($K$), queries ($Q$) and values ($V$), but the network uses the same vector for the key and value. I show the query with a solid line and the values/keys with dotted lines. The matrix $V$ is formed by using the $v$ vectors as rows, while $Q$ is formed by duplicating $q$ in every row. Note, on the left hand side attention is over all input words, while on the right it is only over the words produced so far (ie. content to the left).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Layer normalisation&lt;/em&gt;, a way to rescale weights to keep vector outputs in a nice range, from 
&lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ba, Kiros and Hinton (ArXiv 2016)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Other details, (1) When the outputs are words, the vectors used to initially represent each input word are also used to represent the outputs and in the final linear transformation (though with some rescaling). (2) They use a formula I haven&amp;rsquo;t seen before to adjust the learning rate during training, (3) dropout in several places and label smoothing are used for regularization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simplifications in the figure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For multi-head attention I only show two transforms, while in practise they used 8.&lt;/li&gt;
&lt;li&gt;The shaded regions are duplicated 6 times to create a stack, with the output of one region acting as the input to the next copy of it. The links from left to right are always from the top of the input stack.&lt;/li&gt;
&lt;li&gt;The musical repeat signs indicate that the structure is essentially the same. On the output side this isn&amp;rsquo;t quite true since the attention boxes only take inputs to their left (since output to the right doesn&amp;rsquo;t exist when they are being calculated).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In terms of experiments, it works at least as well if not better than prior approaches, and is a lot faster for machine translation (no speed numbers are given for parsing).
There is also some nice analysis of what it ends out using the attention mechanism to focus on for each word.
It seems like it can provide a way to effectively disambiguate the sense of a word based on its context.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Google also has some blog posts up

&lt;a href=&#34;https://research.googleblog.com/2017/08/transformer-novel-neural-network.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;about the paper&lt;/a&gt;
and

&lt;a href=&#34;https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;about the library&lt;/a&gt;
they released.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{arxiv:1706.03762,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title     = {Attention Is All You Need},
  journal   = {ArXiv},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)</title>
      <link>https://www.jkk.name/post/2017-10-05-deftnn/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-05-deftnn/</guid>
      <description>&lt;p&gt;This paper proposes two techniques for speeding up neural network execution on GPUs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reduce computation when doing matrix-multiply by removing rows.&lt;/li&gt;
&lt;li&gt;Reduce communication on the GPU by halving the number of bits used to represent numbers.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Either of these gives a speed up of ~1.5x and together they give ~2x, across a range of different computer vision tasks+models.&lt;/p&gt;
&lt;h2 id=&#34;core-ideas-in-detail&#34;&gt;Core ideas in detail&lt;/h2&gt;
&lt;p&gt;The first idea, reducing work by eliminating parts of the computation, has been considered before.
In the past, however, the focus was on saving memory in models, and so the most common strategy was to move to a sparse matrix where weights close to zero are dropped.
Here the focus is on speed and they show that while the sparse approach saves memory it can end up being slower because of hardware behaviour.
Instead, they eliminate entire rows of the matrix, which means there is less computation, but it remains dense (and therefore fast).
Rows are identified by measuring correlation between outputs and greedily eliminating rows that correlate highly with the rest of the output.&lt;/p&gt;
&lt;p&gt;The natural question to ask is whether this hurts performance.
First, they do two things to avoid problems, (1) a scale factor is used to make sure the outputs are of the same range that they would have been with the full matrix, and (2) they restart training to fine-tune the network once pruning is set up.
With high enough pruning accuracy does fall, but speed ups can be gained before that is a problem (the exact point depends on the task).&lt;/p&gt;
&lt;p&gt;The second idea relates to numerical representation, and is motivated by measurements of where the bottlenecks are in communication.
Many AI researchers have tried switching to 16 bit representations to save space and time, but here they develop a different floating point encoding that gives more bits to the exponent, and fewer to the mantissa.&lt;/p&gt;
&lt;h2 id=&#34;thoughts&#34;&gt;Thoughts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It would be interesting to see the interaction of this work with the investigation of networks without non-linear functions that can still learn non-linear behaviour because of numerical approximations.&lt;/li&gt;
&lt;li&gt;In the context of language, the weight reduction approach would be interesting to analyse. Specifically, what do we lose in our word vectors depending on the task?&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve always had some interest in making things faster. It would be interesting to know where the remaining bottlenecks are (after applying these changes).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Hill:MICRO:2017,
  author    = {Hill, Parker and Jain, Animesh and Hill1, Mason and Zamirai, Babak and Hsu, Chang-Hong and Laurenzano, Michael A. and Mahlke, Scott and Tang, Lingjia and Mars, Jason},
  title = {DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission},
  booktitle = {The 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  year = {2017},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
