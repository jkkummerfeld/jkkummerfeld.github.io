<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dialogue | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/dialogue/</link>
      <atom:link href="https://www.jkk.name/tag/dialogue/index.xml" rel="self" type="application/rss+xml" />
    <description>dialogue</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Mon, 12 Oct 2020 10:28:13 -0500</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>dialogue</title>
      <link>https://www.jkk.name/tag/dialogue/</link>
    </image>
    
    <item>
      <title>Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-12_taboo/</link>
      <pubDate>Mon, 12 Oct 2020 10:28:13 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-12_taboo/</guid>
      <description>&lt;p&gt;When we crowdsource data for tasks like SRL and sentiment analysis we only care about accuracy.
For tasks where workers write new content, such as paraphrasing and creating questions, we also care about data diversity.
If our data is not diverse then models trained on it will not be robust in the real world.
The core idea of this paper is to encourage creativity by constraining workers.&lt;/p&gt;
&lt;p&gt;We use three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Collect some data.&lt;/li&gt;
&lt;li&gt;Create a taboo list of words / phrases based on the data collected.&lt;/li&gt;
&lt;li&gt;Return to step 1, but tell workers they can&amp;rsquo;t use things in the taboo list.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We explored this idea for task-oriented dialogue.
We identified taboo words using an SVM with a bag-of-words to identify common words associated with specific intents or slot values.
Depending on the taboo word, we got quite different paraphrases.
For example, for the sentence &amp;ldquo;What is the capital of Florida?&amp;rdquo; we collected paraphrases with various taboo words:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Taboo Word&lt;/th&gt;
&lt;th&gt;Paraphrases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;what city is the state capital of florida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;florida&lt;/td&gt;
&lt;td&gt;what is the capital of the sunshine state&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capital&lt;/td&gt;
&lt;td&gt;where is the seat of government in florida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;what&lt;/td&gt;
&lt;td&gt;tell me the name of florida&amp;rsquo;s capital&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These examples show interesting variations, but to see if the variations are significant we tried collecting new test sets for five intent classification datasets and four slot filling datasets.
With just two taboo words, a BERT based model trained on the original dataset did considerably worse on our new data.
The drop varied from 2 to 33 points, with a median of 9.
This indicates that we are capturing ways of expressing these intents that are not well covered by the original data.&lt;/p&gt;
&lt;p&gt;Addressing this issue is simple - train on data collected with our method!
Interestingly, this approach is complementary to the outlier-based approach from 
&lt;a href=&#34;https://www.jkk.name/publication/naacl19outliers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;our NAACL 2019 paper&lt;/a&gt;.
Examples collected using one approach are hard for models trained on data from the other.
Fortunately, training with data from a mix of the two leads to strong results on both.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m particularly excited about this work because the general idea could be applied in so many ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change the task.&lt;/li&gt;
&lt;li&gt;Vary the type of taboo item (e.g. phrases instead of words).&lt;/li&gt;
&lt;li&gt;Try other ways of selecting taboo items.&lt;/li&gt;
&lt;li&gt;Use a different mapping from taboo items to tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, more specific versions of this idea have already been used.

&lt;a href=&#34;https://dl.acm.org/doi/10.1145/985692.985733&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luis von Ahn&amp;rsquo;s ESP game&lt;/a&gt; for image captioning used a taboo list.
Each image had a list of complete labels previously assigned to the image.
New labels could not match an existing label.
That work predates this paper by 16 years, but hasn&amp;rsquo;t had much traction in the NLP community, possibly because of the limitation of requiring complete matches on labels (making it impractical for sentences or even phrases).
I&amp;rsquo;m hopeful that our more general version will be useful in a range of crowdsourcing efforts.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp20taboo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp20taboo,
  title     = {Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness},
  author    = {Larson, Stefan and Zheng, Anthony and Mahendran, Anish and Tekriwal, Rishi and Cheung, Adrian and Guldan, Eric and Leach, Kevin and Kummerfeld, Jonathan K.},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://www.jkk.name/pub/emnlp20taboo.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)</title>
      <link>https://www.jkk.name/post/2020-09-07_chartdialogs/</link>
      <pubDate>Mon, 07 Sep 2020 14:41:34 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-07_chartdialogs/</guid>
      <description>&lt;p&gt;Natural language interfaces to computer systems are an exciting area with new workshops (
&lt;a href=&#34;https://aclanthology.org/volumes/2020.nli-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WNLI&lt;/a&gt; at ACL and 
&lt;a href=&#34;https://intex-sempar.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IntEx-SemPar&lt;/a&gt; at EMNLP), a range of datasets (including my own work on 
&lt;a href=&#34;https://www.jkk.name/publication/acl18sql/&#34;&gt;text-to-SQL&lt;/a&gt;), and many papers.
Most work focuses on either (1) commands for simple APIs, (2) generating a database query, or (3) generating general purpose code.
This paper considers an interesting application: interaction with data visualisation tools.&lt;/p&gt;
&lt;p&gt;Using the full flexibility of these tools is a tall order, so this work focuses on commands to modify style parameters of a figure.
For that setting, the problem can be framed as task-oriented dialogue in which each style parameter (e.g. x-axis font size) is a slot that needs to be defined.
Using this framing of the problem, the paper presents a new dataset of 3,200 conversations in which a person modifies the style of a plot.
These were collected on Mechanical Turk by having one worker describe a target plot and another worker manipulating values for parameters to match it.
There are 12 plot types with 3-13 properties, with the target plot randomly generated.
Baseline approaches do fairly well, but far short of a human (either another worker or one of the authors).&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a large resource with high agreement between annotators and the paper presents detailed analysis and helpful examples.
One experiment I&amp;rsquo;d be curious to see is results with a fixed number of training examples per plot type (or per slot type).
Histograms and scatter plots appear particularly difficult in the breakdown of results by plot type, but they are also the types with the fewest examples (a tenth as many as the type with the most).&lt;/p&gt;
&lt;p&gt;I find this general topic exciting because it brings together several areas of NLP and it seems feasible to create a useful system in the near future.
Hopefully there will be progress on models for this dataset and development of additional resources.
In particular, there was a decision here to limit generation to slot-values, which is powerful, but does not capture the full flexibility of matplotlib (at least not without further work on representing more features this way).
Arbitrary code generation would be a fantastic extension, though creating the data would require some creativity as the approach used here wouldn&amp;rsquo;t directly work.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.328.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{shao-nakashole-2020-chartdialogs,
    title = &amp;quot;{C}hart{D}ialogs: {P}lotting from {N}atural {L}anguage {I}nstructions&amp;quot;,
    author = &amp;quot;Shao, Yutong  and
      Nakashole, Ndapa&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.328&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.328&amp;quot;,
    pages = &amp;quot;3559--3574&amp;quot;,
    abstract = &amp;quot;This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61{\%} plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>DSTC 7 track 1: Next Utterance Selection</title>
      <link>https://www.jkk.name/data/dstc7/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/dstc7/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DSTC 8 track 2: Next Utterance Selection</title>
      <link>https://www.jkk.name/data/dstc8/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jkk.name/data/dstc8/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)</title>
      <link>https://www.jkk.name/post/2019-07-10_disentanglement/</link>
      <pubDate>Wed, 10 Jul 2019 11:19:06 -0400</pubDate>
      <guid>https://www.jkk.name/post/2019-07-10_disentanglement/</guid>
      <description>&lt;p&gt;This post is about my own paper to appear at ACL later this month.
What is interesting about this paper will depend on your research interests, so that&amp;rsquo;s how I&amp;rsquo;ve broken down this blog post.&lt;/p&gt;
&lt;p&gt;A few key points first:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://jkk.name/irc-disentanglement/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data and code&lt;/a&gt; are available on Github.&lt;/li&gt;
&lt;li&gt;The 
&lt;a href=&#34;https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; is also available.&lt;/li&gt;
&lt;li&gt;The general-purpose span labeling and linking 
&lt;a href=&#34;https://jkk.name/slate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;annotation tool&lt;/a&gt; we used is also appearing at ACL.&lt;/li&gt;
&lt;li&gt;Check out 
&lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSTC 8 Track 2&lt;/a&gt;, which is based on this work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;you-study-discourse&#34;&gt;You study discourse&lt;/h3&gt;
&lt;p&gt;We investigated discourse structure when multiple conversations are occurring in the same stream of communication.
In our case, the stream is a technical support channel for Ubuntu on Internet Relay Chat (IRC).
We annotated each message with which message(s) it was a response to.
As far as we are aware, this is the first large-scale corpus with this kind of discourse structure in synchronous chat.
Here is an example from the data, with annotations marked by edges and colours:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/irc-disentanglement-example.png&#34; alt=&#34;IRC Disentanglement Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t frame the paper as being about reply-structure though.
Instead, we focus on a byproduct of these annotations - conversation disentanglement.
Given our graph of reply-structure, each connected component is a single conversation (as shown by each colour in the example).
The key prior work on the disentanglement problem is 
&lt;a href=&#34;https://aclanthology.org/P08-1095&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elsner and Charniak (2008)&lt;/a&gt;, who released the largest annotated resource for the task, with 2,500 messages manually separated into conversations.
We annotated their data with our annotation scheme and 75,000 additional messages.&lt;/p&gt;
&lt;p&gt;We built a set of simple models for predicting reply-structure and did some analysis of assumptions about discourse from prior disentanglement work, but there is certainly more scope for study here.
One direction would be to develop better models for this task.
Another would be to study patterns in the data to understand how people are able to follow the conversation.&lt;/p&gt;
&lt;h3 id=&#34;you-work-on-dialogue&#34;&gt;You work on dialogue&lt;/h3&gt;
&lt;p&gt;There has been a lot of work recently using the Ubuntu dataset from 
&lt;a href=&#34;https://github.com/rkadlec/ubuntu-ranking-dataset-creator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lowe et al., (2015)&lt;/a&gt;, which was produced by heuristically disentangling conversations from the same IRC channel we use.
Their work opened up a fantastic research opportunity by providing 930,000 conversations for training and evaluating dialogue systems.
However, they were unable to evaluate the quality of their conversations because they had no annotated data.&lt;/p&gt;
&lt;p&gt;Using our data, we found that only 20% of their conversations are a true prefix of a conversation (since their next utterance classification task cuts the conversation off part-way, being a true prefix is all that matters).
Many conversations are missing messages, and some have extra messages from other conversations.
Unsurprisingly, our trained model does better, producing conversations that are a true prefix 81% of the time.
We also noticed that their heuristic was incorrectly linking messages far apart in time.
This is not tested by our evaluation set, so we constructed this figure, which shows the problem is quite common:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/irc-disentanglement-comparison.png&#34; alt=&#34;IRC Disentanglement Comparison&#34;&gt;&lt;/p&gt;
&lt;p&gt;The purple results are based on the output of our model over the entire Ubuntu IRC logs.
That output is the basis of 
&lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSTC 8 Track 2&lt;/a&gt;.
Once the competition finishes (October 20th, 2019) we will release all of the conversations.&lt;/p&gt;
&lt;h3 id=&#34;you-am-interested-in-studying-online-communities&#34;&gt;You am interested in studying online communities&lt;/h3&gt;
&lt;p&gt;This is not my area of expertise, but our data and models could enable the exploration of interesting questions.
For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the structure of the community? By looking at who asks for help and who responds we could see patterns of behaviour.&lt;/li&gt;
&lt;li&gt;How does a community evolve over time? This data spans 15 years, during which there were many Ubuntu releases, Stackoverflow was created, other Ubuntu forums were created, etc. It seems likely that those events and more would be reflected in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It would be interesting to apply the model to other communities, but that would require additional in-domain data to get good results.
We have no plans to collect additional data at this stage, and for other channels there are copyright questions that might be difficult to resolve (the Ubuntu channels have an open access license).&lt;/p&gt;
&lt;h3 id=&#34;you-mainly-care-about-neural-network-architectures&#34;&gt;You mainly care about neural network architectures&lt;/h3&gt;
&lt;p&gt;We experimented with a bunch of ideas that didn&amp;rsquo;t improve performance, so our final model is very simple (a feedforward network with features representing the logs and sentences represented by averaging and max-pooling GloVe embeddings).
Maybe that means there is an opportunity for you to improve on our results with a fancy model?
One of our motivations for making such a large new resource was to make it possible to train sophisticated models.&lt;/p&gt;
&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;This project has been going since I started at Michigan as a postdoc funded by a grant from IBM.
The final paper is the result of collaboration with a large group of people from Michigan and IBM.
Thank you!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{acl19disentangle,
  author    = {Kummerfeld, Jonathan K. and Gouravajhala, Sai R. and Peper, Joseph and Athreya, Vignesh and Gunasekara, Chulaka and Ganhotra, Jatin and Patel, Siva Sankalp and Polymenakos, Lazaros and Lasecki, Walter S.},
  title     = {A Large-Scale Corpus for Conversation Disentanglement},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  location  = {Florence, Italy},
  month     = {July},
  year      = {2019},
  url       = {https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf},
  arxiv     = {https://arxiv.org/abs/1810.11118},
  software  = {https://jkk.name/irc-disentanglement},
  data      = {https://jkk.name/irc-disentanglement},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-01-28_crowdassistant/</link>
      <pubDate>Sun, 28 Jan 2018 16:01:20 -0500</pubDate>
      <guid>https://www.jkk.name/post/2018-01-28_crowdassistant/</guid>
      <description>&lt;p&gt;There is a lot of interest in dialogue agents, but a lot of work sits at one of two extremes: either chit-chat agents that just chat, or task-oriented agents that aim to call a specific API for the user.
This work is about trying to integrate a range of systems from both categories, to get something more general purpose as a result.&lt;/p&gt;
&lt;p&gt;The core approach is a hybrid system that switches between different agents behind the scenes (an approach taken by a number of Alexa Prize teams).
The innovation here is that crowd workers will help with the decision (both suggesting things to say and voting on which response to use), and their votes will be used to learn a model to (partially) replace the people over time.&lt;/p&gt;
&lt;p&gt;Unfortunately, the improvement from a learned model of votes is only small (saves only 14% of the crowd effort), and the automated responses are rarely chosen (12% of the time).
That said, it seems like an interesting design with a lot of subtle decisions that require more exploration - the sets of agents (4-6 here, mostly narrow types), the voting scheme (only 1 or 2 votes needed here), choosing which agent responses to show (here, the proportion of previously accepted messages from this agent), and so on.
That choice of which responses to show is particularly tricky, as with this scheme a very domain specific agent might get voted down too much initially and never be chosen when the appropriate time comes.
One potentially interesting alternative would be to let the crowd workers choose which agent&amp;rsquo;s response to see, and possibly even post-edit slightly.&lt;/p&gt;
&lt;p&gt;Note - This post is the first of a (hopefully) regular series again.
However, rather than keeping it weekday-ly, I plan to do three times a week, at least until the ACL deadline.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{blah,
  title = {Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time},
  author    = {Huang, Ting-Hao (Kenneth) and Chang, Joseph Chee and Bigham, Jeffrey P.},
  booktitle = {CHI},
  year = {2018},
  url = {https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-09_framesdataset/</link>
      <pubDate>Thu, 09 Nov 2017 19:47:08 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-09_framesdataset/</guid>
      <description>&lt;p&gt;Another paper about a dataset of dialogues, but this time with structure.
Like the paper from yesterday, the aim is a dataset of task-oriented conversations, but with more complexity than prior work.
The difference is that this work includes a structured representation of the state of the conversation: frames.&lt;/p&gt;
&lt;p&gt;A frame is essentially a tuple describing a query, e.g. (Destination: Sydney, Origin: Ann Arbor, price: 1500 USD).
There are multiple frames in a dialogue (a departure from DSTC tasks), and utterances are labeled with dialogue acts that capture modifications to the frames as well as references to them.
This structure sounds fairly general, though the focus here was on vacation planning, where the user is buying a package.
The setup doesn&amp;rsquo;t maximise the potential complexity though, as there are a small number of set packages available, rather than the complex tradeoffs of flight+hotel combinations that exist in practise.
Looking at the example dialogues in the paper, it has complete sentences of some complexity.
One thing I&amp;rsquo;m still curious about is disagreements between annotators, as for the complete task the score was 0.62 +/- 5 (with dialogue acts being trickier than slot values, and no scores for frame references on their own).&lt;/p&gt;
&lt;p&gt;Comparing to the Stanford dataset this is smaller (11k vs. 1.4k), but has more turns per dialogue (11 vs. 15) and probably longer turns too, judging by the examples.
The tasks are completely different, but both come with small tables of information that are private to the two participants and required for almost every turn in the conversation.
Evaluating on both could be a great way to show the flexibility of a dialogue system, but the lack of frames for the Stanford data and the difficulty of running a human evaluation for this data limits the feasible types of multi-domain experiments.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/W17-5526&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{elasri-EtAl:2017:W17-55,
  author    = {El Asri, Layla  and  Schulz, Hannes  and  Sharma, Shikhar  and  Zumer, Jeremie  and  Harris, Justin  and  Fine, Emery  and  Mehrotra, Rahul  and  Suleman, Kaheer},
  title     = {Frames: a corpus for adding memory to goal-oriented dialogue systems},
  booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  month     = {August},
  year      = {2017},
  address   = {Saarbrucken, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {207--219},
  url       = {https://aclanthology.org/W17-5526}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-08_graphdialogue/</link>
      <pubDate>Wed, 08 Nov 2017 18:46:04 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-08_graphdialogue/</guid>
      <description>&lt;p&gt;Task-oriented dialogue systems are often focused on a very narrow task, to the point where the state can be described completely with a tuple (e.g. preferences for a restaurant).
This paper sets up a more challenging task with more complex language use, while still having a specific goal and directly relevant structured information.
They collected 11,000 dialogues, where two people have private lists of friends and are trying to identify which friend they have in common.
While this is a lot of data, the mechanical turk workers are clearly moving fast, with dialogues taking 1.5 minutes on average, and in 18% of cases they get the friend wrong.&lt;/p&gt;
&lt;p&gt;The algorithmic contribution is that the lists of people are represented as a graph, where nodes are properties like company and hobby.
The graph is used to generate vectors for each person by running a form of message passing over its structure.
During generation, the LSTM uses attention over these vectors to inform the output choice.&lt;/p&gt;
&lt;p&gt;A few interesting things in the output:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are cases where the output is incorrect, as in, says a fact about the structured information / knowledge base that is false.&lt;/li&gt;
&lt;li&gt;Evaluation is tricky, and over the metrics they consider sometimes this wins, but sometimes the baseline system (rules) does better. In particular, success on bot-bot evaluation doesn&amp;rsquo;t seem to clearly transfer to bot-human experiments.&lt;/li&gt;
&lt;li&gt;The utterances are very fluent, but that may be because it&amp;rsquo;s essentially copying from the training data. It looks like there is diversity in the dataset, but a lot of utterances do fit a template of &amp;ldquo;I have X who Y&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1162&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{he-EtAl:2017:Long4,
  author    = {He, He  and  Balakrishnan, Anusha  and  Eric, Mihail  and  Liang, Percy},
  title     = {Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1766--1776},
  abstract  = {We study a \emph{symmetric collaborative dialogue} setting
	in which two agents, each with private knowledge,
	must strategically communicate to achieve a common goal.
	The open-ended dialogue state in this setting poses new challenges for existing
	dialogue systems.
	We collected a dataset of 11K human-human dialogues,
	which exhibits interesting lexical, semantic, and strategic elements.
	To model
	both structured knowledge and unstructured language,
	we propose a neural model with dynamic knowledge graph embeddings
	that evolve as the dialogue progresses.
	Automatic and human evaluations show that our model is both more effective
	at achieving the goal and more human-like than baseline neural and rule-based
	models.},
  url       = {https://aclanthology.org/P17-1162}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-03_discourserelations/</link>
      <pubDate>Fri, 03 Nov 2017 15:40:32 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-11-03_discourserelations/</guid>
      <description>&lt;p&gt;Automatically generating high quality meeting notes and summaries would be awesome, but involves solving many challenges.
Here, they assume speech recognition is already done and we also know the structure over utterances indicating which previous utterance each is a response to.
The task is to label each of those utterance-utterance pairs with a type (e.g. elaboration) and to select the key phrase of each utterance.&lt;/p&gt;
&lt;p&gt;Two datasets are used, the AMI and ICSO meeting corpora, which have all of the required information.
The new idea here is to jointly model the choice of link label and the key phrase, which is intuitive.
To show the value of joint modeling they run a version of the system with the same linear model, but with independent inference, which performs quite a bit worse.&lt;/p&gt;
&lt;p&gt;One neat follow up is that by combining the key phrases into a list you get a form of summary.
According to automatic metrics it is quite a bit better than running the summarisation system they compare to, though it&amp;rsquo;s still a long way from a human summary.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/P17-1090&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{qin-wang-kim:2017:Long,
  author    = {Qin, Kechen  and  Wang, Lu  and  Kim, Joseph},
  title     = {Joint Modeling of Content and Discourse Relations in Dialogues},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {974--984},
  url       = {https://aclanthology.org/P17-1090}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
