<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tacl | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/tacl/</link>
      <atom:link href="https://www.jkk.name/tag/tacl/index.xml" rel="self" type="application/rss+xml" />
    <description>tacl</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Mon, 27 Nov 2017 11:21:45 -0500</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>tacl</title>
      <link>https://www.jkk.name/tag/tacl/</link>
    </image>
    
    <item>
      <title>Ordinal Common-sense Inference (Zhang et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-27_commonsense/</link>
      <pubDate>Mon, 27 Nov 2017 11:21:45 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-27_commonsense/</guid>
      <description>&lt;p&gt;When people read a sentence they form an entire world around it, making inferences about unwritten properties based on their prior knowledge.
If we want NLP systems to do the same, we need data to train and test this common sense aspect of language understanding.&lt;/p&gt;
&lt;p&gt;This paper is about a new dataset of automatically generated sentence pairs with human ratings.
The ratings indicate that given the first sentence, the second sentence is either very likely, likely, plausible, technically possible, or impossible.
These ratings are crowdsourced, using the median of three ratings per example.
The pay rates are fairly low, at $3.45 / hour (1.99c / example and 20.71 seconds / example), though it&amp;rsquo;s possible that the time is being skewed by outliers, and it&amp;rsquo;s unclear exactly how pay was determined (does this include Amazon&amp;rsquo;s cut? Why is it an average cost per example, rather than just the cost?).&lt;/p&gt;
&lt;p&gt;The main contribution is the novel way of generating the sentences.
For each prompt sentence, an argument is chosen, and then a hypothesis is generated in one of three ways (all trained with Gigaword).
(1) A sequence-to-sequence model takes the full sentence as input and generates a sentence.
(2) The same as (1), but with only the argument provided.
(3) A sentence is sampled from templates generated by abstraction of sentences in the training data.
Together these produce a diverse set of examples that get a range of ratings, with only &amp;lsquo;likely&amp;rsquo; being somewhat rarer.
They also labeled some pairs from SNLI and COPA, to enable analysis of how this task compares.&lt;/p&gt;
&lt;p&gt;They also provide a set of baselines for the new task.
Using the baselines, they show that the generated sentences are somewhat more difficult than the pairs from existing datasets.
The standard metrics proposed are MSE and Spearman&amp;rsquo;s Rho (both necessary because otherwise always guessing the middle would get an MSE better than any of the proposed baselines).
Interestingly, regression does quite a bit better than a set of one-vs-all SVMs on MSE, and also slightly better on rho (I&amp;rsquo;m surprised because while there is an ordinal scale, it doesn&amp;rsquo;t feel like it should have a strong continuous interpretation).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1082&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1082,
	author = {Zhang, Sheng  and Rudinger, Rachel  and Duh, Kevin  and Van Durme, Benjamin },
	title = {Ordinal Common-sense Inference},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	keywords = {},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1082},
	pages = {379--395}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-15_entityvectors/</link>
      <pubDate>Wed, 15 Nov 2017 18:01:27 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-15_entityvectors/</guid>
      <description>&lt;p&gt;Since word2vec was released there have been a series of X2vec papers, though none have had the success of word vectors.
In this case the idea is to represent entities and chunks of text (words, sentences, paragraphs).&lt;/p&gt;
&lt;p&gt;Entities are represented with vectors.
To get the vector for a chunk of text, they:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sum word vectors for the text.&lt;/li&gt;
&lt;li&gt;Rescale to be of unit length.&lt;/li&gt;
&lt;li&gt;Multiply by a weight matrix and add a bias.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then to learn these, negative log likelihood is used, where the probability is defined as a softmax over the dot product between entity and text vectors.
The data is a portion of Wikipedia annotated with entities as indicated by links (plus they say the entity the page is about is implicitly part of every sentence).&lt;/p&gt;
&lt;p&gt;With these new vectors in hand, they try textual similarity, with strong results.
They also build a very simple entity linking system, a feed-forward network with these representations plus a few other features, and beat all prior work.
Similarly
They apply the same modeling approach to Quizball QA, also with strong results.&lt;/p&gt;
&lt;p&gt;The simplicity and effectiveness of the model really is impressive.
Some qualitative examples are included, but hard to find trends in.
It does seem like a more reasonable vector learning approach than skip-thought and other similar approaches that rely only on text context - the entities provide something different, but clearly closely related.
That said, I feel like more ablation is needed to see what role each of these pieces is playing (are they learning better vectors, or using them in a way that is more effective? Or both?).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1065&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1065,
	author = {Yamada, Ikuya  and Shindo, Hiroyuki  and Takeda, Hideaki  and Takefuji, Yoshiyasu },
	title = {Learning Distributed Representations of Texts and Entities from Knowledge Base},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1065},
	pages = {397--411}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>In-Order Transition-based Constituent Parsing (Liu et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-14_inorderparsing/</link>
      <pubDate>Tue, 14 Nov 2017 14:10:39 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-14_inorderparsing/</guid>
      <description>&lt;p&gt;Shift-reduce constituency parsing incrementally builds the parse either bottom-up or top-down.
The difference is whether a non-terminal is placed on the stack before or after the words that it spans.
This corresponds to two forms of depth-first traversal of the tree: pre-order or post-order.&lt;/p&gt;
&lt;p&gt;The idea in this paper is to do an in-order traversal, which in a binary tree means traversing the left child of a node, then the node, then its right child.
In this context that means putting the non-terminal symbol on the stack after the first word it spans, but before the rest.
The model follows the stack-LSTM approach of Dyer et al., with non-terminals always fed into the LSTM first during composition, regardless of where it was inserted into the stack.&lt;/p&gt;
&lt;p&gt;This leads to a 0.5 F1 gain on standard parsing metrics, with no hyperparameter tuning.
High-level error analysis seems to show it just does better everywhere.
I wonder whether further gains could be realised with a label-sensitive ordering.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/view/1199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1199,
	author = {Liu, Jiangming  and Zhang, Yue },
	title = {In-Order Transition-based Constituent Parsing},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	year = {2017},
	issn = {2307-387X},
	url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1199},
	pages = {413--424}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)</title>
      <link>https://www.jkk.name/post/2017-10-11_multimt/</link>
      <pubDate>Wed, 11 Oct 2017 17:29:04 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-11_multimt/</guid>
      <description>&lt;p&gt;This paper is a detailed analysis of a surprisingly effective simple idea: train a machine translation system with sentence pairs from multiple languages, adjusting the input to have an extra token at the end that says what the target language is.
To deal with class imbalance, data is oversampled to have all language pairs be equally represented (though even without that, it works fairly well).&lt;/p&gt;
&lt;p&gt;The biggest advantage of this approach is that a single model can handle translation between many pairs, rather than needing $O(n^2)$ models for $n$ languages.
The performance is slightly lower on average, but the single model can manage with far fewer parameters.
In one example, twelve models are combined into a single model with as many parameters as one of the twelve, and the results are lower by just 0.76 BLEU on average.
Another advantage of the model is the ability to handle code-switched language, though they didn&amp;rsquo;t have evaluation datasets to get an quantitative measure of accuracy.&lt;/p&gt;
&lt;p&gt;Having this model also opens up the possibility of translating between pairs of languages with no parallel training data (A -&amp;gt; B).
As long as there is data (A -&amp;gt; C) and (D -&amp;gt; B), sentences from A can be fed in with B as the target language.
For closely related languages this works very well, and in particular, better than going via another language such that there is data for the two language pairs.
For example, going from Portuguese to Spanish with the multilingual model scores 24.75, whereas going via English scores 21.62 and a model with explicit training data gets 31.50.
Going between less related languages is less successful, with direct Spanish to Japanese scoring 9.14, and going via English scoring 18.00.
One thing I wish the paper had is more exploration of this result - what does it get right when scoring 9.14?
For the time being at least, going via a third language still seems necessary, and presumably the best language to use is whichever one the performance is highest on.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://aclweb.org/anthology/Q/Q17/Q17-1024.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1611.04558.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv version&lt;/a&gt; which appears to be the same aside from one extra figure of the model architecture.&lt;/p&gt;
&lt;p&gt;As an aside, it is interesting to see the timeline for this paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;November 2016, Submission to ArXiv and in the TACL submission batch&lt;/li&gt;
&lt;li&gt;March 2017, TACL revision batch&lt;/li&gt;
&lt;li&gt;October 2017, TACL published&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{TACL1081,
	author    = {Johnson, Melvin  and Schuster, Mike  and Le, Quoc  and Krikun, Maxim  and Wu, Yonghui  and Chen, Zhifeng  and Thorat, Nikhil  and Viégas, Fernanda  and Wattenberg, Martin  and Corrado, Greg  and Hughes, Macduff  and Dean, Jeffrey},
	title     = {Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	journal   = {Transactions of the Association for Computational Linguistics},
	volume    = {5},
	year      = {2017},
	issn      = {2307-387X},
	url       = {https://www.transacl.org/ojs/index.php/tacl/article/view/1081},
	pages     = {339--351}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
