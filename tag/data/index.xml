<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/data/</link>
      <atom:link href="https://www.jkk.name/tag/data/index.xml" rel="self" type="application/rss+xml" />
    <description>data</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Fri, 25 Sep 2020 10:17:18 -0500</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>data</title>
      <link>https://www.jkk.name/tag/data/</link>
    </image>
    
    <item>
      <title>Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)</title>
      <link>https://www.jkk.name/post/2020-09-25_crowdqasrl/</link>
      <pubDate>Fri, 25 Sep 2020 10:17:18 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-25_crowdqasrl/</guid>
      <description>&lt;p&gt;Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments.
Over the last few years, 
&lt;a href=&#34;https://www.cs.washington.edu/people/faculty/lsz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luke Zettlemoyer&amp;rsquo;s Group&lt;/a&gt; has been exploring using question-answer pairs to represent this structure.
This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank.
However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.&lt;/p&gt;
&lt;p&gt;I got three main things out of this paper:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It shifted my approach to crowdsourcing to consider workers more like traditional expert annotators.&lt;/li&gt;
&lt;li&gt;It reinforced the idea that small shifts in crowd workflows can have a major impact on annotation quality.&lt;/li&gt;
&lt;li&gt;QA-SRL can capture roles not covered by PropBank.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The work also provides a new dataset that will be useful for future work on this problem, and useful benchmarks of systems and measurements of data quality.
Expanding on the three points above:&lt;/p&gt;
&lt;p&gt;Crowd workers: The paper argues in favour of putting more time into training workers.
Most of the work I&amp;rsquo;ve seen in NLP for crowdsourcing (including my own) focuses on modifying task design or using ML post-processing to improve results.
Here, they run a large-scale qualification task and filter workers based on their performance, then train those workers by paying them to read a set of instructions (23 text-dense slides) and do two small annotation rounds with feedback after each one.
This increases the upfront cost, but reduces the cost of annotation by reducing the need for multiple annotations of each item.
The paper doesn&amp;rsquo;t provide quite enough detail to quantify the cost.
We do know that to get to 11 workers they needed to train 30 workers at a cost of 2 hours each plus 30 minutes of researcher time each.
If we assume 60 workers did the preliminary round, each taking 5 minutes, and that workers cost &lt;span&gt;$&lt;/span&gt;12 / hour (&lt;span&gt;$&lt;/span&gt;10 to the workers, &lt;span&gt;$&lt;/span&gt;2 to Amazon), that&amp;rsquo;s almost &lt;span&gt;$&lt;/span&gt;800 plus 15 hours of researcher time.
For a large annotation effort, the savings during annotation will make that worth it (or, as in this case, it will lead to higher quality data).
I am curious which aspect was more important though - filtering the pool of workers, or training workers.&lt;/p&gt;
&lt;p&gt;Workflow impact: In previous QA-SRL work, one worker wrote a question and its answers and two workers checked the question and independently added answers.
Here, two workers independently write a question+answer and a third work consolidates the annotations into a final annotation.
The cost for a label is about the same (54c / predicate vs. 51c / predicate), but coverage is considerably higher.
The design space for crowd workflows is huge and this is another example of how important it is to explore.
It&amp;rsquo;s also possible that the changes in recruitment and training were more critical than the workflow shift, but the study didn&amp;rsquo;t include evaluation with only one or the other.&lt;/p&gt;
&lt;p&gt;QA-SRL vs. PropBank: This may be less surprising to someone who works more on SRL, but they found their approach captured many implicit roles that PropBank does not.
Specifically, of 100 annotated arguments that were not in PropBank, 68 were valid implicit arguments.
I&amp;rsquo;m curious about what those implicit arguments are capturing.
Maybe targeted re-annotation could be used to add them to PropBank (identifying relevant sentences by trace parsing).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.626/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/plroit/qasrl-gs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1309592830537543681?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My Tweet&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{roit-etal-2020-controlled,
    title = &amp;quot;Controlled Crowdsourcing for High-Quality {QA}-{SRL} Annotation&amp;quot;,
    author = &amp;quot;Roit, Paul  and
      Klein, Ayal  and
      Stepanov, Daniela  and
      Mamou, Jonathan  and
      Michael, Julian  and
      Stanovsky, Gabriel  and
      Zettlemoyer, Luke  and
      Dagan, Ido&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.626&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.626&amp;quot;,
    pages = &amp;quot;7008--7013&amp;quot;,
    abstract = &amp;quot;Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)</title>
      <link>https://www.jkk.name/post/2020-09-07_chartdialogs/</link>
      <pubDate>Mon, 07 Sep 2020 14:41:34 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-07_chartdialogs/</guid>
      <description>&lt;p&gt;Natural language interfaces to computer systems are an exciting area with new workshops (
&lt;a href=&#34;https://aclanthology.org/volumes/2020.nli-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WNLI&lt;/a&gt; at ACL and 
&lt;a href=&#34;https://intex-sempar.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IntEx-SemPar&lt;/a&gt; at EMNLP), a range of datasets (including my own work on 
&lt;a href=&#34;https://www.jkk.name/publication/acl18sql/&#34;&gt;text-to-SQL&lt;/a&gt;), and many papers.
Most work focuses on either (1) commands for simple APIs, (2) generating a database query, or (3) generating general purpose code.
This paper considers an interesting application: interaction with data visualisation tools.&lt;/p&gt;
&lt;p&gt;Using the full flexibility of these tools is a tall order, so this work focuses on commands to modify style parameters of a figure.
For that setting, the problem can be framed as task-oriented dialogue in which each style parameter (e.g. x-axis font size) is a slot that needs to be defined.
Using this framing of the problem, the paper presents a new dataset of 3,200 conversations in which a person modifies the style of a plot.
These were collected on Mechanical Turk by having one worker describe a target plot and another worker manipulating values for parameters to match it.
There are 12 plot types with 3-13 properties, with the target plot randomly generated.
Baseline approaches do fairly well, but far short of a human (either another worker or one of the authors).&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a large resource with high agreement between annotators and the paper presents detailed analysis and helpful examples.
One experiment I&amp;rsquo;d be curious to see is results with a fixed number of training examples per plot type (or per slot type).
Histograms and scatter plots appear particularly difficult in the breakdown of results by plot type, but they are also the types with the fewest examples (a tenth as many as the type with the most).&lt;/p&gt;
&lt;p&gt;I find this general topic exciting because it brings together several areas of NLP and it seems feasible to create a useful system in the near future.
Hopefully there will be progress on models for this dataset and development of additional resources.
In particular, there was a decision here to limit generation to slot-values, which is powerful, but does not capture the full flexibility of matplotlib (at least not without further work on representing more features this way).
Arbitrary code generation would be a fantastic extension, though creating the data would require some creativity as the approach used here wouldn&amp;rsquo;t directly work.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.328.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{shao-nakashole-2020-chartdialogs,
    title = &amp;quot;{C}hart{D}ialogs: {P}lotting from {N}atural {L}anguage {I}nstructions&amp;quot;,
    author = &amp;quot;Shao, Yutong  and
      Nakashole, Ndapa&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.328&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.328&amp;quot;,
    pages = &amp;quot;3559--3574&amp;quot;,
    abstract = &amp;quot;This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61{\%} plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)</title>
      <link>https://www.jkk.name/post/2020-09-03_checklist/</link>
      <pubDate>Thu, 03 Sep 2020 14:44:29 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-03_checklist/</guid>
      <description>&lt;p&gt;It is difficult to predict how well a model will work in the real world.
Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to.
This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories.
Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.&lt;/p&gt;
&lt;p&gt;The paper organises these tests along two axes.
One is the type of test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invariance: Giving the same answer when changes are made that should not impact the model prediction.&lt;/li&gt;
&lt;li&gt;Directional: Giving an answer that differs in a way that matches the intended impact of a change.&lt;/li&gt;
&lt;li&gt;Minimum Function Tests: A range of other tests that consider specific cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other axis is the linguistic property being varied:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vocabulary Change&lt;/li&gt;
&lt;li&gt;Named Entity Variation&lt;/li&gt;
&lt;li&gt;Temporal Shift&lt;/li&gt;
&lt;li&gt;Negation&lt;/li&gt;
&lt;li&gt;Semantic Role Swap&lt;/li&gt;
&lt;li&gt;Various Other Changes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, an invariance test on vocabulary would be that replacing words with their synonyms should not change the result.&lt;/p&gt;
&lt;p&gt;The paper tests the idea on (1) sentiment analysis on SST-2, (2) identifying matching questions on QQP, and (3) machien comprehension on SQuAD.
Researchers / developers using the method are more effective at finding issues than those asked to write tests without this framework to approach the problem.&lt;/p&gt;
&lt;p&gt;Understanding system errors has been an interest of mine for a long time now (back to my 2012 parsing paper) and from my experience with startups it is definitely challenging to develop effective tests for NLP models.
I&amp;rsquo;m curious to see how this approach works out when used iteratively.
When users modify their model or data to address the problems do they actually fix them or just overfit to the new set of tests?
Another open question is how to apply these to problems with more structured output (e.g. text-to-SQL).
Some would easily apply, e.g. invariance tests, while others would be more difficult.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.442/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{ribeiro-etal-2020-beyond,
    title = &amp;quot;Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist&amp;quot;,
    author = &amp;quot;Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.442&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.442&amp;quot;,
    pages = &amp;quot;4902--4912&amp;quot;,
    abstract = &amp;quot;Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)</title>
      <link>https://www.jkk.name/post/2019-07-10_disentanglement/</link>
      <pubDate>Wed, 10 Jul 2019 11:19:06 -0400</pubDate>
      <guid>https://www.jkk.name/post/2019-07-10_disentanglement/</guid>
      <description>&lt;p&gt;This post is about my own paper to appear at ACL later this month.
What is interesting about this paper will depend on your research interests, so that&amp;rsquo;s how I&amp;rsquo;ve broken down this blog post.&lt;/p&gt;
&lt;p&gt;A few key points first:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://jkk.name/irc-disentanglement/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data and code&lt;/a&gt; are available on Github.&lt;/li&gt;
&lt;li&gt;The 
&lt;a href=&#34;https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; is also available.&lt;/li&gt;
&lt;li&gt;The general-purpose span labeling and linking 
&lt;a href=&#34;https://jkk.name/slate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;annotation tool&lt;/a&gt; we used is also appearing at ACL.&lt;/li&gt;
&lt;li&gt;Check out 
&lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSTC 8 Track 2&lt;/a&gt;, which is based on this work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;you-study-discourse&#34;&gt;You study discourse&lt;/h3&gt;
&lt;p&gt;We investigated discourse structure when multiple conversations are occurring in the same stream of communication.
In our case, the stream is a technical support channel for Ubuntu on Internet Relay Chat (IRC).
We annotated each message with which message(s) it was a response to.
As far as we are aware, this is the first large-scale corpus with this kind of discourse structure in synchronous chat.
Here is an example from the data, with annotations marked by edges and colours:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/irc-disentanglement-example.png&#34; alt=&#34;IRC Disentanglement Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t frame the paper as being about reply-structure though.
Instead, we focus on a byproduct of these annotations - conversation disentanglement.
Given our graph of reply-structure, each connected component is a single conversation (as shown by each colour in the example).
The key prior work on the disentanglement problem is 
&lt;a href=&#34;https://aclanthology.org/P08-1095&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elsner and Charniak (2008)&lt;/a&gt;, who released the largest annotated resource for the task, with 2,500 messages manually separated into conversations.
We annotated their data with our annotation scheme and 75,000 additional messages.&lt;/p&gt;
&lt;p&gt;We built a set of simple models for predicting reply-structure and did some analysis of assumptions about discourse from prior disentanglement work, but there is certainly more scope for study here.
One direction would be to develop better models for this task.
Another would be to study patterns in the data to understand how people are able to follow the conversation.&lt;/p&gt;
&lt;h3 id=&#34;you-work-on-dialogue&#34;&gt;You work on dialogue&lt;/h3&gt;
&lt;p&gt;There has been a lot of work recently using the Ubuntu dataset from 
&lt;a href=&#34;https://github.com/rkadlec/ubuntu-ranking-dataset-creator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lowe et al., (2015)&lt;/a&gt;, which was produced by heuristically disentangling conversations from the same IRC channel we use.
Their work opened up a fantastic research opportunity by providing 930,000 conversations for training and evaluating dialogue systems.
However, they were unable to evaluate the quality of their conversations because they had no annotated data.&lt;/p&gt;
&lt;p&gt;Using our data, we found that only 20% of their conversations are a true prefix of a conversation (since their next utterance classification task cuts the conversation off part-way, being a true prefix is all that matters).
Many conversations are missing messages, and some have extra messages from other conversations.
Unsurprisingly, our trained model does better, producing conversations that are a true prefix 81% of the time.
We also noticed that their heuristic was incorrectly linking messages far apart in time.
This is not tested by our evaluation set, so we constructed this figure, which shows the problem is quite common:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/irc-disentanglement-comparison.png&#34; alt=&#34;IRC Disentanglement Comparison&#34;&gt;&lt;/p&gt;
&lt;p&gt;The purple results are based on the output of our model over the entire Ubuntu IRC logs.
That output is the basis of 
&lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSTC 8 Track 2&lt;/a&gt;.
Once the competition finishes (October 20th, 2019) we will release all of the conversations.&lt;/p&gt;
&lt;h3 id=&#34;you-am-interested-in-studying-online-communities&#34;&gt;You am interested in studying online communities&lt;/h3&gt;
&lt;p&gt;This is not my area of expertise, but our data and models could enable the exploration of interesting questions.
For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the structure of the community? By looking at who asks for help and who responds we could see patterns of behaviour.&lt;/li&gt;
&lt;li&gt;How does a community evolve over time? This data spans 15 years, during which there were many Ubuntu releases, Stackoverflow was created, other Ubuntu forums were created, etc. It seems likely that those events and more would be reflected in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It would be interesting to apply the model to other communities, but that would require additional in-domain data to get good results.
We have no plans to collect additional data at this stage, and for other channels there are copyright questions that might be difficult to resolve (the Ubuntu channels have an open access license).&lt;/p&gt;
&lt;h3 id=&#34;you-mainly-care-about-neural-network-architectures&#34;&gt;You mainly care about neural network architectures&lt;/h3&gt;
&lt;p&gt;We experimented with a bunch of ideas that didn&amp;rsquo;t improve performance, so our final model is very simple (a feedforward network with features representing the logs and sentences represented by averaging and max-pooling GloVe embeddings).
Maybe that means there is an opportunity for you to improve on our results with a fancy model?
One of our motivations for making such a large new resource was to make it possible to train sophisticated models.&lt;/p&gt;
&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;This project has been going since I started at Michigan as a postdoc funded by a grant from IBM.
The final paper is the result of collaboration with a large group of people from Michigan and IBM.
Thank you!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{acl19disentangle,
  author    = {Kummerfeld, Jonathan K. and Gouravajhala, Sai R. and Peper, Joseph and Athreya, Vignesh and Gunasekara, Chulaka and Ganhotra, Jatin and Patel, Siva Sankalp and Polymenakos, Lazaros and Lasecki, Walter S.},
  title     = {A Large-Scale Corpus for Conversation Disentanglement},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  location  = {Florence, Italy},
  month     = {July},
  year      = {2019},
  url       = {https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf},
  arxiv     = {https://arxiv.org/abs/1810.11118},
  software  = {https://jkk.name/irc-disentanglement},
  data      = {https://jkk.name/irc-disentanglement},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution (Chen et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-11-08_corefdata/</link>
      <pubDate>Thu, 08 Nov 2018 11:29:32 -0500</pubDate>
      <guid>https://www.jkk.name/post/2018-11-08_corefdata/</guid>
      <description>&lt;p&gt;The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset).
Some of these are discussed in my 
&lt;a href=&#34;https://www.jkk.name/publication/conll11coreference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoNLL Shared Task submission paper&lt;/a&gt;, the biggest being the choice to not annotate mentions that are not coreferent.
This paper describes a new dataset that has a different set of compromises, specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A broader definition of coreference (e.g. appositives are coreferent)&lt;/li&gt;
&lt;li&gt;All mentions annotated&lt;/li&gt;
&lt;li&gt;Different annotation methods for different subsets of the data (training data is double annotated and then adjudicated, while the development and test data is triple annotated, all pairs of annotations are adjudicated, then the outcomes are merged by voting)&lt;/li&gt;
&lt;li&gt;A variety of genres, but generally simpler language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dataset is 10x the size of OntoNotes and freely available, which is fantastic.
The source text is 2/3rds the RACE dataset (English reading comprehension exams from China), and 1/3rd scraped websites.
Measurements of annotator agreement suggest the annotations are not as consistent as OntoNotes, but still good enough to be a useful resource.
I do disagree with one aspect of the paper&amp;rsquo;s analysis - the results show a substantial gain in performance when providing gold mentions, suggesting to me that it remains an important challenge in coreference resolution.
I&amp;rsquo;m also curious whether my 
&lt;a href=&#34;https://www.jkk.name/publication/emnlp13analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coreference analysis tool&lt;/a&gt; would find different patterns in errors on this dataset compared to OntoNotes.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D18-1016&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://preschool-lab.github.io/PreCo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Chen:EMNLP:2018,
  author    = {Chen, Hong and Fan, Zhenhua and Lu, Hao and Yuille, Alan and Rong, Shu},
  title     = {PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  pages     = {172--181},
  location  = {Brussels, Belgium},
  url       = {https://aclanthology.org/D18-1016},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-11-09_framesdataset/</link>
      <pubDate>Thu, 09 Nov 2017 19:47:08 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-11-09_framesdataset/</guid>
      <description>&lt;p&gt;Another paper about a dataset of dialogues, but this time with structure.
Like the paper from yesterday, the aim is a dataset of task-oriented conversations, but with more complexity than prior work.
The difference is that this work includes a structured representation of the state of the conversation: frames.&lt;/p&gt;
&lt;p&gt;A frame is essentially a tuple describing a query, e.g. (Destination: Sydney, Origin: Ann Arbor, price: 1500 USD).
There are multiple frames in a dialogue (a departure from DSTC tasks), and utterances are labeled with dialogue acts that capture modifications to the frames as well as references to them.
This structure sounds fairly general, though the focus here was on vacation planning, where the user is buying a package.
The setup doesn&amp;rsquo;t maximise the potential complexity though, as there are a small number of set packages available, rather than the complex tradeoffs of flight+hotel combinations that exist in practise.
Looking at the example dialogues in the paper, it has complete sentences of some complexity.
One thing I&amp;rsquo;m still curious about is disagreements between annotators, as for the complete task the score was 0.62 +/- 5 (with dialogue acts being trickier than slot values, and no scores for frame references on their own).&lt;/p&gt;
&lt;p&gt;Comparing to the Stanford dataset this is smaller (11k vs. 1.4k), but has more turns per dialogue (11 vs. 15) and probably longer turns too, judging by the examples.
The tasks are completely different, but both come with small tables of information that are private to the two participants and required for almost every turn in the conversation.
Evaluating on both could be a great way to show the flexibility of a dialogue system, but the lack of frames for the Stanford data and the difficulty of running a human evaluation for this data limits the feasible types of multi-domain experiments.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/W17-5526&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{elasri-EtAl:2017:W17-55,
  author    = {El Asri, Layla  and  Schulz, Hannes  and  Sharma, Shikhar  and  Zumer, Jeremie  and  Harris, Justin  and  Fine, Emery  and  Mehrotra, Rahul  and  Suleman, Kaheer},
  title     = {Frames: a corpus for adding memory to goal-oriented dialogue systems},
  booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  month     = {August},
  year      = {2017},
  address   = {Saarbrucken, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {207--219},
  url       = {https://aclanthology.org/W17-5526}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
