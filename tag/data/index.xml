<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data | Jonathan K. Kummerfeld</title>
    <link>http://www.jkk.name/tag/data/</link>
      <atom:link href="http://www.jkk.name/tag/data/index.xml" rel="self" type="application/rss+xml" />
    <description>data</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Jonathan K. Kummerfeld</copyright><lastBuildDate>Thu, 03 Sep 2020 14:44:29 -0500</lastBuildDate>
    <image>
      <url>http://www.jkk.name/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>data</title>
      <link>http://www.jkk.name/tag/data/</link>
    </image>
    
    <item>
      <title>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)</title>
      <link>http://www.jkk.name/post/2020-09-03_checklist/</link>
      <pubDate>Thu, 03 Sep 2020 14:44:29 -0500</pubDate>
      <guid>http://www.jkk.name/post/2020-09-03_checklist/</guid>
      <description>&lt;p&gt;It is difficult to predict how well a model will work in the real world.
Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to.
This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories.
Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.&lt;/p&gt;
&lt;p&gt;The paper organises these tests along two axes.
One is the type of test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invariance: Giving the same answer when changes are made that should not impact the model prediction.&lt;/li&gt;
&lt;li&gt;Directional: Giving an answer that differs in a way that matches the intended impact of a change.&lt;/li&gt;
&lt;li&gt;Minimum Function Tests: A range of other tests that consider specific cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other axis is the linguistic property being varied:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vocabulary Change&lt;/li&gt;
&lt;li&gt;Named Entity Variation&lt;/li&gt;
&lt;li&gt;Temporal Shift&lt;/li&gt;
&lt;li&gt;Negation&lt;/li&gt;
&lt;li&gt;Semantic Role Swap&lt;/li&gt;
&lt;li&gt;Various Other Changes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, an invariance test on vocabulary would be that replacing words with their synonyms should not change the result.&lt;/p&gt;
&lt;p&gt;The paper tests the idea on (1) sentiment analysis on SST-2, (2) identifying matching questions on QQP, and (3) machien comprehension on SQuAD.
Researchers / developers using the method are more effective at finding issues than those asked to write tests without this framework to approach the problem.&lt;/p&gt;
&lt;p&gt;Understanding system errors has been an interest of mine for a long time now (back to my 2012 parsing paper) and from my experience with startups it is definitely challenging to develop effective tests for NLP models.
I&amp;rsquo;m curious to see how this approach works out when used iteratively.
When users modify their model or data to address the problems do they actually fix them or just overfit to the new set of tests?
Another open question is how to apply these to problems with more structured output (e.g. text-to-SQL).
Some would easily apply, e.g. invariance tests, while others would be more difficult.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.442/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{ribeiro-etal-2020-beyond,
    title = &amp;quot;Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist&amp;quot;,
    author = &amp;quot;Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://www.aclweb.org/anthology/2020.acl-main.442&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.442&amp;quot;,
    pages = &amp;quot;4902--4912&amp;quot;,
    abstract = &amp;quot;Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)</title>
      <link>http://www.jkk.name/post/2019-07-10_disentanglement/</link>
      <pubDate>Wed, 10 Jul 2019 11:19:06 -0400</pubDate>
      <guid>http://www.jkk.name/post/2019-07-10_disentanglement/</guid>
      <description>&lt;p&gt;This post is about my own paper to appear at ACL later this month.
What is interesting about this paper will depend on your research interests, so that&amp;rsquo;s how I&amp;rsquo;ve broken down this blog post.&lt;/p&gt;
&lt;p&gt;A few key points first:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://jkk.name/irc-disentanglement/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data and code&lt;/a&gt; are available on Github.&lt;/li&gt;
&lt;li&gt;The 
&lt;a href=&#34;https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; is also available.&lt;/li&gt;
&lt;li&gt;The general-purpose span labeling and linking 
&lt;a href=&#34;https://jkk.name/slate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;annotation tool&lt;/a&gt; we used is also appearing at ACL.&lt;/li&gt;
&lt;li&gt;Check out 
&lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSTC 8 Track 2&lt;/a&gt;, which is based on this work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;you-study-discourse&#34;&gt;You study discourse&lt;/h3&gt;
&lt;p&gt;We investigated discourse structure when multiple conversations are occurring in the same stream of communication.
In our case, the stream is a technical support channel for Ubuntu on Internet Relay Chat (IRC).
We annotated each message with which message(s) it was a response to.
As far as we are aware, this is the first large-scale corpus with this kind of discourse structure in synchronous chat.
Here is an example from the data, with annotations marked by edges and colours:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.jkk.name/img/post/irc-disentanglement-example.png&#34; alt=&#34;IRC Disentanglement Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t frame the paper as being about reply-structure though.
Instead, we focus on a byproduct of these annotations - conversation disentanglement.
Given our graph of reply-structure, each connected component is a single conversation (as shown by each colour in the example).
The key prior work on the disentanglement problem is 
&lt;a href=&#34;https://www.aclweb.org/anthology/P08-1095&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elsner and Charniak (2008)&lt;/a&gt;, who released the largest annotated resource for the task, with 2,500 messages manually separated into conversations.
We annotated their data with our annotation scheme and 75,000 additional messages.&lt;/p&gt;
&lt;p&gt;We built a set of simple models for predicting reply-structure and did some analysis of assumptions about discourse from prior disentanglement work, but there is certainly more scope for study here.
One direction would be to develop better models for this task.
Another would be to study patterns in the data to understand how people are able to follow the conversation.&lt;/p&gt;
&lt;h3 id=&#34;you-work-on-dialogue&#34;&gt;You work on dialogue&lt;/h3&gt;
&lt;p&gt;There has been a lot of work recently using the Ubuntu dataset from 
&lt;a href=&#34;https://github.com/rkadlec/ubuntu-ranking-dataset-creator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lowe et al., (2015)&lt;/a&gt;, which was produced by heuristically disentangling conversations from the same IRC channel we use.
Their work opened up a fantastic research opportunity by providing 930,000 conversations for training and evaluating dialogue systems.
However, they were unable to evaluate the quality of their conversations because they had no annotated data.&lt;/p&gt;
&lt;p&gt;Using our data, we found that only 20% of their conversations are a true prefix of a conversation (since their next utterance classification task cuts the conversation off part-way, being a true prefix is all that matters).
Many conversations are missing messages, and some have extra messages from other conversations.
Unsurprisingly, our trained model does better, producing conversations that are a true prefix 81% of the time.
We also noticed that their heuristic was incorrectly linking messages far apart in time.
This is not tested by our evaluation set, so we constructed this figure, which shows the problem is quite common:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.jkk.name/img/post/irc-disentanglement-comparison.png&#34; alt=&#34;IRC Disentanglement Comparison&#34;&gt;&lt;/p&gt;
&lt;p&gt;The purple results are based on the output of our model over the entire Ubuntu IRC logs.
That output is the basis of 
&lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSTC 8 Track 2&lt;/a&gt;.
Once the competition finishes (October 20th, 2019) we will release all of the conversations.&lt;/p&gt;
&lt;h3 id=&#34;you-am-interested-in-studying-online-communities&#34;&gt;You am interested in studying online communities&lt;/h3&gt;
&lt;p&gt;This is not my area of expertise, but our data and models could enable the exploration of interesting questions.
For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the structure of the community? By looking at who asks for help and who responds we could see patterns of behaviour.&lt;/li&gt;
&lt;li&gt;How does a community evolve over time? This data spans 15 years, during which there were many Ubuntu releases, Stackoverflow was created, other Ubuntu forums were created, etc. It seems likely that those events and more would be reflected in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It would be interesting to apply the model to other communities, but that would require additional in-domain data to get good results.
We have no plans to collect additional data at this stage, and for other channels there are copyright questions that might be difficult to resolve (the Ubuntu channels have an open access license).&lt;/p&gt;
&lt;h3 id=&#34;you-mainly-care-about-neural-network-architectures&#34;&gt;You mainly care about neural network architectures&lt;/h3&gt;
&lt;p&gt;We experimented with a bunch of ideas that didn&amp;rsquo;t improve performance, so our final model is very simple (a feedforward network with features representing the logs and sentences represented by averaging and max-pooling GloVe embeddings).
Maybe that means there is an opportunity for you to improve on our results with a fancy model?
One of our motivations for making such a large new resource was to make it possible to train sophisticated models.&lt;/p&gt;
&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;This project has been going since I started at Michigan as a postdoc funded by a grant from IBM.
The final paper is the result of collaboration with a large group of people from Michigan and IBM.
Thank you!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{acl19disentangle,
  author    = {Jonathan K. Kummerfeld and Sai R. Gouravajhala and Joseph Peper and Vignesh Athreya and Chulaka Gunasekara and Jatin Ganhotra and Siva Sankalp Patel and Lazaros Polymenakos and Walter S. Lasecki},
  title     = {A Large-Scale Corpus for Conversation Disentanglement},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  location  = {Florence, Italy},
  month     = {July},
  year      = {2019},
  url       = {https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf},
  arxiv     = {https://arxiv.org/abs/1810.11118},
  software  = {https://jkk.name/irc-disentanglement},
  data      = {https://jkk.name/irc-disentanglement},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution (Chen et al., 2018)</title>
      <link>http://www.jkk.name/post/2018-11-08_corefdata/</link>
      <pubDate>Thu, 08 Nov 2018 11:29:32 -0500</pubDate>
      <guid>http://www.jkk.name/post/2018-11-08_corefdata/</guid>
      <description>&lt;p&gt;The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset).
Some of these are discussed in my 
&lt;a href=&#34;http://jkk.name/publication/conll11coreference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoNLL Shared Task submission paper&lt;/a&gt;, the biggest being the choice to not annotate mentions that are not coreferent.
This paper describes a new dataset that has a different set of compromises, specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A broader definition of coreference (e.g. appositives are coreferent)&lt;/li&gt;
&lt;li&gt;All mentions annotated&lt;/li&gt;
&lt;li&gt;Different annotation methods for different subsets of the data (training data is double annotated and then adjudicated, while the development and test data is triple annotated, all pairs of annotations are adjudicated, then the outcomes are merged by voting)&lt;/li&gt;
&lt;li&gt;A variety of genres, but generally simpler language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dataset is 10x the size of OntoNotes and freely available, which is fantastic.
The source text is 2/3rds the RACE dataset (English reading comprehension exams from China), and 1/3rd scraped websites.
Measurements of annotator agreement suggest the annotations are not as consistent as OntoNotes, but still good enough to be a useful resource.
I do disagree with one aspect of the paper&amp;rsquo;s analysis - the results show a substantial gain in performance when providing gold mentions, suggesting to me that it remains an important challenge in coreference resolution.
I&amp;rsquo;m also curious whether my 
&lt;a href=&#34;http://jkk.name/publication/emnlp13analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coreference analysis tool&lt;/a&gt; would find different patterns in errors on this dataset compared to OntoNotes.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://aclweb.org/anthology/D18-1016&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://preschool-lab.github.io/PreCo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{Chen:EMNLP:2018,
  author    = {Chen, Hong and Fan, Zhenhua and Lu, Hao and Yuille, Alan and Rong, Shu},
  title     = {PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  pages     = {172--181},
  location  = {Brussels, Belgium},
  url       = {http://aclweb.org/anthology/D18-1016},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</title>
      <link>http://www.jkk.name/post/2017-11-09_framesdataset/</link>
      <pubDate>Thu, 09 Nov 2017 19:47:08 -0500</pubDate>
      <guid>http://www.jkk.name/post/2017-11-09_framesdataset/</guid>
      <description>&lt;p&gt;Another paper about a dataset of dialogues, but this time with structure.
Like the paper from yesterday, the aim is a dataset of task-oriented conversations, but with more complexity than prior work.
The difference is that this work includes a structured representation of the state of the conversation: frames.&lt;/p&gt;
&lt;p&gt;A frame is essentially a tuple describing a query, e.g. (Destination: Sydney, Origin: Ann Arbor, price: 1500 USD).
There are multiple frames in a dialogue (a departure from DSTC tasks), and utterances are labeled with dialogue acts that capture modifications to the frames as well as references to them.
This structure sounds fairly general, though the focus here was on vacation planning, where the user is buying a package.
The setup doesn&amp;rsquo;t maximise the potential complexity though, as there are a small number of set packages available, rather than the complex tradeoffs of flight+hotel combinations that exist in practise.
Looking at the example dialogues in the paper, it has complete sentences of some complexity.
One thing I&amp;rsquo;m still curious about is disagreements between annotators, as for the complete task the score was 0.62 +/- 5 (with dialogue acts being trickier than slot values, and no scores for frame references on their own).&lt;/p&gt;
&lt;p&gt;Comparing to the Stanford dataset this is smaller (11k vs. 1.4k), but has more turns per dialogue (11 vs. 15) and probably longer turns too, judging by the examples.
The tasks are completely different, but both come with small tables of information that are private to the two participants and required for almost every turn in the conversation.
Evaluating on both could be a great way to show the flexibility of a dialogue system, but the lack of frames for the Stanford data and the difficulty of running a human evaluation for this data limits the feasible types of multi-domain experiments.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://aclweb.org/anthology/W17-5526&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{elasri-EtAl:2017:W17-55,
  author    = {El Asri, Layla  and  Schulz, Hannes  and  Sharma, Shikhar  and  Zumer, Jeremie  and  Harris, Justin  and  Fine, Emery  and  Mehrotra, Rahul  and  Suleman, Kaheer},
  title     = {Frames: a corpus for adding memory to goal-oriented dialogue systems},
  booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  month     = {August},
  year      = {2017},
  address   = {Saarbrucken, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {207--219},
  url       = {http://aclweb.org/anthology/W17-5526}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
