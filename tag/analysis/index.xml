<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>analysis | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/analysis/</link>
      <atom:link href="https://www.jkk.name/tag/analysis/index.xml" rel="self" type="application/rss+xml" />
    <description>analysis</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Thu, 03 Sep 2020 14:44:29 -0500</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>analysis</title>
      <link>https://www.jkk.name/tag/analysis/</link>
    </image>
    
    <item>
      <title>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)</title>
      <link>https://www.jkk.name/post/2020-09-03_checklist/</link>
      <pubDate>Thu, 03 Sep 2020 14:44:29 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-03_checklist/</guid>
      <description>&lt;p&gt;It is difficult to predict how well a model will work in the real world.
Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to.
This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories.
Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.&lt;/p&gt;
&lt;p&gt;The paper organises these tests along two axes.
One is the type of test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invariance: Giving the same answer when changes are made that should not impact the model prediction.&lt;/li&gt;
&lt;li&gt;Directional: Giving an answer that differs in a way that matches the intended impact of a change.&lt;/li&gt;
&lt;li&gt;Minimum Function Tests: A range of other tests that consider specific cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other axis is the linguistic property being varied:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vocabulary Change&lt;/li&gt;
&lt;li&gt;Named Entity Variation&lt;/li&gt;
&lt;li&gt;Temporal Shift&lt;/li&gt;
&lt;li&gt;Negation&lt;/li&gt;
&lt;li&gt;Semantic Role Swap&lt;/li&gt;
&lt;li&gt;Various Other Changes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, an invariance test on vocabulary would be that replacing words with their synonyms should not change the result.&lt;/p&gt;
&lt;p&gt;The paper tests the idea on (1) sentiment analysis on SST-2, (2) identifying matching questions on QQP, and (3) machien comprehension on SQuAD.
Researchers / developers using the method are more effective at finding issues than those asked to write tests without this framework to approach the problem.&lt;/p&gt;
&lt;p&gt;Understanding system errors has been an interest of mine for a long time now (back to my 2012 parsing paper) and from my experience with startups it is definitely challenging to develop effective tests for NLP models.
I&amp;rsquo;m curious to see how this approach works out when used iteratively.
When users modify their model or data to address the problems do they actually fix them or just overfit to the new set of tests?
Another open question is how to apply these to problems with more structured output (e.g. text-to-SQL).
Some would easily apply, e.g. invariance tests, while others would be more difficult.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.442/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{ribeiro-etal-2020-beyond,
    title = &amp;quot;Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist&amp;quot;,
    author = &amp;quot;Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.442&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.442&amp;quot;,
    pages = &amp;quot;4902--4912&amp;quot;,
    abstract = &amp;quot;Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models (Weber et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-05-08_seq2seqsensitivity/</link>
      <pubDate>Tue, 08 May 2018 09:00:31 -0400</pubDate>
      <guid>https://www.jkk.name/post/2018-05-08_seq2seqsensitivity/</guid>
      <description>&lt;p&gt;We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models usually end up doing reasonably well.
This paper asks an important question - are those metrics measuring generalisability effectively?
In particular, if we sample our test set from a slightly different distribution of data, do models still work well?&lt;/p&gt;
&lt;p&gt;As a controlled set up they form a simple dataset as follows for each sentence:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go through the sentence left to right&lt;/li&gt;
&lt;li&gt;For each word generate three words in the output, where the output words are randomly sampled from a small vocabulary that is unique to each input word&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is clearly learnable and it seems reasonable that a sequence-to-sequence neural model with attention should be able to learn it.
Experiments show they do, getting close to 100% on a test set sampled the same way as the training set (input length 5-10, no symbol used twice).
However, if the test set is slightly different, with sequences of length 11-15, then results vary from 0% to 98% depending on the random seed in training (other variations also lead to large variations).
What this means is that sometimes the model is not learning to generalise.
They also show that the models that do generalise can only do so in one way (e.g. remain effective when length varies, or remain effective when symbols are used more than once in the input).&lt;/p&gt;
&lt;p&gt;A few takeaways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure your training and testing data are sampled from the distribution you are interested in&lt;/li&gt;
&lt;li&gt;More study of training data order and weight initialisation is needed (these are the two factors impacted by the random seed)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Incidentally, I am a co-author on an ACL paper that points out a similar issue for mapping text questions to SQL queries.
If we restrict the test set to be novel queries (i.e. the model has to generalise) performance falls through the floor.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1805.01445&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{Weber:2018:GenDeep,
  author    = {Weber, Noah and Shekhar, Leena and Balasubramanian, Niranjan},
    title = {The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models},
  journal = {Workshop on New Forms of Generalization in Deep Learning and NLP (NAACL 2018)},
     year = {2018},
      url = {https://arxiv.org/abs/1805.01445},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-05_explainingpredictions/</link>
      <pubDate>Tue, 05 Dec 2017 15:40:45 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-05_explainingpredictions/</guid>
      <description>&lt;p&gt;Interpreting the behaviour of statistical models in NLP has been hard for a long time, but it has gotten even harder with nonlinear models.
The simplest method so far in NLP has been to look at the attention distributions in sequence to sequence models, but that doesn&amp;rsquo;t provide everything we need and obviously only applies when the model has attention.
For looking at the dynamics of the hidden state in an LSTM the Harvard NLP group built a cool 
&lt;a href=&#34;http://lstm.seas.harvard.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;visualisation&lt;/a&gt;, but what about structured outputs?&lt;/p&gt;
&lt;p&gt;This paper considers sequence to sequence models and determines which parts of the input were most important for determining each part of the output.
The steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use a variational autoencoder to get perturbed versions of the input&lt;/li&gt;
&lt;li&gt;Use logistic regression to get scores for every output symbol indicating how sensitive it is to variations in parts of the input&lt;/li&gt;
&lt;li&gt;Create a bipartite graph between inputs and outputs, then find high weight components in the graph&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These components serve as the representation of which parts of the input determine which parts of the output.
Experiments show results that match with past observations and intuitions, which is good for supporting the effectiveness of the method, but it&amp;rsquo;s a shame this didn&amp;rsquo;t uncover any exciting new patterns.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1042&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{alvarezmelis-jaakkola:2017:EMNLP2017,
  author    = {Alvarez-Melis, David  and  Jaakkola, Tommi},
  title     = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {412--421},
  url       = {https://aclanthology.org/D17-1042}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
