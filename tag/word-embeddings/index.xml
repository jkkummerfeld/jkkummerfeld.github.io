<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>word-embeddings | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/word-embeddings/</link>
      <atom:link href="https://www.jkk.name/tag/word-embeddings/index.xml" rel="self" type="application/rss+xml" />
    <description>word-embeddings</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Sat, 10 Oct 2020 20:25:11 -0500</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>word-embeddings</title>
      <link>https://www.jkk.name/tag/word-embeddings/</link>
    </image>
    
    <item>
      <title>Compositional Demographic Word Embeddings (Welch et al., EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-10_demographicembeddings/</link>
      <pubDate>Sat, 10 Oct 2020 20:25:11 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-10_demographicembeddings/</guid>
      <description>&lt;p&gt;Most work in NLP uses datasets with a diverse set of speakers.
In practise, everyone speaks / writes slightly differently and our models would be better if they accounted for that.
This has been the motivation for a line of work by 
&lt;a href=&#34;http://cfwelch.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Charlie Welch&lt;/a&gt; that I&amp;rsquo;ve been a collaborator on (in

&lt;a href=&#34;https://www.jkk.name/publication/cicling19personal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CICLing 2019&lt;/a&gt;,

&lt;a href=&#34;https://www.jkk.name/publication/ieee19personal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Intelligent Systems 2019&lt;/a&gt;,

&lt;a href=&#34;https://www.jkk.name/publication/coling20personal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoLing 2020&lt;/a&gt;,
and this paper).&lt;/p&gt;
&lt;p&gt;Here, the question is how to improve language modeling for a new user of a service who voluntarily provided some demographic information, but you have no other data for.
Our solution is a language model that (1) has a separate word embedding space for each individual demographic value, and (2) forms a word embedding for a given user by composing the embeddings for their demographics.
In experiments on Reddit, this leads to improvements in performance for all demographic groups.&lt;/p&gt;
&lt;p&gt;In the process, we also developed a way to extract demographics of Reddit users.
Prior work has either inferred demographics or looked at flairs (labels in user profiles).
We use self-reported information in posts, such as &amp;ldquo;I am a [blah]&amp;rdquo;.
We use simple regular expressions, which are enough to get two or more demographic values for 61,000 users.
There is also relatively little overlap with a flair based method (less than 0.5% of ours are in a set based on flairs).&lt;/p&gt;
&lt;p&gt;It is important to note that a range of ethical issues exist around the use of demographics in machine learning.
We discuss a range of issues in the paper, but I also wanted to mention a few here.
First, to collect our data, we identified self-reported demographics in Reddit text.
This avoids some of the problems with inferring demographics, but it does mean our sample is biased (it only contains people who wish to publicly share demographics online).
Second, we must consider how our work may be used.
There is a potential positive (improved performance for specific groups), but also the risk that in order to use our ideas developers require users to disclose information or try to infer it automatically.
Third, there is the risk that our work is interpreted as implying that how someone speaks is a consequence of their demographics.
For more detailed discussion of these and other issues see the &amp;ldquo;Limitations and Ethical Considerations&amp;rdquo; section of the paper.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2010.02986.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp20demographics,
  title     = {Compositional Demographic Word Embeddings},
  author    = {Welch, Charles and Kummerfeld, Jonathan K. and P{\&#39;e}rez-Rosas, Ver{\&#39;o}nica and Mihalcea, Rada},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://arxiv.org/pdf/2010.02986.pdf},
  abstract  = {Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-09-29_pretraininglm/</link>
      <pubDate>Tue, 29 Sep 2020 13:38:24 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-29_pretraininglm/</guid>
      <description>&lt;p&gt;This paper explores two questions.
First, what is the impact of a few key design decisions for word embeddings in language models?
Second, based on the first answer, how can we improve results in the situation where we have 10 million+ words of text, but only 1 GPU for training?&lt;/p&gt;
&lt;h2 id=&#34;the-impact-of-tying-freezing-and-pretraining&#34;&gt;The impact of tying, freezing, and pretraining&lt;/h2&gt;
&lt;p&gt;It is standard practise to tie the input and output embeddings of language models (i.e., use the same weights in both places), training them together and initialising them randomly.
Several papers have shown that this improves results by providing more frequent updates to the input embeddings.
But if you have data available for pretraining it is less clear that this is the right approach.
To explore this I&amp;rsquo;m going to use a few symbols:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/pretraining-lm-vary-key.jpg&#34; alt=&#34;Key for Variations on LM table&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here are the results of training an 
&lt;a href=&#34;https://github.com/salesforce/awd-lstm-lm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWD-LSTM&lt;/a&gt; with all variations of these parameters, evaluated on the standard LM development set of the PTB (Std) and a variation that has actual words instead of unk (Rare):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/pretraining-lm-vary.jpg&#34; alt=&#34;Variations on LM (for written form of table see bottom of page)&#34;&gt;&lt;/p&gt;
&lt;p&gt;Light blue shows the standard configuration and light red shows our proposal.
The table is ranked by performance on Std and has four clear sections:&lt;/p&gt;
&lt;p&gt;(a) Frozen random output embeddings.&lt;/p&gt;
&lt;p&gt;(b) Frozen pretrained output embeddings.&lt;/p&gt;
&lt;p&gt;(c) Frozen random input embeddings.&lt;/p&gt;
&lt;p&gt;(d) Various configurations.&lt;/p&gt;
&lt;p&gt;I was surprised by the dramatic difference between input and output embeddings here.
Freezing the output embeddings, even with a good embedding space, leads to terrible performance.
In contrast, freezing input embeddings is fine if they are pretrained, and has a far smaller impact when they are random.&lt;/p&gt;
&lt;p&gt;Evaluating with rare words, the big picture is mostly the same, but pretraining has a bigger impact.
One interesting difference is that the top five models all use pretrained input embeddings, with a large gap from there to the next results.
At the same time, pretraining the output embeddings seems to have only a small impact (when holding all other variables fixed).
Finally, the best results freeze the input embeddings.
Our explanation is that embeddings become inconsistent when they aren&amp;rsquo;t frozen.
The vectors for words in the training set are moved but the ones seen only in pretraining stay where they are, leading to an inconsistent embedding space.&lt;/p&gt;
&lt;p&gt;The paper then goes through a series of experiments to explore this, varying data domain, similarity of pretraining data, and more.
Here I&amp;rsquo;m going to jump straight to the final results.
The table below considers a dataset with 43 million in-domain tokens for pretraining and 7 million for LM training.
The other models are the standard AWD-LSTM, an n-gram language model, and two version of GPT-2 (without finetuning):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jkk.name/img/post/pretraining-final.jpg&#34; alt=&#34;Final results (for written form of table see bottom of page)&#34;&gt;&lt;/p&gt;
&lt;p&gt;For word level prediction perplexity is reduced by 4.
However, if we train and test with BPE there is no improvement (see the 
&lt;a href=&#34;https://arxiv.org/abs/1911.11423&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SHA-RNN paper&lt;/a&gt; for some issues with comparing BPE and word evaluation).
So if your application works with BPE this finding isn&amp;rsquo;t useful, but for word-level modeling it probably is.&lt;/p&gt;
&lt;p&gt;A few notes about this work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A natural next step would be to explore ways to train the language model with more data.
Modifying the AWD-LSTM code to support training sets larger than GPU memory could render pretraining unnecessary (though at the cost of much longer training).
In some experiments (not in the paper), we found that when the pretraining set and training set were the same, pretraining didn&amp;rsquo;t improve performance, but it did speed up training.&lt;/li&gt;
&lt;li&gt;Properties of evaluation datasets have shaped the direction of work on language modeling.
It&amp;rsquo;s important to think beyond the hyperparameters that are easy to vary (e.g., hidden vector dimensions) when adapting a model for a new scenario.&lt;/li&gt;
&lt;li&gt;Writing robust research code is hard.
We tried getting several other models to run with our variations, but going beyond reproducing results to actually modifying code proved hard.
Even for the AWD-LSTM, we failed to reproduce results except when we went back to one of the earliest releases.&lt;/li&gt;
&lt;li&gt;This paper was saved by author response.
The initial reviews were 3.5, 2.5, 3.5 and based on the response and reviewer discussion the 2.5 went to a 4.
The response contained answers to reviewer questions, including a bunch of statistics about the data that are now in the final paper.
I have always been a fan of author response.
It can lead to more informed acceptance decisions and more useful feedback to authors.
To achieve that, both authors and reviewers need to engage with it though.
In particular, reviewers need to give something of substance to be responded to and they need to carefully read and consider the response.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp20lm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp20lm,
  title     = {Improving Low Compute Language Modeling with In-Domain Embedding Initialisation},
  author    = {Welch, Charles and Mihalcea, Rada and Kummerfeld, Jonathan K.},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2020},
  url       = {https://www.jkk.name/pub/emnlp20lm.pdf},
  abstract  = {Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;tables-in-written-form&#34;&gt;Tables in written form&lt;/h2&gt;
&lt;h3 id=&#34;table-with-training-variations&#34;&gt;Table with training variations&lt;/h3&gt;
&lt;p&gt;Each section is presented separately below, with the model described using five words followed by the result on the standard data and the result on the data with rare words.&lt;/p&gt;
&lt;p&gt;First section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tied   frozen   dice  frozen   dice, 680, 1120&lt;/li&gt;
&lt;li&gt;untied frozen   dice  frozen   dice, 680, 1120&lt;/li&gt;
&lt;li&gt;untied unfrozen dice  frozen   dice, 680, 431&lt;/li&gt;
&lt;li&gt;untied unfrozen train frozen   dice, 220, 372&lt;/li&gt;
&lt;li&gt;untied frozen   train frozen   dice, 218, 360&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Second section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;untied frozen   dice  frozen   train, 121, 202&lt;/li&gt;
&lt;li&gt;untied unfrozen dice  frozen   train, 95.0, 170&lt;/li&gt;
&lt;li&gt;untied unfrozen train frozen   train, 91.3, 147&lt;/li&gt;
&lt;li&gt;tied   frozen   train frozen   train, 90.7, 136&lt;/li&gt;
&lt;li&gt;untied frozen   train frozen   train, 90.7, 136&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Third section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;untied frozen   dice  unfrozen dice, 82.2, 143&lt;/li&gt;
&lt;li&gt;untied frozen   dice  unfrozen train, 81.4, 142&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fourth section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;untied unfrozen dice  unfrozen dice, 65.3, 120&lt;/li&gt;
&lt;li&gt;untied unfrozen dice  unfrozen train, 64.1, 113&lt;/li&gt;
&lt;li&gt;untied unfrozen train unfrozen dice, 62.5, 105&lt;/li&gt;
&lt;li&gt;untied unfrozen train unfrozen train, 61.7, 98.5&lt;/li&gt;
&lt;li&gt;untied frozen   train unfrozen train, 61.6, 97.1&lt;/li&gt;
&lt;li&gt;tied   unfrozen dice  unfrozen dice, 61.3, 112&lt;/li&gt;
&lt;li&gt;untied frozen   train unfrozen dice, 61.1, 98.1&lt;/li&gt;
&lt;li&gt;tied   unfrozen train unfrozen train, 59.8, 98.7&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;final-results-table&#34;&gt;Final results table&lt;/h3&gt;
&lt;p&gt;Models with word level evaluation, giving development results then test results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N-Gram, 92.3, 95.0&lt;/li&gt;
&lt;li&gt;Baseline AWD-LSTM, 52.8, 53.5&lt;/li&gt;
&lt;li&gt;Our approach, 49.0, 49.4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Models with BPE evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N-Gram, 56.7, 55.3&lt;/li&gt;
&lt;li&gt;GPT-2 (112m), 46.4, 43.8&lt;/li&gt;
&lt;li&gt;Baseline AWD-LSTM, 37.8, 36.7&lt;/li&gt;
&lt;li&gt;Our approach, 38.3, 37.2&lt;/li&gt;
&lt;li&gt;GPT-2 (774m), 32.5, 33.7&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Dice Icon by Andrew Doane from the Noun Project.
Fire and Snowflake Icons by Freepik from &lt;a href=&#34;http://www.flaticon.com&#34;&gt;www.flaticon.com&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)</title>
      <link>https://www.jkk.name/post/2017-12-07_rarewordvectors/</link>
      <pubDate>Thu, 07 Dec 2017 20:45:39 -0500</pubDate>
      <guid>https://www.jkk.name/post/2017-12-07_rarewordvectors/</guid>
      <description>&lt;p&gt;Word vectors are great for common words, but what about rare words?
People can have a fairly good understanding of a word given only a few instances, but it&amp;rsquo;s fairly standard to turn all words with a frequency of less than 5 into UNK when learning word vectors.&lt;/p&gt;
&lt;p&gt;One simple approach is to add up word vectors from the context of the rare word and use that as the representation.
This paper proposes using a tweaked version of word2vec: keep vectors for frequent words fixed, increase the learning rate, use a fixed width context window, initialise with the additive approach, and only subsample by discarding frequent words.
All of those make sense, though I am curious whether it would be better to just decrease subsampling or disable it entirely.&lt;/p&gt;
&lt;p&gt;The results are mixed, with the improvement over the additive approach data dependent.
That might partly reflect the tasks though - something downstream like POS tagging would have been interesting, particularly since the LSTM may already be capturing contextual information that covers what the additive approach has, but not what this adds.
Ultimately this is not a solution to this problem, but it&amp;rsquo;s an idea to keep in mind.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/D17-1030&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{herbelot-baroni:2017:EMNLP2017,
  author    = {Herbelot, Aur\&#39;{e}lie  and  Baroni, Marco},
  title     = {High-risk learning: acquiring new word vectors from tiny data},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {304--309},
  url       = {https://aclanthology.org/D17-1030}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
