<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>language-model | Jonathan K. Kummerfeld</title>
    <link>http://www.jkk.name/tag/language-model/</link>
      <atom:link href="http://www.jkk.name/tag/language-model/index.xml" rel="self" type="application/rss+xml" />
    <description>language-model</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Jonathan K. Kummerfeld</copyright><lastBuildDate>Mon, 16 Apr 2018 20:55:22 -0400</lastBuildDate>
    <image>
      <url>http://www.jkk.name/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>language-model</title>
      <link>http://www.jkk.name/tag/language-model/</link>
    </image>
    
    <item>
      <title>An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)</title>
      <link>http://www.jkk.name/post/2018-04-16_lm_analysis/</link>
      <pubDate>Mon, 16 Apr 2018 20:55:22 -0400</pubDate>
      <guid>http://www.jkk.name/post/2018-04-16_lm_analysis/</guid>
      <description>&lt;p&gt;Assigning a probability distribution over the next word or character in a sequence (language modeling) is a useful component of many systems, such as speech recognition and translation.
Recently neural networks have come to dominate in performance, with a range of clever innovations in network structure.
This paper is not about new models, but rather explores the current evaluation and how well carefully tuned baseline models can do.&lt;/p&gt;
&lt;p&gt;The key observations for me were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are issues with the PTB dataset for character-level evaluation - it removes all punctuation, makes numbers &amp;lsquo;N&amp;rsquo;, and removes rare words (i.e. it is a character-level version of the token-level task).
Given that the original Penn Treebank exists, I would have been interested to see a comparison with the PTB without any simplification.
The other dataset, enwik8, makes sense as a testing ground for compression algorithms, but is a little odd for modeling language, since it is the first 100 million bytes of a Wikipedia XML dump.
The paper does have another dataset, WikiText, which sounds good, but then there is no character-level evaluation!&lt;/li&gt;
&lt;li&gt;The LSTM is able to achieve ~SotA results for character-level modeling.
The key seems to be careful design of the softmax that produces the final probability distribution:
(1) rare words are clustered and represented by a single value in the distribution calculation, and
(2) word vectors are shared between input and output.&lt;/li&gt;
&lt;li&gt;Dropout matters more than the network design, and multiple forms of dropout should be tuned jointly.
This comes from analysis of a set of models trained with random variation in hyperparameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1803.08240&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{2018arXiv180308240M,
   author = {{Merity}, S. and {Shirish Keskar}, N. and {Socher}, R.},
    title = {An Analysis of Neural Language Modeling at Multiple Scales},
  journal = {ArXiv e-prints},
     year = {2018},
      url = {https://arxiv.org/abs/1803.08240},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</title>
      <link>http://www.jkk.name/post/2017-10-30_neuralsequence/</link>
      <pubDate>Mon, 30 Oct 2017 13:28:30 -0400</pubDate>
      <guid>http://www.jkk.name/post/2017-10-30_neuralsequence/</guid>
      <description>&lt;p&gt;Language is bursty, with rare words occurring in clumps, the simplest example being an unusual name that occurs a lot in one news article, but not in other articles.
This paper is about how to modify a neural language model to take this into consideration, by adapting the model over time.&lt;/p&gt;
&lt;p&gt;The main idea is to have one model of overall word usage (global) and a separate model that shifts over time to take into consideration the current text (local).
The idea of adapting is not new (as the paper makes clear), but the key here is an update rule that is a modified form of RMSprop, combining the local and global models.
It also seems like performing the updates after every 5 words is important, balancing frequency with informativeness (though no ablation of frequencies is presented).
Conveniently, this is orthogonal to many other ideas and can essentially be stapled on top of a range of sequential architectures, consistently leading to improvements.&lt;/p&gt;
&lt;p&gt;One question left open is how this would work in generation.
The paper describes how it could be applied and could provide improvements, but it also seems likely to risk the repetitive outputs seen in many dialogue systems.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1709.07432.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@ARTICLE{2017arXiv170907432K,
   author = {{Krause}, B. and {Kahembwe}, E. and {Murray}, I. and {Renals}, S.},
    title = &amp;quot;{Dynamic Evaluation of Neural Sequence Models}&amp;quot;,
  journal = {ArXiv e-prints},
archivePrefix = &amp;quot;arXiv&amp;quot;,
   eprint = {1709.07432},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
     year = 2017,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170907432K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
