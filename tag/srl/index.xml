<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>srl | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/srl/</link>
      <atom:link href="https://www.jkk.name/tag/srl/index.xml" rel="self" type="application/rss+xml" />
    <description>srl</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Sun, 04 Oct 2020 15:10:12 -0500</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>srl</title>
      <link>https://www.jkk.name/tag/srl/</link>
    </image>
    
    <item>
      <title>A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-04_crowdsrl/</link>
      <pubDate>Sun, 04 Oct 2020 15:10:12 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-04_crowdsrl/</guid>
      <description>&lt;p&gt;My 
&lt;a href=&#34;https://www.jkk.name/post/2020-09-25_crowdqasrl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions.
This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.&lt;/p&gt;
&lt;p&gt;The core new idea is a filtering process in which workers identify &lt;em&gt;incorrect&lt;/em&gt; answers for a task.
This is the first step of a three stage process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Five workers iteratively filter the options for a label (either for a predicate or argument) until there are only three.&lt;/li&gt;
&lt;li&gt;Five workers select the correct answer.&lt;/li&gt;
&lt;li&gt;If the workers disagree or any of them indicates uncertainty, ask an expert to annotate the example.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To make this work, we use an automatic system for identifying spans for predicates and arguments.
This is better than going straight to the second step because it makes the set of labels less overwhelming, focusing effort on the subtle distinctions between the options.&lt;/p&gt;
&lt;p&gt;This mixture of crowd and expert effort achieves high accuracy (94%) while only having 12% of examples annotated by experts.
The cost is about 52 cents per label for crowd work plus the cost of the expert.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a little tricky to compare the cost with prior work.
Comparing to other work on SRL, we spend more on the crowd, but less on experts.
Whether that trade-off is worth it will depend on the cost of experts.
In practise, our experts are often members of the research team and so their time is a stronger constraint than the crowdsourcing budget.
In that case, our approach comes out ahead as we can get more data annotated per unit of expert effort (by a factor of four).
The QA-SRL work is quite a bit cheaper, at 54 cents per predicate with 2.9 roles on average (which would be ~$2 + expert effort for our approach), but the type of annotations collected are quite different, with ours providing labels from the sense inventory in Propbank.&lt;/p&gt;
&lt;p&gt;I see a range of interesting potential improvements for future work.
First, bringing in methods of worker training in order to improve their accuracy and so reduce the need for duplicate effort.
Second, combining with ideas from other work, such as having a model deciding whether examples are easy or hard and changing how they are processed accordingly, or using QA-SRL annotation to inform the process.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp-findings20srl.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1314632048070594560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My Tweet&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp-findings20srl,
  title     = {A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels},
  author    = {Jiang, Youxuan and Zhu, Huaiyu and Kummerfeld, Jonathan K. and Li, Yunyao and Lasecki, Walter},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  shortvenue = {Findings of EMNLP},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://www.jkk.name/pub/emnlp-findings20srl.pdf},
  abstract  = {Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95\% accuracy for predicate labels and 93\% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56\% to 14\% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)</title>
      <link>https://www.jkk.name/post/2020-09-25_crowdqasrl/</link>
      <pubDate>Fri, 25 Sep 2020 10:17:18 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-25_crowdqasrl/</guid>
      <description>&lt;p&gt;Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments.
Over the last few years, 
&lt;a href=&#34;https://www.cs.washington.edu/people/faculty/lsz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luke Zettlemoyer&amp;rsquo;s Group&lt;/a&gt; has been exploring using question-answer pairs to represent this structure.
This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank.
However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.&lt;/p&gt;
&lt;p&gt;I got three main things out of this paper:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It shifted my approach to crowdsourcing to consider workers more like traditional expert annotators.&lt;/li&gt;
&lt;li&gt;It reinforced the idea that small shifts in crowd workflows can have a major impact on annotation quality.&lt;/li&gt;
&lt;li&gt;QA-SRL can capture roles not covered by PropBank.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The work also provides a new dataset that will be useful for future work on this problem, and useful benchmarks of systems and measurements of data quality.
Expanding on the three points above:&lt;/p&gt;
&lt;p&gt;Crowd workers: The paper argues in favour of putting more time into training workers.
Most of the work I&amp;rsquo;ve seen in NLP for crowdsourcing (including my own) focuses on modifying task design or using ML post-processing to improve results.
Here, they run a large-scale qualification task and filter workers based on their performance, then train those workers by paying them to read a set of instructions (23 text-dense slides) and do two small annotation rounds with feedback after each one.
This increases the upfront cost, but reduces the cost of annotation by reducing the need for multiple annotations of each item.
The paper doesn&amp;rsquo;t provide quite enough detail to quantify the cost.
We do know that to get to 11 workers they needed to train 30 workers at a cost of 2 hours each plus 30 minutes of researcher time each.
If we assume 60 workers did the preliminary round, each taking 5 minutes, and that workers cost &lt;span&gt;$&lt;/span&gt;12 / hour (&lt;span&gt;$&lt;/span&gt;10 to the workers, &lt;span&gt;$&lt;/span&gt;2 to Amazon), that&amp;rsquo;s almost &lt;span&gt;$&lt;/span&gt;800 plus 15 hours of researcher time.
For a large annotation effort, the savings during annotation will make that worth it (or, as in this case, it will lead to higher quality data).
I am curious which aspect was more important though - filtering the pool of workers, or training workers.&lt;/p&gt;
&lt;p&gt;Workflow impact: In previous QA-SRL work, one worker wrote a question and its answers and two workers checked the question and independently added answers.
Here, two workers independently write a question+answer and a third work consolidates the annotations into a final annotation.
The cost for a label is about the same (54c / predicate vs. 51c / predicate), but coverage is considerably higher.
The design space for crowd workflows is huge and this is another example of how important it is to explore.
It&amp;rsquo;s also possible that the changes in recruitment and training were more critical than the workflow shift, but the study didn&amp;rsquo;t include evaluation with only one or the other.&lt;/p&gt;
&lt;p&gt;QA-SRL vs. PropBank: This may be less surprising to someone who works more on SRL, but they found their approach captured many implicit roles that PropBank does not.
Specifically, of 100 annotated arguments that were not in PropBank, 68 were valid implicit arguments.
I&amp;rsquo;m curious about what those implicit arguments are capturing.
Maybe targeted re-annotation could be used to add them to PropBank (identifying relevant sentences by trace parsing).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://aclanthology.org/2020.acl-main.626/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/plroit/qasrl-gs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1309592830537543681?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My Tweet&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{roit-etal-2020-controlled,
    title = &amp;quot;Controlled Crowdsourcing for High-Quality {QA}-{SRL} Annotation&amp;quot;,
    author = &amp;quot;Roit, Paul  and
      Klein, Ayal  and
      Stepanov, Daniela  and
      Mamou, Jonathan  and
      Michael, Julian  and
      Stanovsky, Gabriel  and
      Zettlemoyer, Luke  and
      Dagan, Ido&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2020.acl-main.626&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.626&amp;quot;,
    pages = &amp;quot;7008--7013&amp;quot;,
    abstract = &amp;quot;Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
