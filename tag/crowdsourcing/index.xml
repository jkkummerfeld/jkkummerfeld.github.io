<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>crowdsourcing | Jonathan K. Kummerfeld</title>
    <link>https://www.jkk.name/tag/crowdsourcing/</link>
      <atom:link href="https://www.jkk.name/tag/crowdsourcing/index.xml" rel="self" type="application/rss+xml" />
    <description>crowdsourcing</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Jonathan K. Kummerfeld</copyright><lastBuildDate>Mon, 12 Oct 2020 10:28:13 -0500</lastBuildDate>
    <image>
      <url>https://www.jkk.name/images/icon_hua0d6326cebee282657b97e45398e76a5_17367_512x512_fill_lanczos_center_2.png</url>
      <title>crowdsourcing</title>
      <link>https://www.jkk.name/tag/crowdsourcing/</link>
    </image>
    
    <item>
      <title>Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-12_taboo/</link>
      <pubDate>Mon, 12 Oct 2020 10:28:13 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-12_taboo/</guid>
      <description>&lt;p&gt;When we crowdsource data for tasks like SRL and sentiment analysis we only care about accuracy.
For tasks where workers write new content, such as paraphrasing and creating questions, we also care about data diversity.
If our data is not diverse then models trained on it will not be robust in the real world.
The core idea of this paper is to encourage creativity by constraining workers.&lt;/p&gt;
&lt;p&gt;We use three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Collect some data.&lt;/li&gt;
&lt;li&gt;Create a taboo list of words / phrases based on the data collected.&lt;/li&gt;
&lt;li&gt;Return to step 1, but tell workers they can&amp;rsquo;t use things in the taboo list.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We explored this idea for task-oriented dialogue.
We identified taboo words using an SVM with a bag-of-words to identify common words associated with specific intents or slot values.
Depending on the taboo word, we got quite different paraphrases.
For example, for the sentence &amp;ldquo;What is the capital of Florida?&amp;rdquo; we collected paraphrases with various taboo words:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Taboo Word&lt;/th&gt;
&lt;th&gt;Paraphrases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;what city is the state capital of florida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;florida&lt;/td&gt;
&lt;td&gt;what is the capital of the sunshine state&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capital&lt;/td&gt;
&lt;td&gt;where is the seat of government in florida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;what&lt;/td&gt;
&lt;td&gt;tell me the name of florida&amp;rsquo;s capital&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These examples show interesting variations, but to see if the variations are significant we tried collecting new test sets for five intent classification datasets and four slot filling datasets.
With just two taboo words, a BERT based model trained on the original dataset did considerably worse on our new data.
The drop varied from 2 to 33 points, with a median of 9.
This indicates that we are capturing ways of expressing these intents that are not well covered by the original data.&lt;/p&gt;
&lt;p&gt;Addressing this issue is simple - train on data collected with our method!
Interestingly, this approach is complementary to the outlier-based approach from 
&lt;a href=&#34;https://www.jkk.name/publication/naacl19outliers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;our NAACL 2019 paper&lt;/a&gt;.
Examples collected using one approach are hard for models trained on data from the other.
Fortunately, training with data from a mix of the two leads to strong results on both.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m particularly excited about this work because the general idea could be applied in so many ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change the task.&lt;/li&gt;
&lt;li&gt;Vary the type of taboo item (e.g. phrases instead of words).&lt;/li&gt;
&lt;li&gt;Try other ways of selecting taboo items.&lt;/li&gt;
&lt;li&gt;Use a different mapping from taboo items to tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, more specific versions of this idea have already been used.

&lt;a href=&#34;https://dl.acm.org/doi/10.1145/985692.985733&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luis von Ahn&amp;rsquo;s ESP game&lt;/a&gt; for image captioning used a taboo list.
Each image had a list of complete labels previously assigned to the image.
New labels could not match an existing label.
That work predates this paper by 16 years, but hasn&amp;rsquo;t had much traction in the NLP community, possibly because of the limitation of requiring complete matches on labels (making it impractical for sentences or even phrases).
I&amp;rsquo;m hopeful that our more general version will be useful in a range of crowdsourcing efforts.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp20taboo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp20taboo,
  title     = {Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness},
  author    = {Larson, Stefan and Zheng, Anthony and Mahendran, Anish and Tekriwal, Rishi and Cheung, Adrian and Guldan, Eric and Leach, Kevin and Kummerfeld, Jonathan K.},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://www.jkk.name/pub/emnlp20taboo.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)</title>
      <link>https://www.jkk.name/post/2020-10-04_crowdsrl/</link>
      <pubDate>Sun, 04 Oct 2020 15:10:12 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-10-04_crowdsrl/</guid>
      <description>&lt;p&gt;My 
&lt;a href=&#34;https://www.jkk.name/post/2020-09-25_crowdqasrl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions.
This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.&lt;/p&gt;
&lt;p&gt;The core new idea is a filtering process in which workers identify &lt;em&gt;incorrect&lt;/em&gt; answers for a task.
This is the first step of a three stage process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Five workers iteratively filter the options for a label (either for a predicate or argument) until there are only three.&lt;/li&gt;
&lt;li&gt;Five workers select the correct answer.&lt;/li&gt;
&lt;li&gt;If the workers disagree or any of them indicates uncertainty, ask an expert to annotate the example.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To make this work, we use an automatic system for identifying spans for predicates and arguments.
This is better than going straight to the second step because it makes the set of labels less overwhelming, focusing effort on the subtle distinctions between the options.&lt;/p&gt;
&lt;p&gt;This mixture of crowd and expert effort achieves high accuracy (94%) while only having 12% of examples annotated by experts.
The cost is about 52 cents per label for crowd work plus the cost of the expert.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a little tricky to compare the cost with prior work.
Comparing to other work on SRL, we spend more on the crowd, but less on experts.
Whether that trade-off is worth it will depend on the cost of experts.
In practise, our experts are often members of the research team and so their time is a stronger constraint than the crowdsourcing budget.
In that case, our approach comes out ahead as we can get more data annotated per unit of expert effort (by a factor of four).
The QA-SRL work is quite a bit cheaper, at 54 cents per predicate with 2.9 roles on average (which would be ~$2 + expert effort for our approach), but the type of annotations collected are quite different, with ours providing labels from the sense inventory in Propbank.&lt;/p&gt;
&lt;p&gt;I see a range of interesting potential improvements for future work.
First, bringing in methods of worker training in order to improve their accuracy and so reduce the need for duplicate effort.
Second, combining with ideas from other work, such as having a model deciding whether examples are easy or hard and changing how they are processed accordingly, or using QA-SRL annotation to inform the process.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.jkk.name/pub/emnlp-findings20srl.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1314632048070594560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My Tweet&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{emnlp-findings20srl,
  title     = {A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels},
  author    = {Jiang, Youxuan and Zhu, Huaiyu and Kummerfeld, Jonathan K. and Li, Yunyao and Lasecki, Walter},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  shortvenue = {Findings of EMNLP},
  month     = {November},
  year      = {2020},
  location  = {Online},
  url       = {https://www.jkk.name/pub/emnlp-findings20srl.pdf},
  abstract  = {Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95\% accuracy for predicate labels and 93\% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56\% to 14\% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)</title>
      <link>https://www.jkk.name/post/2020-09-25_crowdqasrl/</link>
      <pubDate>Fri, 25 Sep 2020 10:17:18 -0500</pubDate>
      <guid>https://www.jkk.name/post/2020-09-25_crowdqasrl/</guid>
      <description>&lt;p&gt;Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments.
Over the last few years, 
&lt;a href=&#34;https://www.cs.washington.edu/people/faculty/lsz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luke Zettlemoyer&amp;rsquo;s Group&lt;/a&gt; has been exploring using question-answer pairs to represent this structure.
This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank.
However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.&lt;/p&gt;
&lt;p&gt;I got three main things out of this paper:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It shifted my approach to crowdsourcing to consider workers more like traditional expert annotators.&lt;/li&gt;
&lt;li&gt;It reinforced the idea that small shifts in crowd workflows can have a major impact on annotation quality.&lt;/li&gt;
&lt;li&gt;QA-SRL can capture roles not covered by PropBank.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The work also provides a new dataset that will be useful for future work on this problem, and useful benchmarks of systems and measurements of data quality.
Expanding on the three points above:&lt;/p&gt;
&lt;p&gt;Crowd workers: The paper argues in favour of putting more time into training workers.
Most of the work I&amp;rsquo;ve seen in NLP for crowdsourcing (including my own) focuses on modifying task design or using ML post-processing to improve results.
Here, they run a large-scale qualification task and filter workers based on their performance, then train those workers by paying them to read a set of instructions (23 text-dense slides) and do two small annotation rounds with feedback after each one.
This increases the upfront cost, but reduces the cost of annotation by reducing the need for multiple annotations of each item.
The paper doesn&amp;rsquo;t provide quite enough detail to quantify the cost.
We do know that to get to 11 workers they needed to train 30 workers at a cost of 2 hours each plus 30 minutes of researcher time each.
If we assume 60 workers did the preliminary round, each taking 5 minutes, and that workers cost &lt;span&gt;$&lt;/span&gt;12 / hour (&lt;span&gt;$&lt;/span&gt;10 to the workers, &lt;span&gt;$&lt;/span&gt;2 to Amazon), that&amp;rsquo;s almost &lt;span&gt;$&lt;/span&gt;800 plus 15 hours of researcher time.
For a large annotation effort, the savings during annotation will make that worth it (or, as in this case, it will lead to higher quality data).
I am curious which aspect was more important though - filtering the pool of workers, or training workers.&lt;/p&gt;
&lt;p&gt;Workflow impact: In previous QA-SRL work, one worker wrote a question and its answers and two workers checked the question and independently added answers.
Here, two workers independently write a question+answer and a third work consolidates the annotations into a final annotation.
The cost for a label is about the same (54c / predicate vs. 51c / predicate), but coverage is considerably higher.
The design space for crowd workflows is huge and this is another example of how important it is to explore.
It&amp;rsquo;s also possible that the changes in recruitment and training were more critical than the workflow shift, but the study didn&amp;rsquo;t include evaluation with only one or the other.&lt;/p&gt;
&lt;p&gt;QA-SRL vs. PropBank: This may be less surprising to someone who works more on SRL, but they found their approach captured many implicit roles that PropBank does not.
Specifically, of 100 annotated arguments that were not in PropBank, 68 were valid implicit arguments.
I&amp;rsquo;m curious about what those implicit arguments are capturing.
Maybe targeted re-annotation could be used to add them to PropBank (identifying relevant sentences by trace parsing).&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.626/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/plroit/qasrl-gs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jkkummerfeld/status/1309592830537543681?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My Tweet&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{roit-etal-2020-controlled,
    title = &amp;quot;Controlled Crowdsourcing for High-Quality {QA}-{SRL} Annotation&amp;quot;,
    author = &amp;quot;Roit, Paul  and
      Klein, Ayal  and
      Stepanov, Daniela  and
      Mamou, Jonathan  and
      Michael, Julian  and
      Stanovsky, Gabriel  and
      Zettlemoyer, Luke  and
      Dagan, Ido&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&amp;quot;,
    month = &amp;quot;jul&amp;quot;,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://www.aclweb.org/anthology/2020.acl-main.626&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.acl-main.626&amp;quot;,
    pages = &amp;quot;7008--7013&amp;quot;,
    abstract = &amp;quot;Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Crowdsourcing Services</title>
      <link>https://www.jkk.name/post/crowdsourcing-services/</link>
      <pubDate>Sun, 14 Apr 2019 10:49:35 -0400</pubDate>
      <guid>https://www.jkk.name/post/crowdsourcing-services/</guid>
      <description>&lt;p&gt;Crowdsourcing, collecting annotations of data from a distributed group of people online, is a major source of data for AI research.
The original idea involved people doing it as volunteers (e.g. Folding@home) or as a byproduct of some other goal (e.g. reCAPTCHA), but most of the data collected in AI today is from paid workers.
Recently, 
&lt;a href=&#34;http://users.umiacs.umd.edu/~hal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hal Daumé III&lt;/a&gt; mentioned on 
&lt;a href=&#34;https://twitter.com/haldaume3/status/1113889907586535425&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt; that Figure Eight, a paid crowdsourcing service, had removed their free licenses for academics, and asked for alternatives (Note, Figure Eight has since been acquired by Appen).
A bunch of people had suggestions which I wanted to record for my own future reference, hence this blog post.&lt;/p&gt;
&lt;p&gt;These fell into a few categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Crowd providers, which directly connect with workers.&lt;/li&gt;
&lt;li&gt;Crowd enhancers, which provide a layer on top of the providers that adds features (e.g. active learning, nice templates, sophisticated workflows).&lt;/li&gt;
&lt;li&gt;Annotation tools, which are designed to integrate with crowd providers (or your own internal workers).&lt;/li&gt;
&lt;li&gt;Interfaces, which make it easier to use one of the crowd providers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I decided not to break the first two categories apart because it was sometimes unclear whether a service was using their own crowd or providing a layer over another, but I have roughly sorted them.
Where possible I have included pricing, though some services did not make it easy to find.
Take note of the description in each case because the data collected varies substantially.
Also note that many tasks can be structured as a classification task (e.g. &amp;ldquo;Is this coreference link correct?&amp;quot;), making many of these services more flexible than the &amp;lsquo;text classification&amp;rsquo; label below may seem (though structuring your task so costs don&amp;rsquo;t explode may require some thought).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.mturk.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mechanical Turk&lt;/a&gt;, a small set of templates and the option to define a web UI that does whatever you want. Cost is a 20% fee on top of whatever you choose to pay workers (though note it jumps to 40% if you have more than 10 assignments for a HIT!).&lt;/li&gt;
&lt;li&gt;Figure Eight (now part of 
&lt;a href=&#34;http://appen.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Appen&lt;/a&gt;), included for completeness, did not investigate further due to the cost.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.gethybrid.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hybrid&lt;/a&gt;, seems to be any task you can define in text (including with links?). 40% fee, though there is a 
&lt;a href=&#34;http://www.gethybrid.io/faq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discount&lt;/a&gt; of some type for academic and non-profit institutions.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://prolific.ac/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prolific&lt;/a&gt;, seems to be that you just provide a link to a site for annotations (originally intended for survey research). 30% fee. Last year they had a 
&lt;a href=&#34;https://blog.prolific.ac/announcing-2018-junior-grant-winners/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;research grant&lt;/a&gt; program.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://gorilla.sc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gorilla&lt;/a&gt;, designed for social science research, but could be used for any classification or free text task. Costs $1.19 / response, though note that you construct a questionnaire with a series of questions. There are also 
&lt;a href=&#34;https://gorilla.sc/support/reference/subscription-FAQ#subscription-types&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discounts&lt;/a&gt; available when collecting thousands of responses.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://scale.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scale&lt;/a&gt;, classification tasks for 8c / annotation. There is an academic program, but details are not available online (mentioned 
&lt;a href=&#34;https://twitter.com/umbrant/status/1114312024970764290&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://aws.amazon.com/sagemaker/groundtruth/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon SageMaker Ground Truth&lt;/a&gt;, text classification for 8c / label, decreasing after 50,000 annotations + a workflow fee of 1.2c / label.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://imerit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iMerit&lt;/a&gt;, NER, classification, and sentiment tasks. When used on the Amazon Marketplace they are 5 dollars / hour (India based workers) or 25 (US based workers).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mechanical-turk-integration-interfaces&#34;&gt;Mechanical Turk Integration Interfaces&lt;/h2&gt;
&lt;p&gt;These are interfaces for Mechanical Turk that provide an easier way to set up HITs without having to mess with Amazon&amp;rsquo;s APIs yourself.
Both are free, but have slightly different features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.cromalab.net/LegionTools/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LegionTools&lt;/a&gt;, self-hosted or not, includes key features for real-time systems.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/webis-de/mturk-manager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MTurk Manager&lt;/a&gt;, self-hosted, includes features for custom views of responses from workers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;annotation-user-interfaces&#34;&gt;Annotation User Interfaces&lt;/h2&gt;
&lt;p&gt;There are many annotation tools for NLP (e.g. my own, 
&lt;a href=&#34;https://www.jkk.name/slate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SLATE&lt;/a&gt;!), but these annotation tools are designed to integrate with providers above to collect annotations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://prodi.gy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prodigy&lt;/a&gt;, span classification (e.g. NER), multiple choice questions (which can be used to do a wide range of tasks), and relations (see 
&lt;a href=&#34;https://prodi.gy/features/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;examples&lt;/a&gt;). Cost is whatever you pay a crowd provider + 390 for a lifetime license, or 10k for a university-wide lifetime license, though they also often give free licenses to academics. One distinctive property is that you download and run it yourself, providing complete control over your data.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lighttag.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LightTAG&lt;/a&gt;, span classification and links. Cost is 1c / annotation + the cost from a crowd provider, but there is an academic license that makes it free.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</title>
      <link>https://www.jkk.name/post/2018-01-28_crowdassistant/</link>
      <pubDate>Sun, 28 Jan 2018 16:01:20 -0500</pubDate>
      <guid>https://www.jkk.name/post/2018-01-28_crowdassistant/</guid>
      <description>&lt;p&gt;There is a lot of interest in dialogue agents, but a lot of work sits at one of two extremes: either chit-chat agents that just chat, or task-oriented agents that aim to call a specific API for the user.
This work is about trying to integrate a range of systems from both categories, to get something more general purpose as a result.&lt;/p&gt;
&lt;p&gt;The core approach is a hybrid system that switches between different agents behind the scenes (an approach taken by a number of Alexa Prize teams).
The innovation here is that crowd workers will help with the decision (both suggesting things to say and voting on which response to use), and their votes will be used to learn a model to (partially) replace the people over time.&lt;/p&gt;
&lt;p&gt;Unfortunately, the improvement from a learned model of votes is only small (saves only 14% of the crowd effort), and the automated responses are rarely chosen (12% of the time).
That said, it seems like an interesting design with a lot of subtle decisions that require more exploration - the sets of agents (4-6 here, mostly narrow types), the voting scheme (only 1 or 2 votes needed here), choosing which agent responses to show (here, the proportion of previously accepted messages from this agent), and so on.
That choice of which responses to show is particularly tricky, as with this scheme a very domain specific agent might get voted down too much initially and never be chosen when the appropriate time comes.
One potentially interesting alternative would be to let the crowd workers choose which agent&amp;rsquo;s response to see, and possibly even post-edit slightly.&lt;/p&gt;
&lt;p&gt;Note - This post is the first of a (hopefully) regular series again.
However, rather than keeping it weekday-ly, I plan to do three times a week, at least until the ACL deadline.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{blah,
  title = {Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time},
  author    = {Huang, Ting-Hao (Kenneth) and Chang, Joseph Chee and Bigham, Jeffrey P.},
  booktitle = {CHI},
  year = {2018},
  url = {https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)</title>
      <link>https://www.jkk.name/post/2017-10-31_realtimecaptioning/</link>
      <pubDate>Tue, 31 Oct 2017 13:23:13 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-31_realtimecaptioning/</guid>
      <description>&lt;p&gt;For any given task, automatic systems are fast, while annotation is accurate.
This work is about bridging that gap to provide a way for a team of annotators to produce real-time high quality labels.
The specific application is speech transcription, in which automatic systems are not accurate, while average people are slow (experts can transcribe in real time, but are very expensive).&lt;/p&gt;
&lt;p&gt;The solution is to carefully break up the task and combine annotations back together.
To get it to work well there are a range of subtle design decisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;People hear the entire audio stream, but with their section at normal volume and the rest quieter. This allows them to focus their effort while still understanding the context.&lt;/li&gt;
&lt;li&gt;The alignment process combines annotations with guidance from a language model and a model of typos based on keyboard layout.&lt;/li&gt;
&lt;li&gt;Words are locked in shortly after being typed, to encourage workers to go on rather than revising their own errors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Follow up work added several more ideas to improve performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time warping, slowing down to half speed for their section, then going to 1.5x for the rest.&lt;/li&gt;
&lt;li&gt;Use ASR as well, either as another worker (with very uncorrelated errors), or as a starting point for human editing (or vice versa).&lt;/li&gt;
&lt;li&gt;Use A* search rather than a greedy algorithm for the alignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Performance does not reach the level of a professional, but is far better than ASR.
From the paper it&amp;rsquo;s tricky to see a final cost, but it is certainly far lower than the professional.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://doi.acm.org/10.1145/2380116.2380122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Lasecki:2012:RCG:2380116.2380122,
 author = {Lasecki, Walter and Miller, Christopher and Sadilek, Adam and Abumoussa, Andrew and Borrello, Donato and Kushalnagar, Raja and Bigham, Jeffrey},
 title = {Real-time Captioning by Groups of Non-experts},
 booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
 series = {UIST &#39;12},
 year = {2012},
 isbn = {978-1-4503-1580-7},
 location = {Cambridge, Massachusetts, USA},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2380116.2380122},
 doi = {10.1145/2380116.2380122},
 acmid = {2380122},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {captioning, crowdsourcing, deaf, hard of hearing, real-time, text alignment, transcription},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</title>
      <link>https://www.jkk.name/post/2017-10-19_mace/</link>
      <pubDate>Thu, 19 Oct 2017 17:08:50 -0400</pubDate>
      <guid>https://www.jkk.name/post/2017-10-19_mace/</guid>
      <description>&lt;p&gt;The standard way to get high quality annotations is to get labels from multiple people and take a majority vote.
Getting multiple annotations costs more, and the quality of annotators can vary considerably (with spamming at one extreme).
One way to avoid the quality issue is to restrict who can do the task (must have done X previous tasks with an accept rate of Y), but that limits the pool of available workers.
Another approach is to try to estimate the quality of annotator work using a statistical model.&lt;/p&gt;
&lt;p&gt;Here a generative model is used, with the following structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T_i$, the true label, sampled with a uniform prior over labels&lt;/li&gt;
&lt;li&gt;$S_{ij}$, a binary variable indicating if the person is spamming or not, sampled as a Bernoulli variable with a Beta prior&lt;/li&gt;
&lt;li&gt;$A_{ij}$, the annotator&amp;rsquo;s decision, if they are spamming it is sampled from a multinomial with parameters specific to them (with a Dirichlet prior), otherwise it is the true label&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$A$ is observed, but $T$ and $S$ are not, so they use expectation maximization to get both model parameters and variable values.
To deal with nonconvexity they use 100 random restarts, deciding which is best based on how well the model describes the data.
Note - this model (and the code) was the basis of the error detection paper I 
&lt;a href=&#34;https://www.jkk.name/post/2017-10-13_errordetection/&#34;&gt;wrote about&lt;/a&gt; recently.&lt;/p&gt;
&lt;p&gt;For predicting annotator quality the model is consistently effective across three datasets, though the Beta and Dirichlet priors are key for one (where annotator agreement was high on average).
For determining the correct answer it is slightly better than majority vote, though the gains are small.
The real advantage comes in deciding whether to discard data, where the choice of what to discard can be guided by the estimate of quality (this is what the error detection paper was doing).
A range of synthetic experiments also show positive results, though their design shares the assumptions about behaviour that are baked into the model.&lt;/p&gt;
&lt;p&gt;I found a few results particularly interesting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As the number of annotators is decreased, the benefit of this approach over majority vote grows to be quite substantial (the main experiments are for data with 10 annotators).&lt;/li&gt;
&lt;li&gt;If you do use majority vote, use an odd number of annotators. Switching to an even number mainly seems to create ties. The right number is also very data dependent.&lt;/li&gt;
&lt;li&gt;Providing gold information as supervision within EM doesn&amp;rsquo;t help much unless it is quite substantial (20%+ of the data)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.aclweb.org/anthology/N13-1132.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hovy-EtAl:2013:NAACL-HLT,
  author    = {Hovy, Dirk  and  Berg-Kirkpatrick, Taylor  and  Vaswani, Ashish  and  Hovy, Eduard},
  title     = {Learning Whom to Trust with MACE},
  booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2013},
  address   = {Atlanta, Georgia},
  publisher = {Association for Computational Linguistics},
  pages     = {1120--1130},
  url       = {http://www.aclweb.org/anthology/N13-1132}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
