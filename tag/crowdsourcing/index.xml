<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>crowdsourcing | Jonathan K. Kummerfeld</title>
    <link>http://www.jkk.name/tag/crowdsourcing/</link>
      <atom:link href="http://www.jkk.name/tag/crowdsourcing/index.xml" rel="self" type="application/rss+xml" />
    <description>crowdsourcing</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Jonathan K. Kummerfeld</copyright><lastBuildDate>Sun, 14 Apr 2019 10:49:35 -0400</lastBuildDate>
    <image>
      <url>http://www.jkk.name/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>crowdsourcing</title>
      <link>http://www.jkk.name/tag/crowdsourcing/</link>
    </image>
    
    <item>
      <title>Crowdsourcing Services</title>
      <link>http://www.jkk.name/post/crowdsourcing-services/</link>
      <pubDate>Sun, 14 Apr 2019 10:49:35 -0400</pubDate>
      <guid>http://www.jkk.name/post/crowdsourcing-services/</guid>
      <description>&lt;p&gt;Crowdsourcing, collecting annotations of data from a distributed group of people online, is a major source of data for AI research.
The original idea involved people doing it as volunteers (e.g. Folding@home) or as a byproduct of some other goal (e.g. reCAPTCHA), but most of the data collected in AI today is from paid workers.
Recently, 
&lt;a href=&#34;http://users.umiacs.umd.edu/~hal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hal Daumé III&lt;/a&gt; mentioned on 
&lt;a href=&#34;https://twitter.com/haldaume3/status/1113889907586535425&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt; that Figure Eight, a paid crowdsourcing service, had removed their free licenses for academics, and asked for alternatives.
A bunch of people had suggestions which I wanted to record for my own future reference, hence this blog post.&lt;/p&gt;
&lt;p&gt;These fell into a few categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Crowd providers, which directly connect with workers.&lt;/li&gt;
&lt;li&gt;Crowd enhancers, which provide a layer on top of the providers that adds features (e.g. active learning, nice templates, sophisticated workflows).&lt;/li&gt;
&lt;li&gt;Annotation tools, which are designed to integrate with crowd providers (or your own internal workers).&lt;/li&gt;
&lt;li&gt;Interfaces, which make it easier to use one of the crowd providers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I decided not to break the first two categories apart because it was sometimes unclear whether a service was using their own crowd or providing a layer over another, but I have roughly sorted them.
Where possible I have included pricing, though some services did not make it easy to find.
Take note of the description in each case because the data collected varies substantially.
Also note that many tasks can be structured as a classification task (e.g. &amp;ldquo;Is this coreference link correct?&amp;quot;), making many of these services more flexible than the &amp;lsquo;text classification&amp;rsquo; label below may seem (though structuring your task so costs don&amp;rsquo;t explode may require some thought).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.mturk.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mechanical Turk&lt;/a&gt;, a small set of templates and the option to define a web UI that does whatever you want. Cost is a 20% fee on top of whatever you choose to pay workers (though note it jumps to 40% if you have more than 10 assignments for a HIT!).&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.figure-eight.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure Eight&lt;/a&gt; (included for completeness, did not investigate further due to the cost)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.gethybrid.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hybrid&lt;/a&gt;, seems to be any task you can define in text (including with links?). 40% fee, though there is a 
&lt;a href=&#34;http://www.gethybrid.io/faq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discount&lt;/a&gt; of some type for academic and non-profit institutions.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://prolific.ac/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prolific&lt;/a&gt;, seems to be that you just provide a link to a site for annotations (originally intended for survey research). 30% fee. Last year they had a 
&lt;a href=&#34;https://blog.prolific.ac/announcing-2018-junior-grant-winners/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;research grant&lt;/a&gt; program.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://gorilla.sc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gorilla&lt;/a&gt;, designed for social science research, but could be used for any classification or free text task. Costs $1.19 / response, though note that you construct a questionnaire with a series of questions. There are also 
&lt;a href=&#34;https://gorilla.sc/support/reference/subscription-FAQ#subscription-types&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discounts&lt;/a&gt; available when collecting thousands of responses.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://scale.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scale&lt;/a&gt;, classification tasks for 8c / annotation. There is an academic program, but details are not available online (mentioned 
&lt;a href=&#34;https://twitter.com/umbrant/status/1114312024970764290&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://aws.amazon.com/sagemaker/groundtruth/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon SageMaker Ground Truth&lt;/a&gt;, text classification for 8c / label, decreasing after 50,000 annotations + a workflow fee of 1.2c / label.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://imerit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iMerit&lt;/a&gt;, NER, classification, and sentiment tasks. When used on the Amazon Marketplace they are 5 dollars / hour (India based workers) or 25 (US based workers).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mechanical-turk-integration-interfaces&#34;&gt;Mechanical Turk Integration Interfaces&lt;/h2&gt;
&lt;p&gt;These are interfaces for Mechanical Turk that provide an easier way to set up HITs without having to mess with Amazon&amp;rsquo;s APIs yourself.
Both are free, but have slightly different features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.cromalab.net/LegionTools/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LegionTools&lt;/a&gt;, self-hosted or not, includes key features for real-time systems.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/webis-de/mturk-manager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MTurk Manager&lt;/a&gt;, self-hosted, includes features for custom views of responses from workers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;annotation-user-interfaces&#34;&gt;Annotation User Interfaces&lt;/h2&gt;
&lt;p&gt;There are many annotation tools for NLP (e.g. my own, 
&lt;a href=&#34;http://jkk.name/slate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SLATE&lt;/a&gt;!), but these annotation tools are designed to integrate with providers above to collect annotations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://prodi.gy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prodigy&lt;/a&gt;, span classification (e.g. NER), multiple choice questions (which can be used to do a wide range of tasks), and relations (see 
&lt;a href=&#34;https://prodi.gy/features/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;examples&lt;/a&gt;). Cost is whatever you pay a crowd provider + 390 for a lifetime license, or 10k for a university-wide lifetime license, though they also often give free licenses to academics. One distinctive property is that you download and run it yourself, providing complete control over your data.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lighttag.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LightTAG&lt;/a&gt;, span classification and links. Cost is 1c / annotation + the cost from a crowd provider, but there is an academic license that makes it free.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</title>
      <link>http://www.jkk.name/post/2018-01-28_crowdassistant/</link>
      <pubDate>Sun, 28 Jan 2018 16:01:20 -0500</pubDate>
      <guid>http://www.jkk.name/post/2018-01-28_crowdassistant/</guid>
      <description>&lt;p&gt;There is a lot of interest in dialogue agents, but a lot of work sits at one of two extremes: either chit-chat agents that just chat, or task-oriented agents that aim to call a specific API for the user.
This work is about trying to integrate a range of systems from both categories, to get something more general purpose as a result.&lt;/p&gt;
&lt;p&gt;The core approach is a hybrid system that switches between different agents behind the scenes (an approach taken by a number of Alexa Prize teams).
The innovation here is that crowd workers will help with the decision (both suggesting things to say and voting on which response to use), and their votes will be used to learn a model to (partially) replace the people over time.&lt;/p&gt;
&lt;p&gt;Unfortunately, the improvement from a learned model of votes is only small (saves only 14% of the crowd effort), and the automated responses are rarely chosen (12% of the time).
That said, it seems like an interesting design with a lot of subtle decisions that require more exploration - the sets of agents (4-6 here, mostly narrow types), the voting scheme (only 1 or 2 votes needed here), choosing which agent responses to show (here, the proportion of previously accepted messages from this agent), and so on.
That choice of which responses to show is particularly tricky, as with this scheme a very domain specific agent might get voted down too much initially and never be chosen when the appropriate time comes.
One potentially interesting alternative would be to let the crowd workers choose which agent&amp;rsquo;s response to see, and possibly even post-edit slightly.&lt;/p&gt;
&lt;p&gt;Note - This post is the first of a (hopefully) regular series again.
However, rather than keeping it weekday-ly, I plan to do three times a week, at least until the ACL deadline.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{blah,
  title = {Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time},
  author = {Ting-Hao (Kenneth) Huang, Joseph Chee Chang, and Jeffrey P. Bigham},
  booktitle = {CHI},
  year = {2018},
  url = {https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)</title>
      <link>http://www.jkk.name/post/2017-10-31_realtimecaptioning/</link>
      <pubDate>Tue, 31 Oct 2017 13:23:13 -0400</pubDate>
      <guid>http://www.jkk.name/post/2017-10-31_realtimecaptioning/</guid>
      <description>&lt;p&gt;For any given task, automatic systems are fast, while annotation is accurate.
This work is about bridging that gap to provide a way for a team of annotators to produce real-time high quality labels.
The specific application is speech transcription, in which automatic systems are not accurate, while average people are slow (experts can transcribe in real time, but are very expensive).&lt;/p&gt;
&lt;p&gt;The solution is to carefully break up the task and combine annotations back together.
To get it to work well there are a range of subtle design decisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;People hear the entire audio stream, but with their section at normal volume and the rest quieter. This allows them to focus their effort while still understanding the context.&lt;/li&gt;
&lt;li&gt;The alignment process combines annotations with guidance from a language model and a model of typos based on keyboard layout.&lt;/li&gt;
&lt;li&gt;Words are locked in shortly after being typed, to encourage workers to go on rather than revising their own errors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Follow up work added several more ideas to improve performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time warping, slowing down to half speed for their section, then going to 1.5x for the rest.&lt;/li&gt;
&lt;li&gt;Use ASR as well, either as another worker (with very uncorrelated errors), or as a starting point for human editing (or vice versa).&lt;/li&gt;
&lt;li&gt;Use A* search rather than a greedy algorithm for the alignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Performance does not reach the level of a professional, but is far better than ASR.
From the paper it&amp;rsquo;s tricky to see a final cost, but it is certainly far lower than the professional.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://doi.acm.org/10.1145/2380116.2380122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Lasecki:2012:RCG:2380116.2380122,
 author = {Lasecki, Walter and Miller, Christopher and Sadilek, Adam and Abumoussa, Andrew and Borrello, Donato and Kushalnagar, Raja and Bigham, Jeffrey},
 title = {Real-time Captioning by Groups of Non-experts},
 booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
 series = {UIST &#39;12},
 year = {2012},
 isbn = {978-1-4503-1580-7},
 location = {Cambridge, Massachusetts, USA},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2380116.2380122},
 doi = {10.1145/2380116.2380122},
 acmid = {2380122},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {captioning, crowdsourcing, deaf, hard of hearing, real-time, text alignment, transcription},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</title>
      <link>http://www.jkk.name/post/2017-10-19_mace/</link>
      <pubDate>Thu, 19 Oct 2017 17:08:50 -0400</pubDate>
      <guid>http://www.jkk.name/post/2017-10-19_mace/</guid>
      <description>&lt;p&gt;The standard way to get high quality annotations is to get labels from multiple people and take a majority vote.
Getting multiple annotations costs more, and the quality of annotators can vary considerably (with spamming at one extreme).
One way to avoid the quality issue is to restrict who can do the task (must have done X previous tasks with an accept rate of Y), but that limits the pool of available workers.
Another approach is to try to estimate the quality of annotator work using a statistical model.&lt;/p&gt;
&lt;p&gt;Here a generative model is used, with the following structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T_i$, the true label, sampled with a uniform prior over labels&lt;/li&gt;
&lt;li&gt;$S_{ij}$, a binary variable indicating if the person is spamming or not, sampled as a Bernoulli variable with a Beta prior&lt;/li&gt;
&lt;li&gt;$A_{ij}$, the annotator&amp;rsquo;s decision, if they are spamming it is sampled from a multinomial with parameters specific to them (with a Dirichlet prior), otherwise it is the true label&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$A$ is observed, but $T$ and $S$ are not, so they use expectation maximization to get both model parameters and variable values.
To deal with nonconvexity they use 100 random restarts, deciding which is best based on how well the model describes the data.
Note - this model (and the code) was the basis of the error detection paper I 
&lt;a href=&#34;http://www.jkk.name/post/2017-10-13_errordetection/&#34;&gt;wrote about&lt;/a&gt; recently.&lt;/p&gt;
&lt;p&gt;For predicting annotator quality the model is consistently effective across three datasets, though the Beta and Dirichlet priors are key for one (where annotator agreement was high on average).
For determining the correct answer it is slightly better than majority vote, though the gains are small.
The real advantage comes in deciding whether to discard data, where the choice of what to discard can be guided by the estimate of quality (this is what the error detection paper was doing).
A range of synthetic experiments also show positive results, though their design shares the assumptions about behaviour that are baked into the model.&lt;/p&gt;
&lt;p&gt;I found a few results particularly interesting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As the number of annotators is decreased, the benefit of this approach over majority vote grows to be quite substantial (the main experiments are for data with 10 annotators).&lt;/li&gt;
&lt;li&gt;If you do use majority vote, use an odd number of annotators. Switching to an even number mainly seems to create ties. The right number is also very data dependent.&lt;/li&gt;
&lt;li&gt;Providing gold information as supervision within EM doesn&amp;rsquo;t help much unless it is quite substantial (20%+ of the data)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.aclweb.org/anthology/N13-1132.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hovy-EtAl:2013:NAACL-HLT,
  author    = {Hovy, Dirk  and  Berg-Kirkpatrick, Taylor  and  Vaswani, Ashish  and  Hovy, Eduard},
  title     = {Learning Whom to Trust with MACE},
  booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2013},
  address   = {Atlanta, Georgia},
  publisher = {Association for Computational Linguistics},
  pages     = {1120--1130},
  url       = {http://www.aclweb.org/anthology/N13-1132}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
