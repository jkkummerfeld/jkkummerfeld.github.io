---
title: "Diplomacy"
description: Playing the board game Diplomacy
---

## No-Press / No communication

[Kraus and Lehman (1995)](https://u.cs.biu.ac.il/~krauss/data/articles/diplomacy.pdf)

[Fabregues and Sierra (2011)](https://www.sciencedirect.com/science/article/abs/pii/S0952197611001059)

[Jonge et al. (2018)](https://link.springer.com/chapter/10.1007/978-3-030-17294-7_8)

The first neural model was DipNet [(Paquette et al. 2019)](https://proceedings.neurips.cc/paper/2019/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf), a project I contributed to.
We used supervised learning based on online games, using an encoder-decoder network.
The system plays as well as strong human player and was integrated into [WebDiplomacy](https://webdiplomacy.net/).
We tried applying reinforcement learning, without success.

Immediate follow-up work refined the architecture, filtered the training data, and found ways to make RL effective.
Specifically, [Anthony et al. (2020)](https://arxiv.org/abs/2006.04635) changed the encoder and decoder, and used policy iteration in which a set of actions are sampled and the best option is used to approximate the best response action.
[Gray et al. (2021)](https://openreview.net/forum?id=0-uUGPbIjD) improved the model further, and proposed another training approach.
They sample a set of possible orders for all players, then treat that as the space of moves, estimate regret by treating the space of moves as a game to be optimized, with rollouts of 2-3 turns and reward based on the base model's value network.
The final system plays based on the final regret matching iteration's policy, doing better than prior work.

Building off the RL ideas in the work above, [Bakhtin et al. (2021](https://openreview.net/forum?id=Pq7wIzt3OUE) explored training without using any supervised learning from human games.
To get it to work, they further modified the architecture, and applied an AlphaZero style training approach.
Interestingly, the models produced in this way win in a 6v1 setting (6 self-trained vs 1 prior model) but do worse than expected in a 1v6 setting, suggesting that it has converged to a different strategy, and the same is observed for multiple training runs with different seeds (ie, it converges to different strategies).
The paper does not provide detailed analysis of the actual games, but one possibility is that the agents are converging to a very coordinated strategy (e.g. Russia and Turkey working together).

RL trained models diverge from human styles of play, as observed for DipNet and the [Bakhtin et al. (2021](https://openreview.net/forum?id=Pq7wIzt3OUE) bot.
To address this, [Jacob et al. (2021)](https://arxiv.org/pdf/2112.07544.pdf) modify the policy to be regularized towards a human-style policy.
In other words, there is a penalty for having a distribution over moves that differs from a policy trained with supervised learning.
This leads to improved performance in both playing the game and predicting what an expert player would do (in Chess and Go as well as Diplomacy).

## Press / With communication

Cicero
[Bakhtin et al. (2022)](https://arxiv.org/pdf/2210.05492.pdf)
[Bakhtin et al. (2022)](https://www.science.org/doi/10.1126/science.ade9097)

### Structured Communication

DARPA recently announced the [SHADE Program](https://sam.gov/opp/298ecfd9a9454245a5aca37fb7d970e1/view#general), which will explore bots that can communicate, though with a constrained communication language, rather than full natural language.

## Language

[Kram√°r et al. (2022)](https://www.nature.com/articles/s41467-022-34473-5)

Two studies have looked at the language used in human games, to see if there are markers of deception. [Peskov et al. (2020](https://aclanthology.org/2020.acl-main.353/) introduced a dataset where players indicated whether they were lying while playing the game and recipients indicated whether they thought they were being lied to. This is nice because it is more reliable than post-hoc analysis. Humans and machines are not that great at detecting lies (Lie F1 of at most 27), though the errors are quite different and overall performance is fairly similar. However, they only use a single feature of the game state, and only train on the 9 labeled games (rather than pre-training or similar on other resources).

[Niculae et al. (2015)](https://aclanthology.org/P15-1159/)

