<!doctype html>
<html lang="en" class="no-js">
  <head>
    <link rel="stylesheet" href="https://www.jkk.name/plugins/my-custom/css/mycustom.css"><link rel="stylesheet" href="https://www.jkk.name/plugins/academic-icons/css/academicons.css">
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.89.4" /><META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020) | Jonathan K. Kummerfeld</title>


  <meta name="description" content="This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 50 million&#43; words of text, but only 1 GPU for training?">
<meta property="og:title" content="Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)" />
<meta property="og:description" content="This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 50 million&#43; words of text, but only 1 GPU for training?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.jkk.name/reading-notes/old-blog/2020-09-29_pretraininglm/" /><meta property="article:section" content="reading-notes" />

<meta property="article:modified_time" content="2021-11-03T17:10:03-04:00" /><meta property="og:site_name" content="Jonathan K. Kummerfeld" />

<meta itemprop="name" content="Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)">
<meta itemprop="description" content="This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 50 million&#43; words of text, but only 1 GPU for training?">
<meta itemprop="dateModified" content="2021-11-03T17:10:03-04:00" />
<meta itemprop="wordCount" content="1199">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)"/>
<meta name="twitter:description" content="This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 50 million&#43; words of text, but only 1 GPU for training?"/>




<link rel="preload" href="/scss/main.min.098e976d5113d2dd5db369ca3e1b98f01c9e6e4acb8102f6b9902a4826c64e81.css" as="style">
<link href="/scss/main.min.098e976d5113d2dd5db369ca3e1b98f01c9e6e4acb8102f6b9902a4826c64e81.css" rel="stylesheet" integrity="">


<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>

<script
  src="https://unpkg.com/lunr@2.3.8/lunr.min.js"
  integrity="sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY"
  crossorigin="anonymous"></script>







<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-19179423-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>




  </head>
  <body class="td-page">
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
            
<div id="td-sidebar-menu" class="td-sidebar__inner">
  <nav class="collapse td-sidebar-nav" id="td-section-nav">
    <ul class="td-sidebar-nav__section pr-md-3 ul-0">
      <li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m--li">
  <a href="/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section tree-root" id="m-"><span class="">Jonathan K. Kummerfeld&#39;s Homepage</span></a>
  <ul class="ul-1">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-home-li">
  <a href="/home/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-home"><span class="">Home</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-bio_and_cv-li">
  <a href="/bio_and_cv/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-bio_and_cv"><i class="fas fa-male"></i><span class="">Bio &amp; CV</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-students-li">
  <a href="/students/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-students"><i class="fas fa-users"></i><span class="">Students</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-studentsrecruiting-phd-li">
  <a href="/students/recruiting-phd/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-studentsrecruiting-phd"><span class="">Recruiting: PhD</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-pubs-li">
  <a href="/pubs/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-pubs"><i class="fas fa-file-alt"></i><span class="">Publications</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-teaching-li">
  <a href="/teaching/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-teaching"><i class="fas fa-graduation-cap"></i><span class="">Teaching</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-software-li">
  <a href="/software/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-software"><i class="fas fa-code"></i><span class="">Software</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-data-li">
  <a href="/data/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-data"><i class="fas fa-database"></i><span class="">Datasets</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m-reading-notes-li">
  <a href="/reading-notes/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-reading-notes"><i class="fas fa-book-reader"></i><span class="">Reading Notes</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m-reading-notesold-blog-li">
  <a href="/reading-notes/old-blog/" title="Old Blog Posts" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-reading-notesold-blog"><span class="">Olg Blog Posts</span></a>
  <ul class="ul-3 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-09_parsing-autoencoder-li">
  <a href="/reading-notes/old-blog/2017-10-09_parsing-autoencoder/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-09_parsing-autoencoder"><span class=""> Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-05_explainingpredictions-li">
  <a href="/reading-notes/old-blog/2017-12-05_explainingpredictions/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-05_explainingpredictions"><span class="">A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-16_forumrnn-li">
  <a href="/reading-notes/old-blog/2017-10-16_forumrnn/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-16_forumrnn"><span class="">A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2019-07-10_disentanglement-li">
  <a href="/reading-notes/old-blog/2019-07-10_disentanglement/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2019-07-10_disentanglement"><span class="">A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-01_nonsequencener-li">
  <a href="/reading-notes/old-blog/2017-12-01_nonsequencener/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-01_nonsequencener"><span class="">A Local Detection Approach for Named Entity Recognition and Mention Detection (Xu et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-10-04_crowdsrl-li">
  <a href="/reading-notes/old-blog/2020-10-04_crowdsrl/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-04_crowdsrl"><span class="">A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-12_multidomainwordvector-li">
  <a href="/reading-notes/old-blog/2017-12-12_multidomainwordvector/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-12_multidomainwordvector"><span class="">A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings (Yang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-16_ucca-li">
  <a href="/reading-notes/old-blog/2017-11-16_ucca/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-16_ucca"><span class="">A Transition-Based Directed Acyclic Graph Parser for UCCA (Hershcovich et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-17_twostagediscourseparsing-li">
  <a href="/reading-notes/old-blog/2017-11-17_twostagediscourseparsing/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-17_twostagediscourseparsing"><span class="">A Two-Stage Parsing Method for Text-Level Discourse Analysis (Wang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-29_abstractivesummarisation-li">
  <a href="/reading-notes/old-blog/2017-11-29_abstractivesummarisation/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-29_abstractivesummarisation"><span class="">Abstractive Document Summarization with a Graph-Based Attentional Neural Model (Tan et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-18_neuralamr-li">
  <a href="/reading-notes/old-blog/2017-10-18_neuralamr/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-18_neuralamr"><span class="">Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-04-16_lm_analysis-li">
  <a href="/reading-notes/old-blog/2018-04-16_lm_analysis/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-04-16_lm_analysis"><span class="">An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2019-09-22_conferenceapproach-li">
  <a href="/reading-notes/old-blog/2019-09-22_conferenceapproach/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2019-09-22_conferenceapproach"><span class="">Approaching Conferences</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-07_spineparsinglstm-li">
  <a href="/reading-notes/old-blog/2017-11-07_spineparsinglstm/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-07_spineparsinglstm"><span class="">Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-20_onlyattention-li">
  <a href="/reading-notes/old-blog/2017-10-20_onlyattention/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-20_onlyattention"><span class="">Attention Is All You Need (Vaswani et al., ArXiv 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-21_multiinputattention-li">
  <a href="/reading-notes/old-blog/2017-11-21_multiinputattention/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-21_multiinputattention"><span class="">Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-09-03_checklist-li">
  <a href="/reading-notes/old-blog/2020-09-03_checklist/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-03_checklist"><span class="">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-09-07_chartdialogs-li">
  <a href="/reading-notes/old-blog/2020-09-07_chartdialogs/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-07_chartdialogs"><span class="">ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-10-10_demographicembeddings-li">
  <a href="/reading-notes/old-blog/2020-10-10_demographicembeddings/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-10_demographicembeddings"><span class="">Compositional Demographic Word Embeddings (Welch et al., EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-09-25_crowdqasrl-li">
  <a href="/reading-notes/old-blog/2020-09-25_crowdqasrl/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-25_crowdqasrl"><span class="">Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-05-deftnn-li">
  <a href="/reading-notes/old-blog/2017-10-05-deftnn/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-05-deftnn"><span class="">DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-13_errordetection-li">
  <a href="/reading-notes/old-blog/2017-10-13_errordetection/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-13_errordetection"><span class="">Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-30_neuralsequence-li">
  <a href="/reading-notes/old-blog/2017-10-30_neuralsequence/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-30_neuralsequence"><span class="">Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-24_dynamictransition-li">
  <a href="/reading-notes/old-blog/2017-10-24_dynamictransition/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-24_dynamictransition"><span class="">Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-22_errorrepairparsing-li">
  <a href="/reading-notes/old-blog/2017-11-22_errorrepairparsing/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-22_errorrepairparsing"><span class="">Error-repair Dependency Parsing for Ungrammatical Texts (Sakaguchi et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-09-04_featureengineering-li">
  <a href="/reading-notes/old-blog/2018-09-04_featureengineering/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-09-04_featureengineering"><span class="">Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-01-28_crowdassistant-li">
  <a href="/reading-notes/old-blog/2018-01-28_crowdassistant/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-01-28_crowdassistant"><span class="">Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-06-12_parseradaptation-li">
  <a href="/reading-notes/old-blog/2018-06-12_parseradaptation/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-06-12_parseradaptation"><span class="">Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples (Vidur Joshi et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-06-madlibs-li">
  <a href="/reading-notes/old-blog/2017-10-06-madlibs/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-06-madlibs"><span class="">Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-09_framesdataset-li">
  <a href="/reading-notes/old-blog/2017-11-09_framesdataset/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-09_framesdataset"><span class="">Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-12_amralignment-li">
  <a href="/reading-notes/old-blog/2017-10-12_amralignment/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-12_amralignment"><span class="">Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-11_multimt-li">
  <a href="/reading-notes/old-blog/2017-10-11_multimt/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-11_multimt"><span class="">Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-07_rarewordvectors-li">
  <a href="/reading-notes/old-blog/2017-12-07_rarewordvectors/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-07_rarewordvectors"><span class="">High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-12-10_dynamicannoallocation-li">
  <a href="/reading-notes/old-blog/2020-12-10_dynamicannoallocation/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-12-10_dynamicannoallocation"><span class="">Improving Human-Labeled Data through Dynamic Automatic Conflict Resolution (Sun, et al., CoLing 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id="m-reading-notesold-blog2020-09-29_pretraininglm-li">
  <a href="/reading-notes/old-blog/2020-09-29_pretraininglm/" class="align-left pl-0 active td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-29_pretraininglm"><span class="td-sidebar-nav-active-item">Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-14_inorderparsing-li">
  <a href="/reading-notes/old-blog/2017-11-14_inorderparsing/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-14_inorderparsing"><span class="">In-Order Transition-based Constituent Parsing (Liu et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-10-12_taboo-li">
  <a href="/reading-notes/old-blog/2020-10-12_taboo/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-12_taboo"><span class="">Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-30_taggingrelations-li">
  <a href="/reading-notes/old-blog/2017-11-30_taggingrelations/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-30_taggingrelations"><span class="">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme (Zheng et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-03_discourserelations-li">
  <a href="/reading-notes/old-blog/2017-11-03_discourserelations/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-03_discourserelations"><span class="">Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-15_entityvectors-li">
  <a href="/reading-notes/old-blog/2017-11-15_entityvectors/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-15_entityvectors"><span class="">Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-08_graphdialogue-li">
  <a href="/reading-notes/old-blog/2017-11-08_graphdialogue/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-08_graphdialogue"><span class="">Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-03-05_curriculum-li">
  <a href="/reading-notes/old-blog/2018-03-05_curriculum/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-03-05_curriculum"><span class="">Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning (Tsvetkov et al., 2016)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-19_mace-li">
  <a href="/reading-notes/old-blog/2017-10-19_mace/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-19_mace"><span class="">Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-10_kginlstm-li">
  <a href="/reading-notes/old-blog/2017-11-10_kginlstm/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-10_kginlstm"><span class="">Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-23_alphagozero-li">
  <a href="/reading-notes/old-blog/2017-10-23_alphagozero/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-23_alphagozero"><span class="">Mastering the game of Go without human knowledge (Silver et al., Nature 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-06_literarycharacters-li">
  <a href="/reading-notes/old-blog/2017-11-06_literarycharacters/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-06_literarycharacters"><span class="">Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-26_multimodalwordembeddings-li">
  <a href="/reading-notes/old-blog/2017-10-26_multimodalwordembeddings/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-26_multimodalwordembeddings"><span class="">Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-17_nedisambiguation-li">
  <a href="/reading-notes/old-blog/2017-10-17_nedisambiguation/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-17_nedisambiguation"><span class="">Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-13_languagegame-li">
  <a href="/reading-notes/old-blog/2017-11-13_languagegame/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-13_languagegame"><span class="">Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog (Kottur et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-05_multidomainparsing-li">
  <a href="/reading-notes/old-blog/2017-12-05_multidomainparsing/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-05_multidomainparsing"><span class="">Neural Semantic Parsing over Multiple Knowledge-bases (Herzig et al., 2017)</span></a>
</li>
  </ul>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-advice-li">
  <a href="/advice/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-advice"><i class="fas fa-comment"></i><span class="">Advice</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-adviceposters-li">
  <a href="/advice/posters/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-adviceposters"><span class="">Academic Posters</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sponsor-li">
  <a href="/sponsor/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-sponsor"><i class="fas fa-dollar-sign"></i><span class="">Sponsor</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-contact-li">
  <a href="/contact/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-contact"><i class="fas fa-phone"></i><span class="">Contact</span></a>
</li>
  </ul>
</li>
    </ul>
  </nav>
  <form class="td-sidebar__search d-flex align-items-center">
    



<input
  type="search"
  class="form-control td-search-input"
  placeholder="&#xf002; Search this site…"
  aria-label="Search this site…"
  autocomplete="off"
  
  data-offline-search-index-json-src="/offline-search-index.df2e0aba9d72e7517b93a6c8064ad1be.json"
  data-offline-search-base-href="/"
  data-offline-search-max-results="10"
>


    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>
  </div>


          </aside>
          <aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none">
            
  
  
  
  
  
  
  <div class="td-page-meta ml-2 pb-1 pt-2 mb-0">
  
  
  </div>


            


<div class="td-toc"><nav id="TableOfContents">
  <ul>
    <li><a href="#the-impact-of-tying-freezing-and-pretraining">The impact of tying, freezing, and pretraining</a></li>
    <li><a href="#citation">Citation</a></li>
    <li><a href="#tables-in-written-form">Tables in written form</a>
      <ul>
        <li><a href="#table-with-training-variations">Table with training variations</a></li>
        <li><a href="#final-results-table">Final results table</a></li>
      </ul>
    </li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
  </ul>
</nav></div>



            

	
		
			
		
		



  
  

	
		
			
		
		



  
  

	

          </aside>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            
  

            

            
<div class="td-content">
	<h1>Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)</h1>
	<div class="lead">This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 50 million+ words of text, but only 1 GPU for training?</div>
	<header class="article-meta">
		
		
			
				


			
				


			
		
		
	</header>    
	<p>This paper explores two questions.
First, what is the impact of a few key design decisions for word embeddings in language models?
Second, based on the first answer, how can we improve results in the situation where we have 10 million+ words of text, but only 1 GPU for training?</p>
<h2 id="the-impact-of-tying-freezing-and-pretraining">The impact of tying, freezing, and pretraining</h2>
<p>It is standard practise to tie the input and output embeddings of language models (i.e., use the same weights in both places), training them together and initialising them randomly.
Several papers have shown that this improves results by providing more frequent updates to the input embeddings.
But if you have data available for pretraining it is less clear that this is the right approach.
To explore this I&rsquo;m going to use a few symbols:</p>
<p><img src="/img/post/pretraining-lm-vary-key.jpg" alt="Key for Variations on LM table"></p>
<p>Here are the results of training an <a href="https://github.com/salesforce/awd-lstm-lm">AWD-LSTM</a> with all variations of these parameters, evaluated on the standard LM development set of the PTB (Std) and a variation that has actual words instead of unk (Rare):</p>
<p><img src="/img/post/pretraining-lm-vary.jpg" alt="Variations on LM (for written form of table see bottom of page)"></p>
<p>Light blue shows the standard configuration and light red shows our proposal.
The table is ranked by performance on Std and has four clear sections:</p>
<p>(a) Frozen random output embeddings.</p>
<p>(b) Frozen pretrained output embeddings.</p>
<p>(c) Frozen random input embeddings.</p>
<p>(d) Various configurations.</p>
<p>I was surprised by the dramatic difference between input and output embeddings here.
Freezing the output embeddings, even with a good embedding space, leads to terrible performance.
In contrast, freezing input embeddings is fine if they are pretrained, and has a far smaller impact when they are random.</p>
<p>Evaluating with rare words, the big picture is mostly the same, but pretraining has a bigger impact.
One interesting difference is that the top five models all use pretrained input embeddings, with a large gap from there to the next results.
At the same time, pretraining the output embeddings seems to have only a small impact (when holding all other variables fixed).
Finally, the best results freeze the input embeddings.
Our explanation is that embeddings become inconsistent when they aren&rsquo;t frozen.
The vectors for words in the training set are moved but the ones seen only in pretraining stay where they are, leading to an inconsistent embedding space.</p>
<p>The paper then goes through a series of experiments to explore this, varying data domain, similarity of pretraining data, and more.
Here I&rsquo;m going to jump straight to the final results.
The table below considers a dataset with 43 million in-domain tokens for pretraining and 7 million for LM training.
The other models are the standard AWD-LSTM, an n-gram language model, and two version of GPT-2 (without finetuning):</p>
<p><img src="/img/post/pretraining-final.jpg" alt="Final results (for written form of table see bottom of page)"></p>
<p>For word level prediction perplexity is reduced by 4.
However, if we train and test with BPE there is no improvement (see the <a href="https://arxiv.org/abs/1911.11423">SHA-RNN paper</a> for some issues with comparing BPE and word evaluation).
So if your application works with BPE this finding isn&rsquo;t useful, but for word-level modeling it probably is.</p>
<p>A few notes about this work:</p>
<ul>
<li>A natural next step would be to explore ways to train the language model with more data.
Modifying the AWD-LSTM code to support training sets larger than GPU memory could render pretraining unnecessary (though at the cost of much longer training).
In some experiments (not in the paper), we found that when the pretraining set and training set were the same, pretraining didn&rsquo;t improve performance, but it did speed up training.</li>
<li>Properties of evaluation datasets have shaped the direction of work on language modeling.
It&rsquo;s important to think beyond the hyperparameters that are easy to vary (e.g., hidden vector dimensions) when adapting a model for a new scenario.</li>
<li>Writing robust research code is hard.
We tried getting several other models to run with our variations, but going beyond reproducing results to actually modifying code proved hard.
Even for the AWD-LSTM, we failed to reproduce results except when we went back to one of the earliest releases.</li>
<li>This paper was saved by author response.
The initial reviews were 3.5, 2.5, 3.5 and based on the response and reviewer discussion the 2.5 went to a 4.
The response contained answers to reviewer questions, including a bunch of statistics about the data that are now in the final paper.
I have always been a fan of author response.
It can lead to more informed acceptance decisions and more useful feedback to authors.
To achieve that, both authors and reviewers need to engage with it though.
In particular, reviewers need to give something of substance to be responded to and they need to carefully read and consider the response.</li>
</ul>
<h2 id="citation">Citation</h2>
<p><a href="https://www.jkk.name/pub/emnlp20lm.pdf">Paper</a></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bibtex" data-lang="bibtex"><span style="color:#000">@InProceedings</span><span style="color:#000;font-weight:bold">{</span><span style="color:#f57900">emnlp20lm</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#c4a000">title</span>     <span style="color:#000;font-weight:bold">=</span> <span style="color:#4e9a06">{Improving Low Compute Language Modeling with In-Domain Embedding Initialisation}</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#c4a000">title:</span> <span style="color:#000;font-weight:bold">=</span> <span style="color:#4e9a06">{Improving Low Compute Language Modeling with In-Domain Embedding Initialisation}</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#c4a000">author</span>    <span style="color:#000;font-weight:bold">=</span> <span style="color:#4e9a06">{Welch, Charles and Mihalcea, Rada and Kummerfeld, Jonathan K.}</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#c4a000">booktitle</span> <span style="color:#000;font-weight:bold">=</span> <span style="color:#4e9a06">{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#c4a000">month</span>     <span style="color:#000;font-weight:bold">=</span> <span style="color:#4e9a06">{November}</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#c4a000">year</span>      <span style="color:#000;font-weight:bold">=</span> <span style="color:#4e9a06">{2020}</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#c4a000">url</span>       <span style="color:#000;font-weight:bold">=</span> <span style="color:#4e9a06">{https://www.jkk.name/pub/emnlp20lm.pdf}</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#c4a000">abstract</span>  <span style="color:#000;font-weight:bold">=</span> <span style="color:#4e9a06">{Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.}</span><span style="color:#000;font-weight:bold">,</span>
<span style="color:#000;font-weight:bold">}</span>
</code></pre></div><h2 id="tables-in-written-form">Tables in written form</h2>
<h3 id="table-with-training-variations">Table with training variations</h3>
<p>Each section is presented separately below, with the model described using five words followed by the result on the standard data and the result on the data with rare words.</p>
<p>First section:</p>
<ul>
<li>tied   frozen   dice  frozen   dice, 680, 1120</li>
<li>untied frozen   dice  frozen   dice, 680, 1120</li>
<li>untied unfrozen dice  frozen   dice, 680, 431</li>
<li>untied unfrozen train frozen   dice, 220, 372</li>
<li>untied frozen   train frozen   dice, 218, 360</li>
</ul>
<p>Second section:</p>
<ul>
<li>untied frozen   dice  frozen   train, 121, 202</li>
<li>untied unfrozen dice  frozen   train, 95.0, 170</li>
<li>untied unfrozen train frozen   train, 91.3, 147</li>
<li>tied   frozen   train frozen   train, 90.7, 136</li>
<li>untied frozen   train frozen   train, 90.7, 136</li>
</ul>
<p>Third section:</p>
<ul>
<li>untied frozen   dice  unfrozen dice, 82.2, 143</li>
<li>untied frozen   dice  unfrozen train, 81.4, 142</li>
</ul>
<p>Fourth section:</p>
<ul>
<li>untied unfrozen dice  unfrozen dice, 65.3, 120</li>
<li>untied unfrozen dice  unfrozen train, 64.1, 113</li>
<li>untied unfrozen train unfrozen dice, 62.5, 105</li>
<li>untied unfrozen train unfrozen train, 61.7, 98.5</li>
<li>untied frozen   train unfrozen train, 61.6, 97.1</li>
<li>tied   unfrozen dice  unfrozen dice, 61.3, 112</li>
<li>untied frozen   train unfrozen dice, 61.1, 98.1</li>
<li>tied   unfrozen train unfrozen train, 59.8, 98.7</li>
</ul>
<h3 id="final-results-table">Final results table</h3>
<p>Models with word level evaluation, giving development results then test results:</p>
<ul>
<li>N-Gram, 92.3, 95.0</li>
<li>Baseline AWD-LSTM, 52.8, 53.5</li>
<li>Our approach, 49.0, 49.4</li>
</ul>
<p>Models with BPE evaluation:</p>
<ul>
<li>N-Gram, 56.7, 55.3</li>
<li>GPT-2 (112m), 46.4, 43.8</li>
<li>Baseline AWD-LSTM, 37.8, 36.7</li>
<li>Our approach, 38.3, 37.2</li>
<li>GPT-2 (774m), 32.5, 33.7</li>
</ul>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Dice Icon by Andrew Doane from the Noun Project.
Fire and Snowflake Icons by Freepik from <a href="http://www.flaticon.com">www.flaticon.com</a>.</p>

	
	
	
</div>


          </main>
        </div>
      </div>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Email" aria-label="Email">
    <a class="text-white" target="_blank" rel="noopener" href="mailto:jkummerf@umich.edu" aria-label="Email">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener" href="https://twitter.com/jkkummerfeld" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="LinkedIn" aria-label="LinkedIn">
    <a class="text-white" target="_blank" rel="noopener" href="https://linkedin.com/in/jkkummerfeld/" aria-label="LinkedIn">
      <i class="fab fa-linkedin"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/jkkummerfeld" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="ACL Anthology" aria-label="ACL Anthology">
    <a class="text-white" target="_blank" rel="noopener" href="https://aclanthology.org/anthology/people/j/jonathan-k-kummerfeld/" aria-label="ACL Anthology">
      <i class="icon icon-acl-logo"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Semantic Scholar" aria-label="Semantic Scholar">
    <a class="text-white" target="_blank" rel="noopener" href="https://semanticscholar.org/author/Jonathan-K.-Kummerfeld/1727211" aria-label="Semantic Scholar">
      <i class="ai ai-semantic-scholar"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Google Scholar" aria-label="Google Scholar">
    <a class="text-white" target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=OsoNG9AAAAAJ" aria-label="Google Scholar">
      <i class="ai ai-google-scholar"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="DBLP" aria-label="DBLP">
    <a class="text-white" target="_blank" rel="noopener" href="https://dblp.uni-trier.de/pers/hd/k/Kummerfeld:Jonathan_K=" aria-label="DBLP">
      <i class="ai ai-dblp"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="ArXiv" aria-label="ArXiv">
    <a class="text-white" target="_blank" rel="noopener" href="https://arxiv.org/a/kummerfeld_j_1" aria-label="ArXiv">
      <i class="ai ai-arxiv"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="OrcID" aria-label="OrcID">
    <a class="text-white" target="_blank" rel="noopener" href="https://orcid.org/0000-0001-5030-3016" aria-label="OrcID">
      <i class="ai ai-orcid"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2022 Jonathan K. Kummerfeld All Rights Reserved</small>
        
	
		
	
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>





<script src='/js/tabpane-persist.js'></script>




 

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.css" integrity="sha384-Cqd8ihRLum0CCg8rz0hYKPoLZ3uw+gES2rXQXycqnL5pgVQIflxAUDS7ZSjITLb5" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.js" integrity="sha384-1Or6BdeNQb0ezrmtGeqQHFpppNd7a/gw29xeiSikBbsb44xu3uAo8c7FwbF5jhbd" crossorigin="anonymous"></script>
 


<script defer src='https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/contrib/auto-render.min.js' integrity='sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl' crossorigin='anonymous' onload='renderMathInElement(document.body, null);'></script>














<script src="/js/main.min.3b172c13b62c2bea8b1c9d2599cddc8cf89718a92d792c680871c81ba43d8c85.js" integrity="sha256-OxcsE7YsK&#43;qLHJ0lmc3cjPiXGKkteSxoCHHIG6Q9jIU=" crossorigin="anonymous"></script>




  </body>
</html>
