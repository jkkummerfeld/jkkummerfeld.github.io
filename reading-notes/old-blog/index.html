<!doctype html>
<html lang="en" class="no-js">
  <head>
    <link rel="stylesheet" href="https://www.jkk.name/plugins/my-custom/css/mycustom.css"><link rel="stylesheet" href="https://www.jkk.name/plugins/academic-icons/css/academicons.css">
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.89.3" /><link rel="alternate" type="application/rss&#43;xml" href="https://www.jkk.name/reading-notes/old-blog/index.xml">
<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>Old Blog Posts | Jonathan K. Kummerfeld</title>


  <meta name="description" content="These are blog posts from my old website.
">
<meta property="og:title" content="Old Blog Posts" />
<meta property="og:description" content="These are blog posts from my old website.
" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://www.jkk.name/reading-notes/old-blog/" /><meta property="og:site_name" content="Jonathan K. Kummerfeld" />

<meta itemprop="name" content="Old Blog Posts">
<meta itemprop="description" content="These are blog posts from my old website.
"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Old Blog Posts"/>
<meta name="twitter:description" content="These are blog posts from my old website.
"/>




<link rel="preload" href="/scss/main.min.098e976d5113d2dd5db369ca3e1b98f01c9e6e4acb8102f6b9902a4826c64e81.css" as="style">
<link href="/scss/main.min.098e976d5113d2dd5db369ca3e1b98f01c9e6e4acb8102f6b9902a4826c64e81.css" rel="stylesheet" integrity="">


<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>

<script
  src="https://unpkg.com/lunr@2.3.8/lunr.min.js"
  integrity="sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY"
  crossorigin="anonymous"></script>







<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-19179423-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>




  </head>
  <body class="td-section">
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
            
<div id="td-sidebar-menu" class="td-sidebar__inner">
  <nav class="collapse td-sidebar-nav" id="td-section-nav">
    <ul class="td-sidebar-nav__section pr-md-3 ul-0">
      <li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m--li">
  <a href="/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section tree-root" id="m-"><span class="">Jonathan K. Kummerfeld&#39;s Homepage</span></a>
  <ul class="ul-1">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-about-li">
  <a href="/about/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-about"><i class="fas fa-male"></i><span class="">About</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-students-li">
  <a href="/students/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-students"><i class="fas fa-users"></i><span class="">Students</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-studentsrecruiting-phd-li">
  <a href="/students/recruiting-phd/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-studentsrecruiting-phd"><span class="">Recruiting: PhD</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-pubs-li">
  <a href="/pubs/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-pubs"><i class="fas fa-file-alt"></i><span class="">Publications</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-teaching-li">
  <a href="/teaching/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-teaching"><i class="fas fa-graduation-cap"></i><span class="">Teaching</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-software-li">
  <a href="/software/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-software"><i class="fas fa-code"></i><span class="">Software</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-data-li">
  <a href="/data/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-data"><i class="fas fa-database"></i><span class="">Datasets</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m-reading-notes-li">
  <a href="/reading-notes/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-reading-notes"><i class="fas fa-book-reader"></i><span class="">Reading Notes</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m-reading-notesold-blog-li">
  <a href="/reading-notes/old-blog/" title="Old Blog Posts" class="align-left pl-0 active td-sidebar-link td-sidebar-link__section" id="m-reading-notesold-blog"><span class="td-sidebar-nav-active-item">Olg Blog Posts</span></a>
  <ul class="ul-3 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-09_parsing-autoencoder-li">
  <a href="/reading-notes/old-blog/2017-10-09_parsing-autoencoder/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-09_parsing-autoencoder"><span class=""> Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-05_explainingpredictions-li">
  <a href="/reading-notes/old-blog/2017-12-05_explainingpredictions/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-05_explainingpredictions"><span class="">A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-16_forumrnn-li">
  <a href="/reading-notes/old-blog/2017-10-16_forumrnn/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-16_forumrnn"><span class="">A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2019-07-10_disentanglement-li">
  <a href="/reading-notes/old-blog/2019-07-10_disentanglement/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2019-07-10_disentanglement"><span class="">A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-01_nonsequencener-li">
  <a href="/reading-notes/old-blog/2017-12-01_nonsequencener/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-01_nonsequencener"><span class="">A Local Detection Approach for Named Entity Recognition and Mention Detection (Xu et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-10-04_crowdsrl-li">
  <a href="/reading-notes/old-blog/2020-10-04_crowdsrl/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-04_crowdsrl"><span class="">A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-12_multidomainwordvector-li">
  <a href="/reading-notes/old-blog/2017-12-12_multidomainwordvector/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-12_multidomainwordvector"><span class="">A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings (Yang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-16_ucca-li">
  <a href="/reading-notes/old-blog/2017-11-16_ucca/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-16_ucca"><span class="">A Transition-Based Directed Acyclic Graph Parser for UCCA (Hershcovich et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-17_twostagediscourseparsing-li">
  <a href="/reading-notes/old-blog/2017-11-17_twostagediscourseparsing/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-17_twostagediscourseparsing"><span class="">A Two-Stage Parsing Method for Text-Level Discourse Analysis (Wang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-29_abstractivesummarisation-li">
  <a href="/reading-notes/old-blog/2017-11-29_abstractivesummarisation/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-29_abstractivesummarisation"><span class="">Abstractive Document Summarization with a Graph-Based Attentional Neural Model (Tan et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-18_neuralamr-li">
  <a href="/reading-notes/old-blog/2017-10-18_neuralamr/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-18_neuralamr"><span class="">Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-04-16_lm_analysis-li">
  <a href="/reading-notes/old-blog/2018-04-16_lm_analysis/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-04-16_lm_analysis"><span class="">An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2019-09-22_conferenceapproach-li">
  <a href="/reading-notes/old-blog/2019-09-22_conferenceapproach/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2019-09-22_conferenceapproach"><span class="">Approaching Conferences</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-07_spineparsinglstm-li">
  <a href="/reading-notes/old-blog/2017-11-07_spineparsinglstm/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-07_spineparsinglstm"><span class="">Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-20_onlyattention-li">
  <a href="/reading-notes/old-blog/2017-10-20_onlyattention/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-20_onlyattention"><span class="">Attention Is All You Need (Vaswani et al., ArXiv 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-21_multiinputattention-li">
  <a href="/reading-notes/old-blog/2017-11-21_multiinputattention/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-21_multiinputattention"><span class="">Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-09-03_checklist-li">
  <a href="/reading-notes/old-blog/2020-09-03_checklist/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-03_checklist"><span class="">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-09-07_chartdialogs-li">
  <a href="/reading-notes/old-blog/2020-09-07_chartdialogs/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-07_chartdialogs"><span class="">ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-10-10_demographicembeddings-li">
  <a href="/reading-notes/old-blog/2020-10-10_demographicembeddings/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-10_demographicembeddings"><span class="">Compositional Demographic Word Embeddings (Welch et al., EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-09-25_crowdqasrl-li">
  <a href="/reading-notes/old-blog/2020-09-25_crowdqasrl/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-25_crowdqasrl"><span class="">Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-05-deftnn-li">
  <a href="/reading-notes/old-blog/2017-10-05-deftnn/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-05-deftnn"><span class="">DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-13_errordetection-li">
  <a href="/reading-notes/old-blog/2017-10-13_errordetection/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-13_errordetection"><span class="">Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-30_neuralsequence-li">
  <a href="/reading-notes/old-blog/2017-10-30_neuralsequence/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-30_neuralsequence"><span class="">Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-24_dynamictransition-li">
  <a href="/reading-notes/old-blog/2017-10-24_dynamictransition/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-24_dynamictransition"><span class="">Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-22_errorrepairparsing-li">
  <a href="/reading-notes/old-blog/2017-11-22_errorrepairparsing/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-22_errorrepairparsing"><span class="">Error-repair Dependency Parsing for Ungrammatical Texts (Sakaguchi et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-09-04_featureengineering-li">
  <a href="/reading-notes/old-blog/2018-09-04_featureengineering/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-09-04_featureengineering"><span class="">Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-01-28_crowdassistant-li">
  <a href="/reading-notes/old-blog/2018-01-28_crowdassistant/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-01-28_crowdassistant"><span class="">Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-06-12_parseradaptation-li">
  <a href="/reading-notes/old-blog/2018-06-12_parseradaptation/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-06-12_parseradaptation"><span class="">Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples (Vidur Joshi et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-06-madlibs-li">
  <a href="/reading-notes/old-blog/2017-10-06-madlibs/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-06-madlibs"><span class="">Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-09_framesdataset-li">
  <a href="/reading-notes/old-blog/2017-11-09_framesdataset/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-09_framesdataset"><span class="">Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-12_amralignment-li">
  <a href="/reading-notes/old-blog/2017-10-12_amralignment/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-12_amralignment"><span class="">Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-11_multimt-li">
  <a href="/reading-notes/old-blog/2017-10-11_multimt/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-11_multimt"><span class="">Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-07_rarewordvectors-li">
  <a href="/reading-notes/old-blog/2017-12-07_rarewordvectors/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-07_rarewordvectors"><span class="">High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-12-10_dynamicannoallocation-li">
  <a href="/reading-notes/old-blog/2020-12-10_dynamicannoallocation/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-12-10_dynamicannoallocation"><span class="">Improving Human-Labeled Data through Dynamic Automatic Conflict Resolution (Sun, et al., CoLing 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-09-29_pretraininglm-li">
  <a href="/reading-notes/old-blog/2020-09-29_pretraininglm/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-29_pretraininglm"><span class="">Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-14_inorderparsing-li">
  <a href="/reading-notes/old-blog/2017-11-14_inorderparsing/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-14_inorderparsing"><span class="">In-Order Transition-based Constituent Parsing (Liu et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2020-10-12_taboo-li">
  <a href="/reading-notes/old-blog/2020-10-12_taboo/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-12_taboo"><span class="">Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-30_taggingrelations-li">
  <a href="/reading-notes/old-blog/2017-11-30_taggingrelations/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-30_taggingrelations"><span class="">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme (Zheng et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-03_discourserelations-li">
  <a href="/reading-notes/old-blog/2017-11-03_discourserelations/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-03_discourserelations"><span class="">Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-15_entityvectors-li">
  <a href="/reading-notes/old-blog/2017-11-15_entityvectors/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-15_entityvectors"><span class="">Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-08_graphdialogue-li">
  <a href="/reading-notes/old-blog/2017-11-08_graphdialogue/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-08_graphdialogue"><span class="">Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2018-03-05_curriculum-li">
  <a href="/reading-notes/old-blog/2018-03-05_curriculum/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-03-05_curriculum"><span class="">Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning (Tsvetkov et al., 2016)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-19_mace-li">
  <a href="/reading-notes/old-blog/2017-10-19_mace/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-19_mace"><span class="">Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-10_kginlstm-li">
  <a href="/reading-notes/old-blog/2017-11-10_kginlstm/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-10_kginlstm"><span class="">Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-23_alphagozero-li">
  <a href="/reading-notes/old-blog/2017-10-23_alphagozero/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-23_alphagozero"><span class="">Mastering the game of Go without human knowledge (Silver et al., Nature 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-06_literarycharacters-li">
  <a href="/reading-notes/old-blog/2017-11-06_literarycharacters/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-06_literarycharacters"><span class="">Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-26_multimodalwordembeddings-li">
  <a href="/reading-notes/old-blog/2017-10-26_multimodalwordembeddings/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-26_multimodalwordembeddings"><span class="">Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-10-17_nedisambiguation-li">
  <a href="/reading-notes/old-blog/2017-10-17_nedisambiguation/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-17_nedisambiguation"><span class="">Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-11-13_languagegame-li">
  <a href="/reading-notes/old-blog/2017-11-13_languagegame/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-13_languagegame"><span class="">Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog (Kottur et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesold-blog2017-12-05_multidomainparsing-li">
  <a href="/reading-notes/old-blog/2017-12-05_multidomainparsing/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-05_multidomainparsing"><span class="">Neural Semantic Parsing over Multiple Knowledge-bases (Herzig et al., 2017)</span></a>
</li>
  </ul>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-advice-li">
  <a href="/advice/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-advice"><i class="fas fa-comment"></i><span class="">Advice</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-adviceposters-li">
  <a href="/advice/posters/" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id="m-adviceposters"><span class="">Academic Posters</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sponsor-li">
  <a href="/sponsor/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-sponsor"><i class="fas fa-dollar-sign"></i><span class="">Sponsor</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-contact-li">
  <a href="/contact/" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id="m-contact"><i class="fas fa-phone"></i><span class="">Contact</span></a>
</li>
  </ul>
</li>
    </ul>
  </nav>
  <form class="td-sidebar__search d-flex align-items-center">
    



<input
  type="search"
  class="form-control td-search-input"
  placeholder="&#xf002; Search this site…"
  aria-label="Search this site…"
  autocomplete="off"
  
  data-offline-search-index-json-src="/offline-search-index.c01bb2a4dc353e8cbe7796929323eb9b.json"
  data-offline-search-base-href="/"
  data-offline-search-max-results="10"
>


    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>
  </div>


          </aside>
          <aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none">
            
  
  
  
  
  
  
  <div class="td-page-meta ml-2 pb-1 pt-2 mb-0">
  
  
  </div>


            




            

	
		
			
		
		



  
  

	
		
			
		
		



  
  

	

          </aside>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            
  

            

            
<div class="td-content">
  
	<header class="article-meta">
		
		
			
				


			
				


			
		
		
	</header>
        <div class="section-index">
    
    
    
    
    
    
    
    
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-09_parsing-autoencoder/"> Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)</a>
                </h5>
                <p>By training a parser and language generation system together, we can use semantic parses without associated sentences for training (the sentence becomes a latent representation that is being learnt).</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-12-05_explainingpredictions/">A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)</a>
                </h5>
                <p>To explain structured outputs in terms of which inputs have most impact, treat it as identifying components in a bipartite graph where weights are determined by perturbing the input and observing the impact on outputs.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-16_forumrnn/">A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)</a>
                </h5>
                <p>A proposal for how to improve vector representations of sentences by using attention over (1) fixed vectors, and (2) a context sentence.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2019-07-10_disentanglement/">A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)</a>
                </h5>
                <p></p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-12-01_nonsequencener/">A Local Detection Approach for Named Entity Recognition and Mention Detection (Xu et al., 2017)</a>
                </h5>
                <p>Effective NER can be achieved without sequence prediction using a feedforward network that labels every span with a fixed attention mechanism for getting contextual information.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-10-04_crowdsrl/">A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)</a>
                </h5>
                <p>My <a href="https://www.jkk.name/post/2020-09-25_crowdqasrl/">previous post</a> discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions. This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-12-12_multidomainwordvector/">A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings (Yang et al., 2017)</a>
                </h5>
                <p>To leverage out-of-domain data, learn multiple sets of word vectors but with a loss term that encourages them to be similar.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-16_ucca/">A Transition-Based Directed Acyclic Graph Parser for UCCA (Hershcovich et al., 2017)</a>
                </h5>
                <p>Parsing performance on the semantic structures of UCCA can be boosted by using a transition system that combines ideas from discontinuous and constituent transition systems, covering the full space of structures.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-17_twostagediscourseparsing/">A Two-Stage Parsing Method for Text-Level Discourse Analysis (Wang et al., 2017)</a>
                </h5>
                <p>Breaking discourse parsing into separate relation identification and labeling tasks can boost performance (by dealing with limited training data).</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-29_abstractivesummarisation/">Abstractive Document Summarization with a Graph-Based Attentional Neural Model (Tan et al., 2017)</a>
                </h5>
                <p>Neural abstractive summarisation can be dramatically improved with a beam search that favours output that matches the source document, and further improved with attention based on PageRank, with a modification to avoid attending to the same sentence more than once.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-18_neuralamr/">Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)</a>
                </h5>
                <p>Another paper looking at the issue of output symbol sparsity in AMR parsing, though here the solution is to group the consistent but rare symbols (rather than graph fragments like the paper last week). This drastically increases neural model performance, but does not reach the level of hybrid systems.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-04-16_lm_analysis/">An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)</a>
                </h5>
                <p>Assigning a probability distribution over the next word or character in a sequence (language modeling) is a useful component of many systems&hellip;</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2019-09-22_conferenceapproach/">Approaching Conferences</a>
                </h5>
                <p>Am I getting the most our of time at conferences? This post was a way for me to think through that question and come up with strategies.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-07_spineparsinglstm/">Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)</a>
                </h5>
                <p>Stack-LSTM models for dependency parsing can be adapted to constituency parsing by considering spinal version of the parse and adding a single &lsquo;create-node&rsquo; operation to the transition-based parsing scheme, giving an elegant algorithm and competitive results.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-20_onlyattention/">Attention Is All You Need (Vaswani et al., ArXiv 2017)</a>
                </h5>
                <p>To get context-dependence without recurrence we can use a network that applies attention multiple times over both input and output (as it is generated).</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-21_multiinputattention/">Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)</a>
                </h5>
                <p>To apply attention across multiple input sources, it is best to apply attention independently and then have a second phase of attention over the summary vectors for each source.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-09-03_checklist/">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)</a>
                </h5>
                <p>It is difficult to predict how well a model will work in the real world. Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to. This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories. Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-09-07_chartdialogs/">ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)</a>
                </h5>
                <p>Natural language interfaces to computer systems are an exciting area with new workshops (<a href="https://aclanthology.org/volumes/2020.nli-1/">WNLI</a> at ACL and <a href="https://intex-sempar.github.io/">IntEx-SemPar</a> at EMNLP), a range of datasets (including my own work on <a href="/publication/acl18sql/">text-to-SQL</a>), and many papers. Most work focuses on either (1) commands for simple APIs, (2) generating a database query, or (3) generating general purpose code. This paper considers an interesting application: interaction with data visualisation tools.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-10-10_demographicembeddings/">Compositional Demographic Word Embeddings (Welch et al., EMNLP 2020)</a>
                </h5>
                <p>Most work in NLP uses datasets with a diverse set of speakers. In practise, everyone speaks / writes slightly differently and our models would be better if they accounted for that. This has been the motivation for a line of work by <a href="http://cfwelch.com/">Charlie Welch</a> that I&rsquo;ve been a collaborator on (in <a href="https://www.jkk.name/publication/cicling19personal">CICLing 2019</a>, <a href="https://www.jkk.name/publication/ieee19personal/">IEEE Intelligent Systems 2019</a>, <a href="https://www.jkk.name/publication/coling20personal/">CoLing 2020</a>, and this paper).</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-09-25_crowdqasrl/">Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)</a>
                </h5>
                <p>Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments.  Over the last few years, <a href="https://www.cs.washington.edu/people/faculty/lsz/">Luke Zettlemoyer&rsquo;s Group</a> has been exploring using question-answer pairs to represent this structure.  This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank.  However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-05-deftnn/">DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)</a>
                </h5>
                <p>GPU processing can be sped up ~2x by removing low impact rows from weight matrices, and switching to a specialised floating point representation.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-13_errordetection/">Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)</a>
                </h5>
                <p>When labeling a dataset automatically there are going to be errors, but we can use a generative model and active learning to guide effort to checking the examples most likely to be incorrect.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-30_neuralsequence/">Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</a>
                </h5>
                <p>Language model perplexity can be reduced by maintaining a separate model that is updated during application of the model, allowing adaptation to short-term patterns in the text.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-24_dynamictransition/">Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)</a>
                </h5>
                <p>Transition based algorithms can be transformed into dynamic programs by defining sequences of actions that correspond to the same overall transformation.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-22_errorrepairparsing/">Error-repair Dependency Parsing for Ungrammatical Texts (Sakaguchi et al., 2017)</a>
                </h5>
                <p>Grammatical error correction can be improved by jointly parsing the sentence being corrected.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-09-04_featureengineering/">Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu et al., 2018)</a>
                </h5>
                <p>A common argument in favour of neural networks is that they do not require &lsquo;feature engineering&rsquo;, manually defining functions that produce useful representations of the input data (e.g. a function&hellip;</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-01-28_crowdassistant/">Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</a>
                </h5>
                <p>For a more flexible dialogue system, use the crowd to propose and vote on responses, then introduce agents and a model for voting, gradually learning to replace the crowd.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-06-12_parseradaptation/">Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples (Vidur Joshi et al., 2018)</a>
                </h5>
                <p>Virtually all systems trained using data have trouble when applied to datasets that differ even slightly - even switching from Wall Street&hellip;</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-06-madlibs/">Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)</a>
                </h5>
                <p>A new task and associated evaluation method plus system for Mad Libs - filling in missing words in a story in a funny way. While the system does poorly, using it as a first pass with human rerankers produces funnier stories than people alone.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-09_framesdataset/">Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</a>
                </h5>
                <p>A new dialogue dataset that has annotations of multiple plans (frames) and dialogue acts that indicate modifications to them.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-12_amralignment/">Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)</a>
                </h5>
                <p>Two ideas for improving AMR parsing: (1) take graph distance into consideration when generating alignments, (2) during parsing, for concept generation, generate individual concepts in some cases and frequently occurring subgraphs in other cases.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-11_multimt/">Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)</a>
                </h5>
                <p>A translation model trained on sentence pairs from a mixture of languages can do very well across all of the languages, and even generalise somewhat to new pairs of the languages. That&rsquo;s useful as one model can do the work of $O(n^2)$ models, and with a fraction of the parameters.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-12-07_rarewordvectors/">High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)</a>
                </h5>
                <p>The simplest way to learn word vectors for rare words is to average their context. Tweaking word2vec to make greater use of the context may do slightly better, but it&rsquo;s unclear.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-12-10_dynamicannoallocation/">Improving Human-Labeled Data through Dynamic Automatic Conflict Resolution (Sun, et al., CoLing 2020)</a>
                </h5>
                <p></p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-09-29_pretraininglm/">Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)</a>
                </h5>
                <p>This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 50 million+ words of text, but only 1 GPU for training?</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-14_inorderparsing/">In-Order Transition-based Constituent Parsing (Liu et al., 2017)</a>
                </h5>
                <p>Using in-order traversal for transition based parsing (put the non-terminal on the stack after its first child but before the rest) is consistently better than pre-order / top-down or post-order / bottom-up traversal.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-10-12_taboo/">Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)</a>
                </h5>
                <p>When we crowdsource data for tasks like SRL and sentiment analysis we only care about accuracy.  For tasks where workers write new content, such as paraphrasing and creating questions, we also care about data diversity.  If our data is not diverse then models trained on it will not be robust in the real world.  The core idea of this paper is to encourage creativity by constraining workers.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-30_taggingrelations/">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme (Zheng et al., 2017)</a>
                </h5>
                <p>By encoding the relation type and role of each word in tags, an LSTM can be applied to relation extraction with great success.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-03_discourserelations/">Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)</a>
                </h5>
                <p>Identifying the key phrases in a dialogue at the same time as identifying the type of relations between pairs of utterances leads to substantial improvements on both tasks.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-15_entityvectors/">Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)</a>
                </h5>
                <p>Vectors for words and entities can be learned by trying to model the text written about the entities. This leads to word vectors that score well on similarity tasks and entity vectors that produce excellent results on entity linking and question answering.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-08_graphdialogue/">Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)</a>
                </h5>
                <p>During task-oriented dialogue generation, to take into consideration a table of information about entities, represent it as a graph, run message passing to get vector representations of each entity, and use attention.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-03-05_curriculum/">Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning (Tsvetkov et al., 2016)</a>
                </h5>
                <p>Reordering training sentences for word vectors may impact their usefulness for downstream tasks.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-19_mace/">Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</a>
                </h5>
                <p>By using a generative model to explain worker annotations, we can more effectively predict the correct label, and which workers are spamming.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-10_kginlstm/">Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)</a>
                </h5>
                <p>Incorporating vector representations of entities from structured resources like NELL and WordNet into the output of an LSTM can improve entity and event extraction.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-23_alphagozero/">Mastering the game of Go without human knowledge (Silver et al., Nature 2017)</a>
                </h5>
                <p>By using a single core model to build a game state representation, which then gives input to both state evaluation and move choice, DeepMind are able to apply reinforcement learning with self-play with no supervision and achieve state-of-the-art performance.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-06_literarycharacters/">Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)</a>
                </h5>
                <p>With some tweaks (domain-specific heuristics), coreference systems can be used to identify the set of characters in a novel, which in turn can be used to do large scale tests of hypotheses from literary analysis.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-26_multimodalwordembeddings/">Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)</a>
                </h5>
                <p>By switching from representing words as points in a vector space to multiple gaussian regions we can get a better model, scoring higher on multiple word similarity metrics than a range of techniques.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-17_nedisambiguation/">Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)</a>
                </h5>
                <p>The WikiLinks dataset of text mentions that are hyperlinked to wikipedia articles provides a nice testing space for named entity disambiguation, and a neural network using attention over local context does reasonably well.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-13_languagegame/">Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog (Kottur et al., 2017)</a>
                </h5>
                <p>Constraining the language of a dialogue agent can improve performance by encouraging the use of more compositional language.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-12-05_multidomainparsing/">Neural Semantic Parsing over Multiple Knowledge-bases (Herzig et al., 2017)</a>
                </h5>
                <p>Training a single parser on multiple domains can improve performance, and sharing more parameters (encoder and decoder as opposed to just one) seems to help more.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2019-09-13_diplomacynopress/">No-Press Diplomacy: Modeling Multi-Agent Gameplay (Paquette et al., 2019)</a>
                </h5>
                <p>Games have been a focus of AI research for decades, from Samuel&rsquo;s checkers program in the 1950s, to Deep Blue playing Chess in the 1990s, and AlphaGo playing Go in the 2010s. All of those are two-player&hellip;</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-27_commonsense/">Ordinal Common-sense Inference (Zhang et al., 2017)</a>
                </h5>
                <p>A new task and dataset of 39k examples for common sense reasoning, with a sentence generated for each prompt and a manual label indicating their relation, from very likely to impossible.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-01_mixtureofexperts/">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Shazeer et al., 2017)</a>
                </h5>
                <p>Neural networks for language can be scaled up by using a form of selective computation, where a noisy single-layer model chooses among feed-forward networks (experts) that sit between LSTM layers.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2020-09-17_activelearningbrittle/">Practical Obstacles to Deploying Active Learning (Lowell, et al., EMNLP 2019)</a>
                </h5>
                <p>Training models requires massive amounts of labeled data.  We usually sample data iid from the target domain (e.g. newspapers), but it seems intuitive that this means we wast effort labeling samples that are obvious or easy and so not informative during training.  Active Learning follows that intuition, labeling data incrementally, selecting the next example(s) to label based on what a model considers uncertain.  Lots of work has shown this can be effective for that model, but if the labeled dataset is then used to train another model will it also do well?</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-11-08_corefdata/">PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution (Chen et al., 2018)</a>
                </h5>
                <p>The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset).  Some of these are discussed in&hellip;</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-03-08_sql/">Provenance for Natural Language Queries (Deutch et al., 2017)</a>
                </h5>
                <p>Being able to query a database in natural language could help make data accessible &hellip;</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-01-31_sentencerepfromparaphrases/">Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (Wieting et al., 2017)</a>
                </h5>
                <p>With enough training data, the best vector representation of a sentence is to concatenate an average over word vectors and an average over character trigram vectors.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-31_realtimecaptioning/">Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)</a>
                </h5>
                <p>By dividing a task up among multiple annotators carefully we can achieve high-quality real-time annotation of data, in this case transcription of audio.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-12-06_coreferencearguments/">Revisiting Selectional Preferences for Coreference Resolution (Heinzerling et al., 2017)</a>
                </h5>
                <p>It seems intuitive that a coreference system could benefit from information about what nouns a verb selects for, but experiments on explicitly adding a representation of it to a neural system does not lead to gains, implying it is already learning them or they are not useful.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-20_mrsparser/">Robust Incremental Neural Semantic Graph Parsing (Buys et al., 2017)</a>
                </h5>
                <p>A neural transition based parser with actions to create non-local links can perform well on Minimal Recursion Semantics parsing.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-10_seqqa/">Search-based Neural Structured Learning for Sequential Question Answering (Iyyer et al., ACL 2017)</a>
                </h5>
                <p>A new dataset containing multi-turn questions about a table, and a model that generates a kind of logical form, but scores actions based on the content of the table.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-27_swishactivation/">Searching for Activation Functions (Ramachandran et al., 2017)</a>
                </h5>
                <p>Switching from the ReLU non-linearity, $\text{max}(0, x)$, to Swish, $x \cdot \text{sigmoid}(x)$, consistently improves performance in neural networks across both vision and machine translation tasks.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-12-08_crowdbias/">Sequence Effects in Crowdsourced Annotations (Mathur et al., 2017)</a>
                </h5>
                <p>Annotator sequence bias, where the label for one item affects the label for the next, occurs across a range of datasets. Avoid it by separately randomise the order of items for each annotator.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-10-25_shiftreducedp/">Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice (Mi and Huang, 2015)</a>
                </h5>
                <p>An implementation of the transition-parsing as a dynamic program idea, leading to fast parsing and strong performance.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-11-28_interpretableembeddings/">SPINE: SParse Interpretable Neural Embeddings (Subramanian et al., 2017)</a>
                </h5>
                <p>By introducing a new loss that encourages sparsity, an auto-encoder can be used to go from existing word vectors to new ones that are sparser and more interpretable, though the impact on downstream tasks is mixed.</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2018-05-08_seq2seqsensitivity/">The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models (Weber et al., 2018)</a>
                </h5>
                <p>We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models&hellip;</p>
            </div>
        
            
            <div class="entry">
                <h5>
                    <a href="/reading-notes/old-blog/2017-12-12_wordvectorgeometry/">The strange geometry of skip-gram with negative sampling (Mimno et al., 2017)</a>
                </h5>
                <p>Surprisingly, word2vec (negative skipgram sampling) produces vectors that point in a consistent direction, a pattern not seen in GloVe (but also one that doesn&rsquo;t seem to cause a problem for downstream tasks).</p>
            </div>
        
    
</div>

	
	
	
	
</div>

          </main>
        </div>
      </div>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Email" aria-label="Email">
    <a class="text-white" target="_blank" rel="noopener" href="mailto:jkummerf@umich.edu" aria-label="Email">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener" href="https://twitter.com/jkkummerfeld" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="LinkedIn" aria-label="LinkedIn">
    <a class="text-white" target="_blank" rel="noopener" href="https://linkedin.com/in/jkkummerfeld/" aria-label="LinkedIn">
      <i class="fab fa-linkedin"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/jkkummerfeld" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="ACL Anthology" aria-label="ACL Anthology">
    <a class="text-white" target="_blank" rel="noopener" href="https://aclanthology.org/anthology/people/j/jonathan-k-kummerfeld/" aria-label="ACL Anthology">
      <i class="icon icon-acl-logo"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Semantic Scholar" aria-label="Semantic Scholar">
    <a class="text-white" target="_blank" rel="noopener" href="https://semanticscholar.org/author/Jonathan-K.-Kummerfeld/1727211" aria-label="Semantic Scholar">
      <i class="ai ai-semantic-scholar"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Google Scholar" aria-label="Google Scholar">
    <a class="text-white" target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=OsoNG9AAAAAJ" aria-label="Google Scholar">
      <i class="ai ai-google-scholar"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="DBLP" aria-label="DBLP">
    <a class="text-white" target="_blank" rel="noopener" href="https://dblp.uni-trier.de/pers/hd/k/Kummerfeld:Jonathan_K=" aria-label="DBLP">
      <i class="ai ai-dblp"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="ArXiv" aria-label="ArXiv">
    <a class="text-white" target="_blank" rel="noopener" href="https://arxiv.org/a/kummerfeld_j_1" aria-label="ArXiv">
      <i class="ai ai-arxiv"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="OrcID" aria-label="OrcID">
    <a class="text-white" target="_blank" rel="noopener" href="https://orcid.org/0000-0001-5030-3016" aria-label="OrcID">
      <i class="ai ai-orcid"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2021 Jonathan K. Kummerfeld All Rights Reserved</small>
        
	
		<p class="mt-2"><a href="/about/">About</a></p>
	
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>





<script src='/js/tabpane-persist.js'></script>




 

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.css" integrity="sha384-Cqd8ihRLum0CCg8rz0hYKPoLZ3uw+gES2rXQXycqnL5pgVQIflxAUDS7ZSjITLb5" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.js" integrity="sha384-1Or6BdeNQb0ezrmtGeqQHFpppNd7a/gw29xeiSikBbsb44xu3uAo8c7FwbF5jhbd" crossorigin="anonymous"></script>
 


<script defer src='https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/contrib/auto-render.min.js' integrity='sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl' crossorigin='anonymous' onload='renderMathInElement(document.body, null);'></script>














<script src="/js/main.min.3b172c13b62c2bea8b1c9d2599cddc8cf89718a92d792c680871c81ba43d8c85.js" integrity="sha256-OxcsE7YsK&#43;qLHJ0lmc3cjPiXGKkteSxoCHHIG6Q9jIU=" crossorigin="anonymous"></script>




  </body>
</html>
