<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>Language Models | Jonathan K. Kummerfeld</title>
<meta name="description" content="Work on making and using vector representations of text">
<meta property="og:url" content="https://www.jkk.name/reading-notes/language-models/">
  <meta property="og:site_name" content="Jonathan K. Kummerfeld">
  <meta property="og:title" content="Language Models">
  <meta property="og:description" content="Work on making and using vector representations of text">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="reading-notes">
    <meta property="article:modified_time" content="2024-03-19T13:26:59+11:00">

  <meta itemprop="name" content="Language Models">
  <meta itemprop="description" content="Work on making and using vector representations of text">
  <meta itemprop="dateModified" content="2024-03-19T13:26:59+11:00">
  <meta itemprop="wordCount" content="1555">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Language Models">
  <meta name="twitter:description" content="Work on making and using vector representations of text">
<link rel="preload" href="/scss/main.min.03e0316b11778904117114dfff33a66258e6dee164afef1d5ab33f05677b5508.css" as="style"><link href="/scss/main.min.03e0316b11778904117114dfff33a66258e6dee164afef1d5ab33f05677b5508.css" rel="stylesheet">
<script
  src="https://code.jquery.com/jquery-3.7.1.min.js"
  integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g=="
  crossorigin="anonymous"></script>

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-JVV2VPL5CX"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-JVV2VPL5CX');
        }
      </script>
  </head>
  <body class="td-page">
    <header>
      
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
            <div id="td-sidebar-menu" class="td-sidebar__inner">
  <form class="td-sidebar__search d-flex align-items-center">
    
    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ms-3 fas fa-bars" type="button" data-bs-toggle="collapse" data-bs-target="#td-section-nav" aria-controls="td-section-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>
  <nav class="td-sidebar-nav collapse" id="td-section-nav">
    
    <ul class="td-sidebar-nav__section pe-md-3 ul-0">
      <li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m--li">
  <a href="/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section tree-root" id="m-"><span class="">Jonathan K. Kummerfeld&#39;s Homepage</span></a>
  <ul class="ul-1">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-home-li">
  <a href="/home/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-home"><i class="fas fa-home"></i><span class="">Home</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-bio_and_cv-li">
  <a href="/bio_and_cv/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-bio_and_cv"><i class="fas fa-male"></i><span class="">Bio &amp; CV</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-students-li">
  <a href="/students/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-students"><i class="fas fa-users"></i><span class="">Students</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-studentsrecruiting-phd-li">
  <a href="/students/recruiting-phd/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-studentsrecruiting-phd"><span class="">Recruiting: PhD</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-studentsrecruiting-ms-li">
  <a href="/students/recruiting-ms/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-studentsrecruiting-ms"><span class="">Recruiting: Masters</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-studentsrecruiting-ug-li">
  <a href="/students/recruiting-ug/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-studentsrecruiting-ug"><span class="">Recruiting: Undergraduates</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-pubs-li">
  <a href="/pubs/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-pubs"><i class="fas fa-file-alt"></i><span class="">Publications</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-teaching-li">
  <a href="/teaching/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-teaching"><i class="fas fa-graduation-cap"></i><span class="">Teaching</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-software-li">
  <a href="/software/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-software"><i class="fas fa-code"></i><span class="">Software</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-data-li">
  <a href="/data/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-data"><i class="fas fa-database"></i><span class="">Datasets</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m-reading-notes-li">
  <a href="/reading-notes/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-reading-notes"><i class="fas fa-book-reader"></i><span class="">Reading Notes</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notescoreference-li">
  <a href="/reading-notes/coreference/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notescoreference"><span class="">Coreference Resolution</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notescrowdsourcing-li">
  <a href="/reading-notes/crowdsourcing/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notescrowdsourcing"><span class="">Crowdsourcing and Data Annotation</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesdiplomacy-li">
  <a href="/reading-notes/diplomacy/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesdiplomacy"><span class="">Diplomacy</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesexsempar-li">
  <a href="/reading-notes/exsempar/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesexsempar"><span class="">Executable Semantic Parsing</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id="m-reading-noteslanguage-models-li">
  <a href="/reading-notes/language-models/" class="align-left ps-0 active td-sidebar-link td-sidebar-link__page" id="m-reading-noteslanguage-models"><span class="td-sidebar-nav-active-item">Language Models</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-reading-notesold-blog-li">
  <a href="/reading-notes/old-blog/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-reading-notesold-blog"><span class="">Old Blog Posts</span></a>
  <ul class="ul-3 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-09_parsing-autoencoder-li">
  <a href="/reading-notes/old-blog/2017-10-09_parsing-autoencoder/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-09_parsing-autoencoder"><span class=""> Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-12-05_explainingpredictions-li">
  <a href="/reading-notes/old-blog/2017-12-05_explainingpredictions/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-05_explainingpredictions"><span class="">A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-16_forumrnn-li">
  <a href="/reading-notes/old-blog/2017-10-16_forumrnn/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-16_forumrnn"><span class="">A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2019-07-10_disentanglement-li">
  <a href="/reading-notes/old-blog/2019-07-10_disentanglement/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2019-07-10_disentanglement"><span class="">A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-12-01_nonsequencener-li">
  <a href="/reading-notes/old-blog/2017-12-01_nonsequencener/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-01_nonsequencener"><span class="">A Local Detection Approach for Named Entity Recognition and Mention Detection (Xu et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-10-04_crowdsrl-li">
  <a href="/reading-notes/old-blog/2020-10-04_crowdsrl/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-04_crowdsrl"><span class="">A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-12-12_multidomainwordvector-li">
  <a href="/reading-notes/old-blog/2017-12-12_multidomainwordvector/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-12_multidomainwordvector"><span class="">A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings (Yang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-16_ucca-li">
  <a href="/reading-notes/old-blog/2017-11-16_ucca/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-16_ucca"><span class="">A Transition-Based Directed Acyclic Graph Parser for UCCA (Hershcovich et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-17_twostagediscourseparsing-li">
  <a href="/reading-notes/old-blog/2017-11-17_twostagediscourseparsing/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-17_twostagediscourseparsing"><span class="">A Two-Stage Parsing Method for Text-Level Discourse Analysis (Wang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-29_abstractivesummarisation-li">
  <a href="/reading-notes/old-blog/2017-11-29_abstractivesummarisation/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-29_abstractivesummarisation"><span class="">Abstractive Document Summarization with a Graph-Based Attentional Neural Model (Tan et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-18_neuralamr-li">
  <a href="/reading-notes/old-blog/2017-10-18_neuralamr/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-18_neuralamr"><span class="">Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-04-16_lm_analysis-li">
  <a href="/reading-notes/old-blog/2018-04-16_lm_analysis/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-04-16_lm_analysis"><span class="">An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2019-09-22_conferenceapproach-li">
  <a href="/reading-notes/old-blog/2019-09-22_conferenceapproach/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2019-09-22_conferenceapproach"><span class="">Approaching Conferences</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-07_spineparsinglstm-li">
  <a href="/reading-notes/old-blog/2017-11-07_spineparsinglstm/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-07_spineparsinglstm"><span class="">Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-20_onlyattention-li">
  <a href="/reading-notes/old-blog/2017-10-20_onlyattention/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-20_onlyattention"><span class="">Attention Is All You Need (Vaswani et al., ArXiv 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-21_multiinputattention-li">
  <a href="/reading-notes/old-blog/2017-11-21_multiinputattention/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-21_multiinputattention"><span class="">Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-09-03_checklist-li">
  <a href="/reading-notes/old-blog/2020-09-03_checklist/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-03_checklist"><span class="">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-09-07_chartdialogs-li">
  <a href="/reading-notes/old-blog/2020-09-07_chartdialogs/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-07_chartdialogs"><span class="">ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-10-10_demographicembeddings-li">
  <a href="/reading-notes/old-blog/2020-10-10_demographicembeddings/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-10_demographicembeddings"><span class="">Compositional Demographic Word Embeddings (Welch et al., EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-09-25_crowdqasrl-li">
  <a href="/reading-notes/old-blog/2020-09-25_crowdqasrl/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-25_crowdqasrl"><span class="">Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-05-deftnn-li">
  <a href="/reading-notes/old-blog/2017-10-05-deftnn/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-05-deftnn"><span class="">DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-13_errordetection-li">
  <a href="/reading-notes/old-blog/2017-10-13_errordetection/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-13_errordetection"><span class="">Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-30_neuralsequence-li">
  <a href="/reading-notes/old-blog/2017-10-30_neuralsequence/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-30_neuralsequence"><span class="">Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-24_dynamictransition-li">
  <a href="/reading-notes/old-blog/2017-10-24_dynamictransition/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-24_dynamictransition"><span class="">Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-22_errorrepairparsing-li">
  <a href="/reading-notes/old-blog/2017-11-22_errorrepairparsing/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-22_errorrepairparsing"><span class="">Error-repair Dependency Parsing for Ungrammatical Texts (Sakaguchi et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-09-04_featureengineering-li">
  <a href="/reading-notes/old-blog/2018-09-04_featureengineering/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-09-04_featureengineering"><span class="">Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-01-28_crowdassistant-li">
  <a href="/reading-notes/old-blog/2018-01-28_crowdassistant/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-01-28_crowdassistant"><span class="">Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-06-12_parseradaptation-li">
  <a href="/reading-notes/old-blog/2018-06-12_parseradaptation/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-06-12_parseradaptation"><span class="">Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples (Vidur Joshi et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-06-madlibs-li">
  <a href="/reading-notes/old-blog/2017-10-06-madlibs/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-06-madlibs"><span class="">Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-09_framesdataset-li">
  <a href="/reading-notes/old-blog/2017-11-09_framesdataset/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-09_framesdataset"><span class="">Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-12_amralignment-li">
  <a href="/reading-notes/old-blog/2017-10-12_amralignment/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-12_amralignment"><span class="">Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-11_multimt-li">
  <a href="/reading-notes/old-blog/2017-10-11_multimt/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-11_multimt"><span class="">Google&#39;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-12-07_rarewordvectors-li">
  <a href="/reading-notes/old-blog/2017-12-07_rarewordvectors/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-07_rarewordvectors"><span class="">High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-12-10_dynamicannoallocation-li">
  <a href="/reading-notes/old-blog/2020-12-10_dynamicannoallocation/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-12-10_dynamicannoallocation"><span class="">Improving Human-Labeled Data through Dynamic Automatic Conflict Resolution (Sun, et al., CoLing 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-09-29_pretraininglm-li">
  <a href="/reading-notes/old-blog/2020-09-29_pretraininglm/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-29_pretraininglm"><span class="">Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-14_inorderparsing-li">
  <a href="/reading-notes/old-blog/2017-11-14_inorderparsing/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-14_inorderparsing"><span class="">In-Order Transition-based Constituent Parsing (Liu et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-10-12_taboo-li">
  <a href="/reading-notes/old-blog/2020-10-12_taboo/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-10-12_taboo"><span class="">Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-30_taggingrelations-li">
  <a href="/reading-notes/old-blog/2017-11-30_taggingrelations/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-30_taggingrelations"><span class="">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme (Zheng et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-03_discourserelations-li">
  <a href="/reading-notes/old-blog/2017-11-03_discourserelations/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-03_discourserelations"><span class="">Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-15_entityvectors-li">
  <a href="/reading-notes/old-blog/2017-11-15_entityvectors/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-15_entityvectors"><span class="">Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-08_graphdialogue-li">
  <a href="/reading-notes/old-blog/2017-11-08_graphdialogue/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-08_graphdialogue"><span class="">Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-03-05_curriculum-li">
  <a href="/reading-notes/old-blog/2018-03-05_curriculum/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-03-05_curriculum"><span class="">Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning (Tsvetkov et al., 2016)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-19_mace-li">
  <a href="/reading-notes/old-blog/2017-10-19_mace/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-19_mace"><span class="">Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-10_kginlstm-li">
  <a href="/reading-notes/old-blog/2017-11-10_kginlstm/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-10_kginlstm"><span class="">Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-23_alphagozero-li">
  <a href="/reading-notes/old-blog/2017-10-23_alphagozero/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-23_alphagozero"><span class="">Mastering the game of Go without human knowledge (Silver et al., Nature 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-06_literarycharacters-li">
  <a href="/reading-notes/old-blog/2017-11-06_literarycharacters/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-06_literarycharacters"><span class="">Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-26_multimodalwordembeddings-li">
  <a href="/reading-notes/old-blog/2017-10-26_multimodalwordembeddings/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-26_multimodalwordembeddings"><span class="">Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-17_nedisambiguation-li">
  <a href="/reading-notes/old-blog/2017-10-17_nedisambiguation/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-17_nedisambiguation"><span class="">Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-13_languagegame-li">
  <a href="/reading-notes/old-blog/2017-11-13_languagegame/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-13_languagegame"><span class="">Natural Language Does Not Emerge &#39;Naturally&#39; in Multi-Agent Dialog (Kottur et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-12-05_multidomainparsing-li">
  <a href="/reading-notes/old-blog/2017-12-05_multidomainparsing/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-05_multidomainparsing"><span class="">Neural Semantic Parsing over Multiple Knowledge-bases (Herzig et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2019-09-13_diplomacynopress-li">
  <a href="/reading-notes/old-blog/2019-09-13_diplomacynopress/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2019-09-13_diplomacynopress"><span class="">No-Press Diplomacy: Modeling Multi-Agent Gameplay (Paquette et al., 2019)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-27_commonsense-li">
  <a href="/reading-notes/old-blog/2017-11-27_commonsense/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-27_commonsense"><span class="">Ordinal Common-sense Inference (Zhang et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-01_mixtureofexperts-li">
  <a href="/reading-notes/old-blog/2017-11-01_mixtureofexperts/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-01_mixtureofexperts"><span class="">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Shazeer et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2020-09-17_activelearningbrittle-li">
  <a href="/reading-notes/old-blog/2020-09-17_activelearningbrittle/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2020-09-17_activelearningbrittle"><span class="">Practical Obstacles to Deploying Active Learning (Lowell, et al., EMNLP 2019)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-11-08_corefdata-li">
  <a href="/reading-notes/old-blog/2018-11-08_corefdata/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-11-08_corefdata"><span class="">PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution (Chen et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-03-08_sql-li">
  <a href="/reading-notes/old-blog/2018-03-08_sql/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-03-08_sql"><span class="">Provenance for Natural Language Queries (Deutch et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-01-31_sentencerepfromparaphrases-li">
  <a href="/reading-notes/old-blog/2018-01-31_sentencerepfromparaphrases/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-01-31_sentencerepfromparaphrases"><span class="">Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (Wieting et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-31_realtimecaptioning-li">
  <a href="/reading-notes/old-blog/2017-10-31_realtimecaptioning/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-31_realtimecaptioning"><span class="">Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-12-06_coreferencearguments-li">
  <a href="/reading-notes/old-blog/2017-12-06_coreferencearguments/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-06_coreferencearguments"><span class="">Revisiting Selectional Preferences for Coreference Resolution (Heinzerling et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-20_mrsparser-li">
  <a href="/reading-notes/old-blog/2017-11-20_mrsparser/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-20_mrsparser"><span class="">Robust Incremental Neural Semantic Graph Parsing (Buys et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-10_seqqa-li">
  <a href="/reading-notes/old-blog/2017-10-10_seqqa/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-10_seqqa"><span class="">Search-based Neural Structured Learning for Sequential Question Answering (Iyyer et al., ACL 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-27_swishactivation-li">
  <a href="/reading-notes/old-blog/2017-10-27_swishactivation/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-27_swishactivation"><span class="">Searching for Activation Functions (Ramachandran et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-12-08_crowdbias-li">
  <a href="/reading-notes/old-blog/2017-12-08_crowdbias/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-08_crowdbias"><span class="">Sequence Effects in Crowdsourced Annotations (Mathur et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-10-25_shiftreducedp-li">
  <a href="/reading-notes/old-blog/2017-10-25_shiftreducedp/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-10-25_shiftreducedp"><span class="">Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice (Mi and Huang, 2015)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-11-28_interpretableembeddings-li">
  <a href="/reading-notes/old-blog/2017-11-28_interpretableembeddings/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-11-28_interpretableembeddings"><span class="">SPINE: SParse Interpretable Neural Embeddings (Subramanian et al., 2017)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2018-05-08_seq2seqsensitivity-li">
  <a href="/reading-notes/old-blog/2018-05-08_seq2seqsensitivity/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2018-05-08_seq2seqsensitivity"><span class="">The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models (Weber et al., 2018)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-reading-notesold-blog2017-12-12_wordvectorgeometry-li">
  <a href="/reading-notes/old-blog/2017-12-12_wordvectorgeometry/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesold-blog2017-12-12_wordvectorgeometry"><span class="">The strange geometry of skip-gram with negative sampling (Mimno et al., 2017)</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-reading-notesmisc-li">
  <a href="/reading-notes/misc/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-reading-notesmisc"><span class="">Other</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-advice-li">
  <a href="/advice/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-advice"><i class="fas fa-comment"></i><span class="">Advice</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-adviceposters-li">
  <a href="/advice/posters/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-adviceposters"><span class="">Academic Posters</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id="m-adviceemail-li">
  <a href="/advice/email/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-adviceemail"><span class="">Email Clients</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-group-management-li">
  <a href="/group-management/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-group-management"><i class="fas fa-book"></i><span class="">Group Processes</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sponsor-li">
  <a href="/sponsor/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sponsor"><i class="fas fa-dollar-sign"></i><span class="">Sponsors</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-contact-li">
  <a href="/contact/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-contact"><i class="fas fa-phone"></i><span class="">Contact</span></a>
</li>
  </ul>
</li>
    </ul>
  </nav>
</div>

          </aside>
          <aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none">
            <div class="td-page-meta ms-2 pb-1 pt-2 mb-0">

</div>

            <div class="td-toc">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#model-types">Model Types</a>
      <ul>
        <li><a href="#transformers">Transformers</a></li>
        <li><a href="#recurrent-models">Recurrent models</a></li>
        <li><a href="#mixture-of-experts">Mixture of Experts</a></li>
        <li><a href="#learning-at-evaluation-time">Learning at Evaluation Time</a></li>
      </ul>
    </li>
    <li><a href="#prompt-tuning">Prompt Tuning</a></li>
    <li><a href="#speeding-up">Speeding up</a></li>
    <li><a href="#training-data">Training data</a></li>
    <li><a href="#training-objective">Training objective</a></li>
    <li><a href="#guiding-inference">Guiding Inference</a></li>
    <li><a href="#tokenisation">Tokenisation</a></li>
    <li><a href="#analysis">Analysis</a></li>
    <li><a href="#use-in-evaluation">Use in Evaluation</a></li>
  </ul>
</nav>
      </div>
    
            
	
          </aside>
          <main class="col-12 col-md-9 col-xl-8 ps-md-5" role="main">
            
  

            
            
<div class="td-content">
	<h1>Language Models</h1>
	<div class="lead">Work on making and using vector representations of text</div>
	<header class="article-meta">
		
  </header>
	<h2 id="model-types">Model Types<a class="td-heading-self-link" href="#model-types" aria-label="Heading self-link"></a></h2>
<h3 id="transformers">Transformers<a class="td-heading-self-link" href="#transformers" aria-label="Heading self-link"></a></h3>
<p><a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">The Transformer</a> is the core of the most successful language models.
I wrote <a href="/reading-notes/old-blog/2017-10-20_onlyattention/">a blog post</a> about the original paper, including this figure, which captures the network structure with a few simplifications:</p>
<p><img alt="Google Attention Network" src="/img/post/google-attention.png"></p>
<p>There are a few ideas being brought together here:</p>
<ul>
<li><em>Positional encoding</em>, which is a vector of the same length as the word representation, but that depends only on the position in the input. The original paper used \(f(pos, dim) = sin(pos / 10000^{2 dim / d_w})\) for even dimensions and the cosine equivalent for odd dimensions (where \(d_w\) is the number of dimensions).</li>
<li><em>Multi-head attention</em>, where rather than running attention once on the full vector, multiple linear transforms are applied to get smaller vectors.</li>
<li><em>Scaled dot product attention</em>, the equation is shown in the figure, the key new idea is to rescale by the square root of the dimensionality so that larger vectors don&rsquo;t produce excessively sharp distributions after the softmax is applied. The more general form of this described in the paper has keys (\(K\)), queries (\(Q\)) and values (\(V\)), but the network uses the same vector for the key and value. I show the query with a solid line and the values/keys with dotted lines. The matrix \(V\) is formed by using the \(v\) vectors as rows, while \(Q\) is formed by duplicating \(q\) in every row. Note, on the left hand side attention is over all input words, while on the right it is only over the words produced so far (ie. content to the left).</li>
<li><em>Layer normalisation</em>, a way to rescale weights to keep vector outputs in a nice range, from <a href="https://arxiv.org/abs/1607.06450">Ba, Kiros and Hinton (ArXiv 2016)</a>.</li>
<li>Other details, (1) When the outputs are words, the vectors used to initially represent each input word are also used to represent the outputs and in the final linear transformation (though with some rescaling). (2) They use a formula I haven&rsquo;t seen before to adjust the learning rate during training, (3) dropout in several places and label smoothing are used for regularization.</li>
</ul>
<p>Simplifications in the figure:</p>
<ul>
<li>For multi-head attention I only show two transforms, while in practise they used 8.</li>
<li>The shaded regions are duplicated 6 times to create a stack, with the output of one region acting as the input to the next copy of it. The links from left to right are always from the top of the input stack.</li>
<li>The musical repeat signs indicate that the structure is essentially the same. On the output side this isn&rsquo;t quite true since the attention boxes only take inputs to their left (since output to the right doesn&rsquo;t exist when they are being calculated).</li>
</ul>
<p>Since the original model, a range of variations have been explored.
The <a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM paper (ArXiv, 2022)</a> provides a nice breakdown of variations that enabled training a massive model (Section 2).</p>
<h3 id="recurrent-models">Recurrent models<a class="td-heading-self-link" href="#recurrent-models" aria-label="Heading self-link"></a></h3>
<p>Recurrent models continue to show strong performance on benchmarks like enwiki-8:</p>
<ul>
<li><a href="https://aclanthology.org/2021.emnlp-main.602/">SRU++</a>, 1.02 bpc.</li>
<li><a href="https://arxiv.org/pdf/1911.11423.pdf">SHA-RNN</a>, 1.07 bpc.</li>
</ul>
<h3 id="mixture-of-experts">Mixture of Experts<a class="td-heading-self-link" href="#mixture-of-experts" aria-label="Heading self-link"></a></h3>
<p>Not it&rsquo;s own type, but really a variation on other types: have a mechanism for only using part of the weights in the model at inference, depending on the content of the input.
For example, <a href="https://arxiv.org/abs/2112.06905">GLaM</a> uses a transformer with a set of matrices for feedforward layers, of which two are chosen for each token of input.
The key advantage of this is that it can increase the number of model parameters (and so hopefully take advantage of more data) without increasing the cost of inference.</p>
<h3 id="learning-at-evaluation-time">Learning at Evaluation Time<a class="td-heading-self-link" href="#learning-at-evaluation-time" aria-label="Heading self-link"></a></h3>
<p>Dynamic Evaluation <a href="http://proceedings.mlr.press/v80/krause18a/krause18a.pdf">Krause et al. (ICML 2018)</a> update language model parameters during inference.
When a new word in the sequence is seen, the model is updated.
This boosts performance, probably because it makes related words more likely.
<a href="https://aclanthology.org/2021.findings-emnlp.346.pdf">Yoshida and Gimpel (EMNLP Findings, 2021)</a> introduced a variant, HSO, which updates the hidden states rather than the model parameters.
This idea is particularly helpful with huge LMs.
Again, there is an improvement in performance (small for in-domain language modelling, larger out of domain, and small for downstream tasks).</p>
<h2 id="prompt-tuning">Prompt Tuning<a class="td-heading-self-link" href="#prompt-tuning" aria-label="Heading self-link"></a></h2>
<p>Clear introduction and extensive literature review by <a href="https://arxiv.org/pdf/2107.13586.pdf">Liu et al. (arXiv 2021)</a>, with updates at <a href="http://pretrain.nlpedia.ai/">http://pretrain.nlpedia.ai/</a>.
The general idea is to build a system for a task by running the data through a language model with some pre-processing and post-processing.
For example, for predicting movie review scores, take some text and add &ldquo;This is [blank]&rdquo;, then see what the model predicts for the blank word.
More generally, the pre- and post-processing can be learned functions.
For example, <a href="https://aclanthology.org/2021.emnlp-main.243.pdf">Lester et al. (EMNLP 2021)</a> add vectors to the input that are trained using labeled data.
For all but the largest LMs, this requires prompts of 20 vectors or more, initialised from class labels or vocabulary, with the LM tuned/post-trained for next word prediction (since in their case it was a model that only saw corrupted input).</p>
<p>This idea can also be used in a semi-supervised setup, where prompt-based models are used to label data <a href="https://aclanthology.org/2021.eacl-main.20.pdf">(Schick and Hinrich Schutze, EACL 2021)</a>.</p>
<p>While some examples of prompts feel intuitive, that doesn&rsquo;t mean they all are or that the models are working for the reason we expect.
This was probed by <a href="https://arxiv.org/pdf/2109.01247.pdf">Webson and Pavlick (arXiv 2021)</a>, who evaluated a wide set of explicit prompts for a task (ie., where the prompt is natural language rather than a trained vector).
They found that using what a person would deem a good prompt is no better than irrelevant prompts, null prompts, and one misleading prompt (&ldquo;Does the paragraph start with &rsquo;the&rsquo;?).
For answer types (yes-no vs. dog-cat), a more intuitive answer does appear to do better, but it is unclear why yes-no is better than true-false (for example).
Similarly, <a href="https://aclanthology.org/2021.acl-long.146.pdf">Cao, et. al. (ACL 2021)</a> studied the ability to treat an LM as a database, with prompts retrieving information.
They showed that prior success was mainly due to biases in the evaluation data, with spurious answers occuring (e.g., always predicting London as a place of birth), and higher performing prompts simply fitting the answer distribution of the evaluation data better.</p>
<ul>
<li>Finetuning on instructions <a href="https://arxiv.org/pdf/2109.01652.pdf">https://arxiv.org/pdf/2109.01652.pdf</a></li>
</ul>
<!--

https://arxiv.org/pdf/2208.05577.pdf

-->
<h2 id="speeding-up">Speeding up<a class="td-heading-self-link" href="#speeding-up" aria-label="Heading self-link"></a></h2>
<p>One way to save computation is to only run some of the layers in a stack.
<a href="https://arxiv.org/abs/2207.07061">CALM</a> predicts when early stopping is possible without changing the output, leading to similar results in about half the time.</p>
<h2 id="training-data">Training data<a class="td-heading-self-link" href="#training-data" aria-label="Heading self-link"></a></h2>
<p>Training models on higher quality data can produce strong results even when there is relatively little data.
This has been shown for code <a href="https://arxiv.org/pdf/2306.11644.pdf">(Gunasekar et al., arXiv 2023)</a> and stories <a href="https://arxiv.org/pdf/2305.07759.pdf">(Eldan and Li, arXiv 2023)</a>, where in both cases a large model is used to either generate data or filter existing data to create a small high-qiality dataset for training a small model.</p>
<h2 id="training-objective">Training objective<a class="td-heading-self-link" href="#training-objective" aria-label="Heading self-link"></a></h2>
<ul>
<li>GOLD, <a href="https://openreview.net/pdf?id=RovX-uQ1Hua">https://openreview.net/pdf?id=RovX-uQ1Hua</a></li>
<li>Tempering, <a href="https://aclanthology.org/2021.mtsummit-research.10/">https://aclanthology.org/2021.mtsummit-research.10/</a>, Note that AlphaCode experiments found a lower temperature was best</li>
</ul>
<h2 id="guiding-inference">Guiding Inference<a class="td-heading-self-link" href="#guiding-inference" aria-label="Heading self-link"></a></h2>
<p><a href="http://www.cond.org/talebrush.pdf">Chung et al. (CHI 2022)</a> built a system in which users can specify how the fortune of a protagonist in a generated story should vary over the story (like Kurt Vonnegut&rsquo;s famous drawings).</p>
<h2 id="tokenisation">Tokenisation<a class="td-heading-self-link" href="#tokenisation" aria-label="Heading self-link"></a></h2>
<p>Most LMs today use a word-piece tokenisation, where rare or unseen words are broken into pieces.
For constructing these vocabularies, there are several approaches:</p>
<ul>
<li>BPE <a href="https://aclanthology.org/P16-1162.pdf">https://aclanthology.org/P16-1162.pdf</a>, which can be implemented more efficiently, in O(N log M) time where N is the sequence length and M is the number of merges <a href="https://aclanthology.org/2023.findings-acl.38.pdf">(Zouhar, et al. (ACL 2023)</a></li>
<li>Unigram tokenizer</li>
<li>WordPiece <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf">https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf</a></li>
<li>Unigram LM <a href="https://aclanthology.org/P18-1007/">https://aclanthology.org/P18-1007/</a></li>
</ul>
<p>Compared in <a href="https://aclanthology.org/2020.findings-emnlp.414.pdf">https://aclanthology.org/2020.findings-emnlp.414.pdf</a></p>
<p>Library: SentencePiece <a href="https://github.com/google/sentencepiece">https://github.com/google/sentencepiece</a>, <a href="https://aclanthology.org/D18-2012/">https://aclanthology.org/D18-2012/</a></p>
<p>For inference, <a href="https://aclanthology.org/2021.emnlp-main.160.pdf">Song et al. (EMNLP 2021)</a> improved efficiency by forming a special trie that encodes all the known words and pieces, with extra edges that show what to produce when a match fails.</p>
<h2 id="analysis">Analysis<a class="td-heading-self-link" href="#analysis" aria-label="Heading self-link"></a></h2>
<p>What are these LMs [not] learning?</p>
<p>Syntactic constraints are one way to probe these models, e.g., do they correctly model number agreement for noun-verb pairs?
<a href="https://aclanthology.org/2021.emnlp-main.72.pdf">Wei et al. (EMNLP 2021)</a> showed the answer to that question is yes, though they have difficulty with rarer words (interestingly, a frequency of ~100 seems to be a convergence point, similar to the point at which static word embeddings are stable in <a href="https://aclanthology.org/N18-1190.pdf">my work with Laura Burdick</a>).
BERT also has trouble when one form is far more common than another.</p>
<p>For models made available only via APIs, one challenge is that newer versions may not behave the same as older ones.
<a href="https://arxiv.org/pdf/2307.09009.pdf">Chen et al. (arXiv 2023)</a> show this for GPT 3.5 and GPT 4, with a range of changes in just three months.
Some changes are consistent (e.g., decrease in verbosity for GPT 4), but most vary across tasks, including both higher and lower scores.</p>
<h2 id="use-in-evaluation">Use in Evaluation<a class="td-heading-self-link" href="#use-in-evaluation" aria-label="Heading self-link"></a></h2>
<p>For text generation tasks, evaluation is difficult.
<a href="https://arxiv.org/pdf/1904.09675.pdf">Zhang et al. (ICLR 2020)</a> proposed BERTscore, which uses BERT to calculate similarity between tokens in output and a reference, followed by greedy matching and averaging to get an overall score.
<a href="https://papers.nips.cc/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf">Yuan et al. (NeurIPS 2021)</a> switch to BART and note that for an application like machine translation you can also make a comparison with the source text, and the comparison can be done in one direction or another between reference and generated text.
It is similar in performance to BERTscore, but they also explore finetuning and prompting, which improve performance.
<a href="https://arxiv.org/abs/2112.04139">Kasai et al. (arXiv 2021)</a> propose forming an ensemble of metrics, learning a combined metric that correlates best with buman evaluations.
In their model, leaderboards accept both new models and new metrics, updating the results in the process.</p>

	
</div>


          </main>
        </div>
      </div>
      
<footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        
          
            
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Email" aria-label="Email">
    <a class="text-white" target="_blank" rel="noopener" href="mailto:jonathan.kummerfeld@sydney.edu.au" aria-label="Email">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener" href="https://twitter.com/jkkummerfeld" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="LinkedIn" aria-label="LinkedIn">
    <a class="text-white" target="_blank" rel="noopener" href="https://linkedin.com/in/jkkummerfeld/" aria-label="LinkedIn">
      <i class="fab fa-linkedin"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Mastodon" aria-label="Mastodon">
    <a class="text-white" target="_blank" rel="noopener" href="https://mastodon.social/@jkkummerfeld" aria-label="Mastodon">
      <i class="fa-brands fa-mastodon"></i>
    </a>
  </li>
  
</ul>

          
        
      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        
          
            
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/jkkummerfeld" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="ACL Anthology" aria-label="ACL Anthology">
    <a class="text-white" target="_blank" rel="noopener" href="https://aclanthology.org/people/j/jonathan-k-kummerfeld/" aria-label="ACL Anthology">
      <i class="icon icon-acl-logo"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Semantic Scholar" aria-label="Semantic Scholar">
    <a class="text-white" target="_blank" rel="noopener" href="https://semanticscholar.org/author/Jonathan-K.-Kummerfeld/1727211" aria-label="Semantic Scholar">
      <i class="ai ai-semantic-scholar"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Google Scholar" aria-label="Google Scholar">
    <a class="text-white" target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=OsoNG9AAAAAJ" aria-label="Google Scholar">
      <i class="ai ai-google-scholar"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="DBLP" aria-label="DBLP">
    <a class="text-white" target="_blank" rel="noopener" href="https://dblp.uni-trier.de/pers/hd/k/Kummerfeld:Jonathan_K=" aria-label="DBLP">
      <i class="ai ai-dblp"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="ArXiv" aria-label="ArXiv">
    <a class="text-white" target="_blank" rel="noopener" href="https://arxiv.org/a/kummerfeld_j_1" aria-label="ArXiv">
      <i class="ai ai-arxiv"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="OrcID" aria-label="OrcID">
    <a class="text-white" target="_blank" rel="noopener" href="https://orcid.org/0000-0001-5030-3016" aria-label="OrcID">
      <i class="ai ai-orcid"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="USyd" aria-label="USyd">
    <a class="text-white" target="_blank" rel="noopener" href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/jonathan-kummerfeld.html" aria-label="USyd">
      <i class="icon icon-usyd-logo"></i>
    </a>
  </li>
  
</ul>

          
        
      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <small class="text-white">&copy; 2024 Jonathan K. Kummerfeld All Rights Reserved</small>
        
	
		
	
      </div>
    </div>
  </div>
</footer>


    </div>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
      integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
        integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg=="
        crossorigin="anonymous">
</script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
       integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin="anonymous"
       onload='renderMathInElement(document.body, null);'>
</script>
<script src="/js/main.min.d4f01890352ec227bd6fafbde86e81fb252198076cabc96767a61d749b84753e.js" integrity="sha256-1PAYkDUuwie9b6&#43;96G6B&#43;yUhmAdsq8lnZ6YddJuEdT4=" crossorigin="anonymous"></script>
<script defer src="/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js" integrity="sha256-c0eKfUgHaYrtfjVesj&#43;YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin="anonymous"></script>
<script src='/js/tabpane-persist.js'></script>

  </body>
</html>
