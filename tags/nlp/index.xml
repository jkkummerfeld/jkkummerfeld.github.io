<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nlp on Jonathan K. Kummerfeld</title>
    <link>http://www.jkk.name/tags/nlp/</link>
    <description>Recent content in Nlp on Jonathan K. Kummerfeld</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jonathan K. Kummerfeld</copyright>
    <lastBuildDate>Mon, 09 Oct 2017 14:31:24 -0400</lastBuildDate>
    <atom:link href="/tags/nlp/" rel="self" type="application/rss+xml" />
    
    <item>
      <title> Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)</title>
      <link>http://www.jkk.name/post/2017-10-09_parsing-autoencoder/</link>
      <pubDate>Mon, 09 Oct 2017 14:31:24 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-09_parsing-autoencoder/</guid>
      <description>

&lt;p&gt;Semantic parsing datasets are small because they are expensive to produce (logical forms don&amp;rsquo;t occur naturally and writing them down takes time).
The idea here is to do semi-supervised learning by implementing both a parser and a generator, which are trained together as a form of autoencoder where the intermediate representation is natural language.&lt;/p&gt;

&lt;p&gt;The architecture has four LSTMs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Bidirectional LSTM over a logical form.&lt;/li&gt;
&lt;li&gt;One directional LSTM attending to the first LSTM&amp;rsquo;s hidden states, generating a sentence.&lt;/li&gt;
&lt;li&gt;Bidirectional LSTM over the sentence generated by the second LSTM.&lt;/li&gt;
&lt;li&gt;One directional LSTM attending to the third LSTM&amp;rsquo;s hidden states, generating a logical form.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Usually a component like the second LSTM would choose the max word at each position (or use beam search), but here they want this whole thing to be differentiable, so the distribution over words is used.
At evaluation time only the second half (3+4) is used, with the test sentence as input.&lt;/p&gt;

&lt;p&gt;With this structure, a loss function is defined that compares the input to (1) and the output of (4), which in both cases is a logical form.
As a result, they don&amp;rsquo;t need (logical form, sentence) pairs to train, they can use automatically generated logical forms.
Of course, with only logical forms it would do something random with the intermediate representation, so some supervised examples are also needed (in which case the two halves are trained independently).&lt;/p&gt;

&lt;p&gt;The results are not state-of-the-art, but good on all three tasks (Geoquery, NLmaps, SAIL), and on two they show am improvement over training (3+4) with only supervised data.
Varying the amount of training data gives a less clear picture.
On Geoquery with 5-25% of the data, this approach clearly helps, particularly if the queries are real rather than generated (which is a realistic scenario), but then there is no improvement for 50% or 75%, and at 100% the improvement is small.
On NLmaps there was no generator, and the differences at different data %s seem like noise.
SAIL has the most clear benefit, though it&amp;rsquo;s a particularly small dataset, consisting of paths in just four maps.&lt;/p&gt;

&lt;p&gt;This is a cool idea that seems effective in certain situations.
The generator is key, and it&amp;rsquo;s possible that performance on GeoQuery would be higher with a more sophisticated one (e.g. a tree structured generator, rather than the ngram model used here).
One idea mentioned in the conclusion is to try reversing the setup (3-4-1-2) and training with natural language examples that have no logical form.
How to tradeoff the different data scenarios seems like an interesting challenge!&lt;/p&gt;

&lt;h2 id=&#34;citations&#34;&gt;Citations&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://aclweb.org/anthology/D16-1116&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{kovcisky-EtAl:2016:EMNLP2016,
  author    = {Ko\v{c}isk\&#39;{y}, Tom\&#39;{a}\v{s}  and  Melis, G\&#39;{a}bor  and  Grefenstette, Edward  and  Dyer, Chris  and  Ling, Wang  and  Blunsom, Phil  and  Hermann, Karl Moritz},
  title     = {Semantic Parsing with Semi-Supervised Sequential Autoencoders},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  pages     = {1078--1087},
  url       = {https://aclweb.org/anthology/D16-1116}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)</title>
      <link>http://www.jkk.name/post/2017-10-06-madlibs/</link>
      <pubDate>Fri, 06 Oct 2017 13:31:43 -0400</pubDate>
      
      <guid>http://www.jkk.name/post/2017-10-06-madlibs/</guid>
      <description>

&lt;p&gt;Humor is an incredibly difficult problem, as this paper makes clear in its background section.
Most work has considered very specific types of jokes (e.g. &amp;ldquo;that&amp;rsquo;s what she said&amp;rdquo;, or pairs of words that sound similar to form riddles).
This work contributes (1) a new task, (2) an evaluation method, and (3) an example system.&lt;/p&gt;

&lt;p&gt;The task is Mad Libs, where a story has some words removed and people choose new words to make the story funny.
If you are familiar with the normal version, one key difference is that here people have access to the complete story when they are choosing their words.
A set of 40 &amp;lsquo;stories&amp;rsquo; were written based on Simple Wikipedia articles, and workers on Mechanical Turk wrote words to fill them, with filtering based on judging by other workers.&lt;/p&gt;

&lt;p&gt;The evaluation method involved recruiting a set of judges on Mechanical Turk and asking a series of questions to measure humour for a given response.
As well as judging the overall story, they were asked to select which words contributed the most.
By aggregating these selections as votes, each word was scored as funny or not.&lt;/p&gt;

&lt;p&gt;The system is a linear classifier with a range of features, including scores from a language model.
On its own, it performs very poorly, but using it as a filter to restrict the space of words a person can choose from actually leads to better performance than people on their own.
Of course, it&amp;rsquo;s difficult to analyse the source of improvement;
The authors theorise that it is because it prevents people from selecting words that only they would see is funny.
Another interpretation is that the constraint gives them a smaller space to think about and so they can find more interesting plays on words.&lt;/p&gt;

&lt;p&gt;Finally, as a non-expert in this area, this paper had some nice discussion of the tradeoffs between different ways of generating humour (incongruous vs. coherent content strategies).&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1068&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{hossain-EtAl:2017:EMNLP2017,
  author    = {Hossain, Nabil  and  Krumm, John  and  Vanderwende, Lucy  and  Horvitz, Eric  and  Kautz, Henry},
  title     = {Filling the Blanks (hint: plural noun) for Mad Libs Humor},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {649--658},
  url       = {https://www.aclweb.org/anthology/D17-1068},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
