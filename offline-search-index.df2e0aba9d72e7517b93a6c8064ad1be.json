

















































































[{"body":" I am recruiting PhD students to start in 2022!\nSee my recruiting page for more information.\n   My work: I aim to make language a complementary part of the interface for any application (e.g. from desktop spreadsheets to mobile apps). This involves solving a range of fascinating challenges through new methods in Artificial Intelligence and Crowdsourcing.\nInterests: Executable Semantic Parsing (e.g., text-to-SQL) Dialogue Crowdsourcing Human-in-the-Loop Systems / Hybrid Intelligence\nMe: I am currently a Postdoctoral Research Fellow at the University of Michigan. In mid-2022, I will start as a Senior Lecturer (ie., research tenure-track) at the University of Sydney.\n                               News September, 2021: Congratulations to Laura and Andrew for their EMNLP and Findings of EMNLP papers, on the stability of embeddings in different languages, and an architecture for interpretable models built out of smaller, more focused models.\nAugust, 2021: I’ve been awarded a DECRA Fellowship by the Australian Research Council.\n","categories":"","description":"Jonathan K. Kummerfeld's website, with links to research papers, code, and data.\n","excerpt":"Jonathan K. Kummerfeld's website, with links to research papers, code, …","ref":"/home/","tags":"","title":"Home"},{"body":"Work with me on NLP at the University of Sydney, starting in late 2022 or early 2023!\nHow to apply At Australian universities, you apply directly to individual faculty (rather than the central process used in US universities). To apply for my research group, do the following by Dec 17th:\n(1) Complete this form.\n(2) Email jkk.recruiting@gmail.com with:\n Statement of Purpose (PDF, 2-4 pages), covering your academic background, research interests, and reasons for pursuing a PhD. Transcript(s) (PDF or images) CV / Resume (PDF)  (3) Tell your letter writers to email jkk.recruiting@gmail.com with the subject “2021 Recommendation for [Your Name]” and the letter either in the body of the email or attached as a pdf. The email must come from their professional email address or an administrator in their department. Letters may arrive any time in December (sooner is preferred).\nMake sure to do all three steps by Dec 17th.\nIf you have any questions, email jkk.recruiting@gmail.com.\nOffer Process In early 2022 (January / February / March):\n I will contact you for a phone interview. I will tell you I want you to join my group. You complete the university application form with my help. You receive a formal offer about 3 weeks later. You accept and start in either late 2022 or early 2023!  My Group I am looking for multiple students to work on Natural Language Processing. Projects will also cross over into Crowdsourcing / Human Computation, Data Science, and Machine Learning.\nI am towards the hands-on end as an advisor and aim to build a collaborative, socially connected group. My group will have weekly group meetings and visits by researchers from around the world (virtual for now).\nQualifications All Applicants: You do not need a master’s degree or published papers. That kind of experience is certainly a plus, but so are other activities, such as internships, personal projects, and extra background in non-CS subjects.\nNon-Australians: You must be on-track to complete a Bachelor’s degree in 2022 that includes significant experience in Computer Science. If you are not completing a senior thesis as part of your degree then you may need to start in a Masters of Philosophy degree and transfer to the PhD program after your first year.\nAustralians: You need to be on track to complete an Honours year with first class honours, or a Master’s degree with an independent research component.\nDegree Structure Details here. Key points:\n 3-4 years Two classes total (ie, half a semester of time) No teaching, but you will have opportunities to teach if desired I encourage interested students to do internships in industry research labs  Funding Every student in my group is funded to do just research for the length of their PhD.\nHow does Sydney compare to US universities? I recognise that some applicants may not be familiar with Australian universities. Comparing universities is hard, but here are a few points of comparison:\n US News, comparable to NYU, UNC-Chapel Hill, UT Austin, UW Madison CS Rankings, comparable to JHU, UC Davis CS Rankings, AI only, comparable to UW Madison, UNC-Chapel Hill Times Higher Ed, comparable to UW Madison, Brown QS Rankings, comparable to UCLA, NYU, UCSD  ","categories":"","description":"","excerpt":"Work with me on NLP at the University of Sydney, starting in late 2022 …","ref":"/students/recruiting-phd/","tags":"","title":"Recruiting: PhD"},{"body":"Bio Jonathan K. Kummerfeld is a Postdoctoral Research Fellow in Computer Science and Engineering at the University of Michigan. In 2022, he will start a tenure-track posotion in the School of Computer Science at the University of Sydney. He completed his Ph.D. at the University of California, Berkeley, advised by Prof. Dan Klein. Jonathan’s research has revealed new challenges in syntactic parsing, coreference resolution, and dialogue. He has proposed models and algorithms to address these challenges, improving the speed and accuracy of natural language processing systems. He has been on the program committee for 55 conferences and workshops. He currently serves as an Action Editor for the ACL Rolling Review system, and a standing reviewer for the Computational Linguistics journal and the Transactions of the Association for Computational Linguistics journal. For more details, see his website: https://www.jkk.name\nCV PDF of my CV\nAcademic Path  University of Sydney, Senior Lecturer, 2022 -, and DECRA Fellow, 2023 - 2025 University of Michigan, Postdoctoral Research Fellow, 2016 - 2021, Research Associate 2, 2021 - 2022 University of California, Berkeley, PhD, 2010 - 2016, Advisor: Dan Klein University of Sydney, B.Sc. (Adv) (Hons), 2006 - 2009, Advisor: James R. Curran  Industry Roles Consultant / Technical Adviser\n Everlaw, October 2021 - Clinc, January 2017 - February 2020 Naming Matters, May 2015 - December 2019 WriteLab / WriteTrack (acquired by Chegg), March 2014 - May 2018 Techbridge, September 2013 - May 2014  Intern:\n Google Research, Summer 2012 Google, Summer 2007 / 2008  Awards / Grants  Discovery Early Career Researcher Award (DECRA), 2021 PI on grant to develop new methods for interactive executable semantic parsing EMNLP Conference Outstanding Reviewer, 2020 Bloomberg Data Science Research Grant, 2019 Co-PI on grant to develop adaptive annotation methods Michigan Institute for Data Science Music Initiative, 2018 Co-PI on grant to develop models that map text to music ACL Conference Top Reviewer, 2018 Outstanding Postdoctoral Fellow Award, 2018 For excellence in research, mentorship, teaching, leadership, and service Allen Institute for AI Key Scientific Challenges Program, 2017 Unrestricted funding to support research Chair’s Special Award, 2013 For services to the EECS department during student admissions and recruitment Outstanding Graduate Student Instructor Award, 2011 University wide award in recognition of exceptional achievements as a teacher General Sir John Monash Award, 2009 Postgraduate scholarships for Australians who demonstrate outstanding leadership potential Australia’s most prestigious postgraduate scholarships University Medal in Computer Science, 2009 For outstanding results, awarded at the discretion of the university senate  Publications See my publications page\nTeaching and Talks See my teaching page\nService Area Chair\n ACL: Association for Computational Linguistics Computational Social Science and Social Media, 2020 Resources and Evaluation, 2021 ACL Rolling Review Action Editor, 2021 DSTC: Dialog System Technology Challenges, Task Organiser, 2018, 2019  University Internal\n DEI Committee: Inclusive Environment, Michigan, 2020-2021 Hiring Comittee for Research Investigator, Michigan Medical School, 2017 Graduate Admissions Committee, Michigan Artificial Intelligence, 2017 Graduate Admissions Committee, Berkeley Artificial Intelligence, 2015 President, Berkeley Computer Science Graduate Student Association, 2012–2013  Thesis Committee\n Laura (Wendlandt) Burdick, Michigan, 2020 Sai R. Gouravajhala, Michigan, 2020 Janarthanan Rajendran, Michigan, 2020  Journal Reviewer\n CL: Computational Linguistics, 2020, 2021 TACL: Transactions of the Assoc.~for Computational Linguistics, 2016, 2020-2022 JAIR: Journal of Artificial Intelligence Research, 2017 CSL: Computer Speech \u0026 Language, 2017, 2019 Behaviour \u0026 Information Technology, 2021  Program Committee - Conferences\n ACL: Assoc.~for Computational Linguistics, 2014 (2nd), 2016, 2017, 2018, 2019 NAACL: North American Chapter of the ACL, 2015, 2016, 2018, 2019, 2021 EMNLP: Empirical Methods in NLP, 2015, 2016, 2017, 2018, 2019, 2020, 2021 EACL: European Chapter of the ACL, 2017, 2021 AACL: Asia-Pacific Chapter of the ACL, 2020 CoNLL: Conf.~on Natural Language Learning, 2016, 2017, 2018, 2019, 2020, 2021 IJCNLP: International Joint Conference on Natural Language Processing, 2017 AAAI: Association for the Advancement of Artificial Intelligence, 2016, 2019 IJCAI: International Joint Conference on Artificial Intelligence, 2019 CHI: Conference on Human Factors in Computing Systems, 2019, 2022 CSCW: Computer-Supported Cooperative Work, 2020, 2021 UIST: ACM User Interface Software and Technology Symposium, 2019 WWW / WebConf: World Wide Web Conference, 2016, 2019 ACII: Affective Computing \u0026 Intelligent Interaction, 2021  Program Committee - Workshops\n SRW: Student Research, ACL / NAACL, 2014, 2015, 2016, 2018, 2019, 2020, 2021 (x2), 2022 W-NUT: Noisy User-generated Text, 2018, 2019, 2020, 2021 IWPT: International Conference on Parsing Technologies, 2020, 2021 NLP-CSS: NLP and Computational Social Science, 2019, 2020 EthNLP: Ethics in NLP, 2017 Gen-Deep: New Forms of Generalization in Deep Learning and NLP, 2018 RepEval: Evaluating Vector Space Representations for NLP, 2019 DeepLo: Deep Learning for Low Resource NLP, 2019, 2022 IntEx-SemPar: Interactive and Executable Semantic Parsing, 2020 DaSH-LA: Data Science with Human-in-the-Loop: Language Advances, 2021 WiNLPL Widening NLP, 2021  Other Reviewer\n Tapia Conference Poster Reviewer, 2020 Tapia Doctoral Consortium Judge, 2020  Mentor\n SRW: Student Research Workshop, ACL / NAACL, 2019, 2020 ACL Year-Round Mentorship Program, 2021-2022  Press Interviews and mentions of my work in media:\n “AI Teaches Itself Diplomacy”, IEEE Spectrum, February 2021 “46 - Parsing with Traces, with Jonathan Kummerfeld”, NLP Highlights, December 2017  Misc My academic family tree is: Dan Klein - Chris Manning - Joan Bresnan - Noam Chomsky - Zellig Harris - James Alan Montgomery - Hermann Vollrat Hilprecht - Friedrich Delitzsch and Heinrich Leberecht Fleischer - Unkown and Franz Julius Delitzsch - Unknown\nMy Erdős number is at most 5: J. K. Kummerfeld - D. Klein - J. Andreas - R. Kleinberg - L. Lovász - P. Erdős.\n","categories":"","description":"","excerpt":"Bio Jonathan K. Kummerfeld is a Postdoctoral Research Fellow in …","ref":"/bio_and_cv/","tags":"","title":"Bio \u0026 CV"},{"body":"If you are interested in joining my group, please see the link above.\nCurrent PhD Student Mentees / Collaborators:\n Andrew Lee (University of Michigan) Jordan Huffaker (University of Michigan)  Past PhD Student Mentees / Collaborators:\n Allie Lahnala, (TU Darmstad, PhD student) Catherine Finegan-Dollak, (IBM, Research Staff Member) Charlie Welch, (University of Marburg, Postdoc) Janarthanan Rajendran, (Mila, Postdoc) Laura Burdick, (University of Michigan, Lecturer III) Max Smith Sai R. Gouravajhala, (Lilt, Senior Research Scientist) Stefan Larson Youxuan Lucy Jiang, (Groundspeed Analytics, Senior UX Researcher \u0026 Designer)  Undergraduate \u0026 Masters Mentees:\n Joseph Peper (transfer), Research Opportunity Program, 2016-2017 Thomas Searle (1st year), Research Opportunity Program, 2016-2017 Hyun A. Chung (2nd year), Research Opportunity Program, 2017-2018 Orion Cleaver (2nd year), Research Opportunity Program, 2017-2018 Alexandra Wu (2nd year), Explore CS Research, Spring 2019 Yuyi Qu (2nd year), Explore CS Research, Spring 2019 Yuntian Zhao (2nd year), Explore CS Research, Spring 2020 Prateek Jain (4th year), Explore CS Research, Spring 2020 Joseph Lee (3rd year), Explore CS Research, Spring 2021 Shulin Pan (2nd year), Explore CS Research, Spring 2021 Olivia Garrahan (1st year), CROMA Lab, 2018-2019 Sophia He (2nd year), CROMA Lab, Fall 2018 Varun Kutirakulam (2nd year), CROMA Lab, Fall 2018 Jack Bandon (3rd year), CROMA Lab, Fall 2019 Utkarsh Mehta (3rd year), CROMA Lab, Fall 2019 Kejia Yang (Masters), CROMA Lab, 2018-2019 Andrew Vernier (4th year), CROMA Lab, 2019-2020 Yiming Shi (4th year), CROMA Lab, 2019-2020 Zihan Li (Masters), CROMA Lab, Fall 2019 Vignesh Athreya (Masters), Project Sapphire, Spring 2017 Jiyong Shim (3rd year), Berkeley AI Lab, 2013-2014 Philip Su (3rd year), Intern at Naming Matters, Summer 2015  High School Mentoring / Outreach:\n Australian Informatics Olympiad, 2007–2010 National Computer Science School, 2007, 2008, 2010 Computational Linguistics Olympiads, 2013, 2014, 2015, 2016  ","categories":"","description":"","excerpt":"If you are interested in joining my group, please see the link above. …","ref":"/students/","tags":"","title":"Students"},{"body":"2021 Analyzing the Surprising Variability in Word Embedding Stability Across Languages Laura Burdick, Jonathan K. Kummerfeld, Rada Mihalcea EMNLP, 2021  PDF Show Abstract Show BibTeX    Word embeddings are powerful representations that form the foundation of many natural language processing architectures, both in English and in other languages. To gain further insight into word embeddings, we explore their stability (e.g., overlap between the nearest neighbors of a word in different embedding spaces) in diverse languages. We discuss linguistic properties that are related to stability, drawing out insights about correlations with affixing, language gender systems, and other features. This has implications for embedding use, particularly in research that uses them to study language trends.     @InProceedings{emnlp21stability, title = {Analyzing the Surprising Variability in Word Embedding Stability Across Languages}, author = {Burdick, Laura and Kummerfeld, Jonathan K. and Mihalcea, Rada}, doi = {}, booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2021}, location = {}, pages = {}, url = {}, arxiv = {https://arxiv.org/pdf/2004.14876.pdf}, }     Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health Andrew Lee, Jonathan K. Kummerfeld, Lawrence C. An, Rada Mihalcea Findings of EMNLP, 2021  PDF Show Abstract Show BibTeX    Many statistical models have high accuracy on test benchmarks, but are not explainable, struggle in low-resource scenarios, cannot be reused for multiple tasks, and cannot easily integrate domain expertise. These factors limit their use, particularly in settings such as mental health, where it is difficult to annotate datasets and model outputs have significant impact. We introduce a micromodel architecture to address these challenges. Our approach allows researchers to build interpretable representations that embed domain knowledge and provide explanations throughout the model's decision process. We demonstrate the idea on multiple mental health tasks: depression classification, PTSD classification, and suicidal risk assessment. Our systems consistently produce strong results, even in low-resource scenarios, and are more interpretable than alternative methods.     @InProceedings{emnlp-findings21micromodels, title = {Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health}, author = {Lee, Andrew and Kummerfeld, Jonathan K. and An, Lawrence C. and Mihalcea, Rada}, booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Findings}, month = {November}, year = {2021}, location = {}, pages = {}, url = {https://arxiv.org/pdf/2109.13770.pdf}, doi = {}, arxiv = {https://arxiv.org/abs/2109.13770}, }     Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing Jonathan K. Kummerfeld ACL (short), 2021  PDF Show Abstract Show BibTeX Supplementary Material    Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S. federal minimum wage. Meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage. Through analysis of worker discussions and guidance for researchers, we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the qualifications needed for better paid tasks. We discuss alternatives to this qualification and conduct a study of the correlation between qualifications and work quality on two NLP tasks. We find that it is possible to reduce the burden on workers while still collecting high quality data.     @InProceedings{acl21fair-work, author = {Kummerfeld, Jonathan K.}, title = {Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing}, location = {Online}, booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)}, month = {August}, year = {2021}, url = {https://aclanthology.org/2021.acl-short.44/}, pages = {343--349}, doi = {10.18653/v1/2021.acl-short.44}, arxiv = {https://arxiv.org/abs/2105.12762}, supplementary = {https://aclanthology.org/attachments/2021.acl-short.44.OptionalSupplementaryMaterial.zip}, }     Exploring Self-Identified Counseling Expertise in Online Support Forums Allison Lahnala, Yuntian Zhao, Charles Welch, Jonathan K. Kummerfeld, Lawrence C. An, Kenneth Resnicow, Rada Mihalcea, Verónica Pérez-Rosas Findings of ACL, 2021  PDF Show Abstract Show BibTeX    A growing number of people engage in online health forums, making it important to understand the quality of the advice they receive. In this paper, we explore the role of expertise in responses provided to help-seeking posts regarding mental health. We study the differences between (1) interactions with peers; and (2) interactions with self-identified mental health professionals. First, we show that a classifier can distinguish between these two groups, indicating that their language use does in fact differ. To understand this difference, we perform several analyses addressing engagement aspects, including whether their comments engage the support-seeker further as well as linguistic aspects, such as dominant language and linguistic style matching. Our work contributes toward the developing efforts of understanding how health experts engage with health information- and support-seekers in social networks. More broadly, it is a step toward a deeper understanding of the styles of interactions that cultivate supportive engagement in online communities.     @InProceedings{acl21counseling, author = {Lahnala, Allison and Zhao, Yuntian and Welch, Charles and Kummerfeld, Jonathan K. and An, Lawrence C. and Resnicow, Kenneth and Mihalcea, Rada and P{\\\\'e}rez-Rosas, Ver{\\\\'o}nica}, title = {Exploring Self-Identified Counseling Expertise in Online Support Forums}, location = {Online}, booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, month = {August}, year = {2021}, url = {https://aclanthology.org/2021.findings-acl.392/}, pages = {4467--4480}, doi = {10.18653/v1/2021.findings-acl.392}, }     Chord Embeddings: Analyzing What They Capture and Their Role for Next Chord Prediction and Artist Attribute Prediction Allison Lahnala, Gauri Kambhatla, Jiajun Peng, Matthew Whitehead, Gillian Minnehan, Eric Guldan, Jonathan K. Kummerfeld, Anıl Çamcı, Rada Mihalcea EvoMusArt, 2021  PDF Show Abstract Show BibTeX    Natural language processing methods have been applied in a variety of music studies, drawing the connection between music and language. In this paper, we expand those approaches by investigating chord embeddings, which we apply in two case studies to address two key questions: (1) what musical information do chord embeddings capture?; and (2) how might musical applications benefit from them? In our analysis, we show that they capture similarities between chords that adhere to important relationships described in music theory. In the first case study, we demonstrate that using chord embeddings in a next chord prediction task yields predictions that more closely match those by experienced musicians. In the second case study, we show the potential benefits of using the representations in tasks related to musical stylometrics.     @InProceedings{evomusart21, title = {Chord Embeddings: Analyzing What They Capture and Their Role for Next Chord Prediction and Artist Attribute Prediction}, author = {Lahnala, Allison and Kambhatla, Gauri and Peng, Jiajun and Whitehead, Matthew and Minnehan, Gillian and Guldan, Eric and Kummerfeld, Jonathan K. and Çamcı, Anıl and Mihalcea, Rada}, booktitle = {Proceedings of the 10th International Conference on Artificial Intelligence in Music, Sound, Art and Design}, year = {2021}, month = {April}, url = {https://arxiv.org/pdf/2102.02917.pdf}, pages = {171--186}, }     Overview of the Eighth Dialog System Technology Challenge: DSTC8 Seokhwan Kim, Michel Galley, Chulaka Gunasekara, Sungjin Lee, Adam Atkinson, Peng Baolin, Hannes Schulz, Jianfeng Gao, Jinchao Li, Mahmoud Adada, Minlie Huang, Luis Lastras, Jonathan K. Kummerfeld, Walter S. Lasecki, Chiori Hori, Anoop Cherian, Tim Marks, Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta IEEE: TASLP, 2021  PDF Show Abstract Show BibTeX    This paper introduces the Eighth Dialog System Technology Challenge. In line with recent challenges, the eighth edition focuses on applying end-to-end dialog technologies in a pragmatic way for multi-domain task-completion, noetic response selection, audio visual scene-aware dialog, and schema-guided dialog state tracking tasks. This paper describes the task definition, provided datasets, baselines and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks.     @Article{dstc8-ieee, title = {Overview of the Eighth Dialog System Technology Challenge: DSTC8}, author = {Kim, Seokhwan and Galley, Michel and Gunasekara, Chulaka and Lee, Sungjin and Atkinson, Adam and Baolin, Peng and Schulz, Hannes and Gao, Jianfeng and Li, Jinchao and Adada, Mahmoud and Huang, Minlie and Lastras, Luis and Kummerfeld, Jonathan K. and Lasecki, Walter S. and Hori, Chiori and Cherian, Anoop and Marks, Tim and Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav}, journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing}, publisher = {IEEE}, doi = {10.1109/TASLP.2021.3078368}, year = {2021}, url = {https://ieeexplore.ieee.org/document/9426430}, volume = {29}, number = {1}, pages = {2529--2540}, }     To Batch or Not to Batch? Comparing Batching and Curriculum Learning Strategies across Tasks and Datasets Laura Burdick, Jonathan K. Kummerfeld, Rada Mihalcea Mathematics, 2021  PDF Show Abstract Show BibTeX    Many natural language processing architectures are greatly affected by seemingly small design decisions, such as batching and curriculum learning (how the training data are ordered during training). In order to better understand the impact of these decisions, we present a systematic analysis of different curriculum learning strategies and different batching strategies. We consider multiple datasets for three tasks: text classification, sentence and phrase similarity, and part-of-speech tagging. Our experiments demonstrate that certain curriculum learning and batching decisions do increase performance substantially for some tasks.     @Article{math21batch, author = {Burdick, Laura and Kummerfeld, Jonathan K. and Mihalcea, Rada}, title = {To Batch or Not to Batch? Comparing Batching and Curriculum Learning Strategies across Tasks and Datasets}, journal = {Mathematics}, volume = {9}, year = {2021}, number = {18}, article-number = {2234}, url = {https://www.jkk.name/pub/math21batch.pdf}, issn = {2227-7390}, doi = {10.3390/math9182234} }     Learning to Learn End-to-End Goal-Oriented Dialog From Related Dialog Tasks Janarthanan Rajendran, Jonathan K. Kummerfeld, Satinder Singh EMNLP Workshop: NLP4ConvAI, 2021  PDF Show Abstract Show BibTeX        @InProceedings{ws-emnlp-convai21learn, author = {Rajendran, Janarthanan and Kummerfeld, Jonathan K. and Singh, Satinder}, title = {Learning to Learn End-to-End Goal-Oriented Dialog From Related Dialog Tasks}, location = {Online}, booktitle = {3rd Workshop on NLP for ConvAI}, month = {November}, year = {2021}, url = {https://arxiv.org/pdf/2110.15724.pdf}, pages = {}, doi = {}, arxiv = {https://arxiv.org/abs/2110.15724}, }     2020 Compositional Demographic Word Embeddings Charles Welch, Jonathan K. Kummerfeld, Verónica Pérez-Rosas, Rada Mihalcea EMNLP, 2020  PDF Show Abstract Show BibTeX Blog Post    Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.     @InProceedings{emnlp20demographics, title = {Compositional Demographic Word Embeddings}, author = {Welch, Charles and Kummerfeld, Jonathan K. and P{\\\\'e}rez-Rosas, Ver{\\\\'o}nica and Mihalcea, Rada}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, month = {November}, year = {2020}, location = {Online}, pages = {4076--4089}, url = {https://www.aclweb.org/anthology/2020.emnlp-main.334}, doi = {10.18653/v1/2020.emnlp-main.334}, arxiv = {https://arxiv.org/abs/2010.02986}, blog_post = {/post/2020-10-10_demographicembeddings/}, }     Improving Low Compute Language Modeling with In-Domain Embedding Initialisation Charles Welch, Rada Mihalcea, Jonathan K. Kummerfeld EMNLP (short), 2020  PDF Show Abstract Show BibTeX Supplementary Material Blog Post Software    Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.     @InProceedings{emnlp20lm, title = {Improving Low Compute Language Modeling with In-Domain Embedding Initialisation}, author = {Welch, Charles and Mihalcea, Rada and Kummerfeld, Jonathan K.}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2020}, location = {Online}, pages = {8625--8634}, url = {https://www.aclweb.org/anthology/2020.emnlp-main.696}, supplementary = {https://www.aclweb.org/anthology/attachments/2020.emnlp-main.696.OptionalSupplementaryMaterial.zip}, doi = {10.18653/v1/2020.emnlp-main.696}, software = {https://github.com/jkkummerfeld/emnlp20lm}, arxiv = {https://arxiv.org/abs/2009.14109}, blog_post = {/post/2020-09-29_PretrainingLM/}, }     Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness Stefan Larson, Anthony Zheng, Anish Mahendran, Rishi Tekriwal, Adrian Cheung, Eric Guldan, Kevin Leach, Jonathan K. Kummerfeld EMNLP (short), 2020  PDF Show Abstract Show BibTeX Blog Post    Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our method, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard models. Finally, we show that our approach is complementary to recent work on improving data diversity, and training on data collected with our approach leads to more robust models.     @InProceedings{emnlp20taboo, title = {Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness}, author = {Larson, Stefan and Zheng, Anthony and Mahendran, Anish and Tekriwal, Rishi and Cheung, Adrian and Guldan, Eric and Leach, Kevin and Kummerfeld, Jonathan K.}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, month = {November}, year = {2020}, location = {Online}, pages = {8097--8106}, url = {https://www.aclweb.org/anthology/2020.emnlp-main.650}, doi = {10.18653/v1/2020.emnlp-main.650}, blog_post = {/post/2020-10-10_taboo/}, }     A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels Youxuan Jiang, Huaiyu Zhu, Jonathan K. Kummerfeld, Yunyao Li, Walter Lasecki Findings of EMNLP, 2020  PDF Show Abstract Show BibTeX Blog Post    Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95% accuracy for predicate labels and 93% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56% to 14% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.     @InProceedings{emnlp-findings20srl, title = {A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels}, author = {Jiang, Youxuan and Zhu, Huaiyu and Kummerfeld, Jonathan K. and Li, Yunyao and Lasecki, Walter}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings}, month = {November}, year = {2020}, location = {Online}, pages = {415--421}, url = {https://www.aclweb.org/anthology/2020.findings-emnlp.38}, doi = {10.18653/v1/2020.findings-emnlp.38}, blog_post = {/post/2020-10-04_CrowdSRL/}, }     Crowdsourced Detection of Emotionally Manipulative Language Jordan S. Huffaker, Jonathan K. Kummerfeld, Walter S. Lasecki, Mark S. Ackerman CHI, 2020  PDF Show Abstract Show BibTeX    Detecting rhetoric that manipulates readers’ emotions requires distinguishing intrinsically emotional content (IEC; e.g., a parent losing a child) from emotionally manipulative language (EML; e.g., using fear-inducing language to spread anti-vaccine propaganda). However, this remains an open classifcation challenge for both automatic and crowdsourcing approaches. Machine Learning approaches only work in narrow domains where labeled training data is available, and non-expert annotators tend to confate IEC with EML. We introduce an approach, anchor comparison, that leverages workers’ ability to identify and remove instances of EML in text to create a paraphrased 'anchor text', which is then used as a comparison point to classify EML in the original content. We evaluate our approach with a dataset of news-style text snippets and show that precision and recall can be tuned for system builders’ needs. Our contribution is a crowdsourcing approach that enables non-expert disentanglement of social references from content.     @InProceedings{chi20anchor, author = {Huffaker, Jordan S. and Kummerfeld, Jonathan K. and Lasecki, Walter S. and Ackerman, Mark S.}, title = {Crowdsourced Detection of Emotionally Manipulative Language}, year = {2020}, publisher = {Association for Computing Machinery}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, isbn = {9781450367080}, address = {New York, NY, USA}, doi = {10.1145/3313831.3376375}, pages = {1--14}, location = {Honolulu, HI, USA}, url = {https://www.jkk.name/pub/chi20anchor.pdf}, }     Overview of the seventh Dialog System Technology Challenge: DSTC7 Luis Fernando D'Haro, Koichiro Yoshino, Chiori Hori, Tim K. Marks, Lazaros Polymenakos, Jonathan K. Kummerfeld, Michel Galley, Xiang Gao CSL, 2020  PDF Show Abstract Show BibTeX Data    This paper provides detailed information about the seventh Dialog System Technology Challenge (DSTC7) and its three tracks aimed to explore the problem of building robust and accurate end-to-end dialog systems. In more detail, DSTC7 focuses on developing and exploring end-to-end technologies for the following three pragmatic challenges: (1) sentence selection for multiple domains, (2) generation of informational responses grounded in external knowledge, and (3) audio visual scene-aware dialog to allow conversations with users about objects and events around them. This paper summarizes the overall setup and results of DSTC7, including detailed descriptions of the different tracks, provided datasets and annotations, overview of the submitted systems and their final results. For Track 1, LSTM-based models performed best across both datasets, allowing teams to effectively handle task variants where no correct answer was present or when multiple paraphrases were included. For Track 2, RNN-based architectures augmented to incorporate facts by using two types of encoders: a dialog encoder and a fact encoder plus using attention mechanisms and a pointer-generator approach provided the best results. Finally, for Track 3, the best model used Hierarchical Attention mechanisms to combine the text and vision information obtaining a 22% better result than the baseline LSTM system for the human rating score. More than 220 participants were registered and about 40 teams participated in the final challenge. 32 scientific papers reporting the systems submitted to DSTC7, and 3 general technical papers for dialog technologies, were presented during the one-day wrap-up workshop at AAAI-19. During the workshop, we reviewed the state-of-the-art systems, shared novel approaches to the DSTC7 tasks, and discussed the future directions for the challenge (DSTC8).     @article{csl20dstc, title = {Overview of the seventh Dialog System Technology Challenge: DSTC7}, journal = {Computer Speech \u0026 Language}, pages = {101068}, year = {2020}, issn = {0885-2308}, doi = {https://doi.org/10.1016/j.csl.2020.101068}, url = {https://www.jkk.name/pub/csl20dstc.pdf}, alt-url = {http://www.sciencedirect.com/science/article/pii/S0885230820300012}, data = {https://ibm.github.io/dstc-noesis/public/index.html}, author = {D'Haro, Luis Fernando and Yoshino, Koichiro and Hori, Chiori and Marks, Tim K. and Polymenakos, Lazaros and Kummerfeld, Jonathan K. and Galley, Michel and Gao, Xiang}, }     Inconsistencies in Crowdsourced Slot-Filling Annotations: A Typology and Identification Methods Stefan Larson, Adrian Cheung, Anish Mahendran, Kevin Leach, Jonathan K. Kummerfeld CoLing, 2020  PDF Show Abstract Show BibTeX    Slot-filling models in task-driven dialog systems rely on carefully annotated training data. However, annotations by crowd workers are often inconsistent or contain errors. Simple solutions like manually checking annotations or having multiple workers label each sample are expensive and waste effort on samples that are correct. If we can identify inconsistencies, we can focus effort where it is needed. Toward this end, we define six inconsistency types in slot-filling annotations. Using three new noisy crowd-annotated datasets, we show that a wide range of inconsistencies occur and can impact system performance if not addressed. We then introduce automatic methods of identifying inconsistencies. Experiments on our new datasets show that these methods effectively reveal inconsistencies in data, though there is further scope for improvement.     @InProceedings{coling20svp, title = {Inconsistencies in Crowdsourced Slot-Filling Annotations: A Typology and Identification Methods}, author = {Larson, Stefan and Cheung, Adrian and Mahendran, Anish and Leach, Kevin and Kummerfeld, Jonathan K.}, year = {2020}, month = {December}, booktitle = {Proceedings of the 28th International Conference on Computational Linguistics}, url = {https://www.aclweb.org/anthology/2020.coling-main.442}, pages = {5035--5046}, location = {Barcelona, Spain (Online)}, }     Exploring the Value of Personalized Word Embeddings Charles Welch, Jonathan K. Kummerfeld, Verónica Pérez-Rosas, Rada Mihalcea CoLing (short), 2020  PDF Show Abstract Show BibTeX    In this paper, we introduce personalized word embeddings, and examine their value for language modeling. We compare the performance of our proposed prediction model when using personalized versus generic word representations, and study how these representations can be leveraged for improved performance. We provide insight into what types of words can be more accurately predicted when building personalized models. Our results show that a subset of words belonging to specific psycholinguistic categories tend to vary more in their representations across users and that combining generic and personalized word embeddings yields the best performance, with a 4.7{%} relative reduction in perplexity. Additionally, we show that a language model using personalized word embeddings can be effectively used for authorship attribution.     @InProceedings{coling20personal, title = {Exploring the Value of Personalized Word Embeddings}, author = {Welch, Charles and Kummerfeld, Jonathan K. and P{\\\\'e}rez-Rosas, Ver{\\\\'o}nica and Mihalcea, Rada}, year = {2020}, month = {December}, booktitle = {Proceedings of the 28th International Conference on Computational Linguistics}, pages = {6856--6862}, location = {Barcelona, Spain (Online)}, url = {https://www.aclweb.org/anthology/2020.coling-main.604}, }     2019 No-Press Diplomacy: Modeling Multi-Agent Gameplay Philip Paquette, Yuchen Lu, Steven Bocco, Max O. Smith, Satya Ortiz-Gagné, Jonathan K. Kummerfeld, Joelle Pineau, Satinder Singh, Aaron Courville NeurIPS, 2019  PDF Show Abstract Show BibTeX Supplementary Material Blog Post    Diplomacy is a seven-player non-stochastic, non-cooperative game, where agents acquire resources through a mix of teamwork and betrayal. Reliance on trust and coordination makes Diplomacy the first non-cooperative multi-agent benchmark for complex sequential social dilemmas in a rich environment. In this work, we focus on training an agent that learns to play the No Press version of Diplomacy where there is no dedicated communication channel between players. We present DipNet, a neural-network-based policy model for No Press Diplomacy. The model was trained on a new dataset of more than 150,000 human games. Our model is trained by supervised learning (SL) from expert trajectories, which is then used to initialize a reinforcement learning (RL) agent trained through self-play. Both the SL and RL agents demonstrate state-of-the-art No Press performance by beating popular rule-based bots.     @InProceedings{neurips19diplomacy, author = {Paquette, Philip and Lu, Yuchen and Bocco, Steven and Smith, Max O. and Ortiz-Gagn{\\\\'e}, Satya and Kummerfeld, Jonathan K. and Pineau, Joelle and Singh, Satinder and Courville, Aaron}, title = {No-Press Diplomacy: Modeling Multi-Agent Gameplay}, booktitle = {Advances in Neural Information Processing Systems 32}, year = {2019}, month = {December}, pages = {4476--4487}, url = {https://papers.nips.cc/paper/8697-no-press-diplomacy-modeling-multi-agent-gameplay}, supplementary = {https://papers.nips.cc/paper/8697-no-press-diplomacy-modeling-multi-agent-gameplay-supplemental.zip}, arxiv = {https://arxiv.org/abs/1909.02128}, blog_post = {http://localhost:1313/post/2019-09-13_diplomacynopress/}, }     An Evaluation for Intent Classification and Out-of-Scope Prediction Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, Jason Mars EMNLP (short), 2019  PDF Show Abstract Show BibTeX Data    Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scope---i.e., queries that do not fall into any of the system's supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.     @InProceedings{emnlp19data, title = {An Evaluation for Intent Classification and Out-of-Scope Prediction}, author = {Larson, Stefan and Mahendran, Anish and Peper, Joseph J. and Clarke, Christopher and Lee, Andrew and Hill, Parker and Kummerfeld, Jonathan K. and Leach, Kevin and Laurenzano, Michael A. and Tang, Lingjia and Mars, Jason}, booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2019}, location = {Hong Kong, China}, pages = {1311--1316}, url = {https://www.aclweb.org/anthology/D19-1131/}, doi = {10.18653/v1/D19-1131}, data = {https://github.com/clinc/oos-eval}, arxiv = {https://arxiv.org/abs/1909.02027}, }     A Large-Scale Corpus for Conversation Disentanglement Jonathan K. Kummerfeld, Sai R. Gouravajhala, Joseph J. Peper, Vignesh Athreya, Chulaka Gunasekara, Jatin Ganhotra, Siva Sankalp Patel, Lazaros Polymenakos, Walter S. Lasecki ACL, 2019  PDF Show Abstract Show BibTeX Supplementary Material Blog Post Data Software Poster    Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our data is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. We use our data to re-examine prior work, in particular, finding that 89% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research.     @InProceedings{acl19disentangle, author = {Kummerfeld, Jonathan K. and Gouravajhala, Sai R. and Peper, Joseph J. and Athreya, Vignesh and Gunasekara, Chulaka and Ganhotra, Jatin and Patel, Siva Sankalp and Polymenakos, Lazaros and Lasecki, Walter S.}, title = {A Large-Scale Corpus for Conversation Disentanglement}, location = {Florence, Italy}, booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, month = {July}, year = {2019}, url = {https://www.aclweb.org/anthology/P19-1374.pdf}, pages = {3846--3856}, doi = {10.18653/v1/P19-1374}, arxiv = {https://arxiv.org/abs/1810.11118}, software = {https://www.jkk.name/irc-disentanglement}, data = {https://www.jkk.name/irc-disentanglement}, blog_post = {/post/2019-07-10_disentanglement/}, supplementary = {https://www.aclweb.org/anthology/attachments/P19-1374.Supplementary.pdf}, poster = {https://www.jkk.name/pub/acl19disentangle_poster.pdf}, }     SLATE: A Super-Lightweight Annotation Tool for Experts Jonathan K. Kummerfeld ACL (demo), 2019  PDF Show Abstract Show BibTeX Software Poster    Many annotation tools have been developed, covering a wide variety of tasks and providing features like user management, pre-processing, and automatic labeling. However, all of these tools use a Graphical User Interface, and often require substantial effort for installation and configuration. This paper presents a new annotation tool that is designed to fill the niche of a lightweight interface for users with a terminal-based workflow. Slate supports annotation at different scales (spans of characters, tokens, and lines, or a document) and of different types (free text, labels, and links), with easily customisable keybindings, and unicode support. In a user study comparing with other tools it was consistently the easiest to install and use. Slate fills a need not met by existing systems, and has already been used to annotate two corpora, one of which involved over 250 hours of annotation effort.     @InProceedings{acl19slate, title = {SLATE: A Super-Lightweight Annotation Tool for Experts}, author = {Kummerfeld, Jonathan K.}, booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations}, location = {Florence, Italy}, month = {July}, year = {2019}, pages = {7--12}, url = {https://www.aclweb.org/anthology/P19-3002.pdf}, doi = {10.18653/v1/P19-3002}, software = {https://www.jkk.name/slate}, poster = {https://www.jkk.name/pub/acl19slate_poster.pdf}, }     Outlier Detection for Improved Data Quality and Diversity in Dialog Systems Stefan Larson, Anish Mahendran, Andrew Lee, Jonathan K. Kummerfeld, Parker Hill, Michael Laurenzano, Johann Hauswald, Lingjia Tang, Jason Mars NAACL, 2019  PDF Show Abstract Show BibTeX Data    In a corpus of data, outliers are either errors: mistakes in the data that are counterproductive, or are unique: informative samples that improve model robustness. Identifying outliers can lead to better datasets by (1) removing noise in datasets and (2) guiding collection of additional data to fill gaps. However, the problem of detecting both outlier types has received relatively little attention in NLP, particularly for dialog systems. We introduce a simple and effective technique for detecting both erroneous and unique samples in a corpus of short texts using neural sentence embeddings combined with distance-based outlier detection. We also present a novel data collection pipeline built atop our detection technique to automatically and iteratively mine unique data samples while discarding erroneous samples. Experiments show that our outlier detection technique is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models.     @InProceedings{naacl19outliers, title = {Outlier Detection for Improved Data Quality and Diversity in Dialog Systems}, author = {Larson, Stefan and Mahendran, Anish and Lee, Andrew and Kummerfeld, Jonathan K. and Hill, Parker and Laurenzano, Michael and Hauswald, Johann and Tang, Lingjia and Mars, Jason}, year = {2019}, booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pages = {517--527}, data = {https://github.com/clinc/uniqueness}, month = {June}, arxiv = {https://arxiv.org/abs/1904.03122}, url = {https://www.aclweb.org/anthology/N19-1051.pdf}, doi = {10.18653/v1/N19-1051}, location = {Minneapolis, Minnesota}, }     Look Who’s Talking: Inferring Speaker Attributes from Personal Longitudinal Dialog Charles Welch, Verónica Pérez-Rosas, Jonathan K. Kummerfeld, Rada Mihalcea Best Student Paper - CICLing, 2019  PDF Show Abstract Show BibTeX    We examine a large dialog corpus obtained from the conversation history of a single individual with 104 conversation partners. The corpus consists of half a million instant messages, across several messaging platforms. We focus our analyses on seven speaker attributes, each of which partitions the set of speakers, namely: gender; relative age; family member; romantic partner; classmate; co-worker; and native to the same country. In addition to the content of the messages, we examine conversational aspects such as the time messages are sent, messaging frequency, psycholinguistic word categories, linguistic mirroring, and graph-based features reflecting how people in the corpus mention each other. We present two sets of experiments predicting each attribute using (1) short context windows; and (2) a larger set of messages. We find that using all features leads to gains of 9-14% over using message text only.     @InProceedings{cicling19personal, title = {Look Who's Talking: Inferring Speaker Attributes from Personal Longitudinal Dialog}, year = {2019}, author = {Welch, Charles and P{\\\\'e}rez-Rosas, Ver{\\\\'o}nica and Kummerfeld, Jonathan K. and Mihalcea, Rada}, address = {La Rochelle, France}, booktitle = {Proceedings of the 20th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing)}, publisher = {Springer}, month = {April}, awards = {Best Student Paper Award}, url = {https://arxiv.org/abs/1904.11610}, arxiv = {https://arxiv.org/abs/1904.11610}, }     Learning from Personal Longitudinal Dialog Data Charles Welch, Verónica Pérez-Rosas, Jonathan K. Kummerfeld, Rada Mihalcea IEEE Intelligent Systems, 2019  PDF Show Abstract Show BibTeX    We explore the use of longitudinal dialog data for two dialog prediction tasks: next message prediction and response time prediction. We show that a neural model using personal data that leverages a combination of message content, style matching, time features, and speaker attributes leads to the best results for both tasks, with error rate reductions of up to 15% compared to a classifier that relies exclusively on message content and to a classifier that does not use personal data.     @Article{ieee19personal, author = {Welch, Charles and P{\\\\'e}rez-Rosas, Ver{\\\\'o}nica and Kummerfeld, Jonathan K. and Mihalcea, Rada}, editor = {Erik Cambria}, title = {Learning from Personal Longitudinal Dialog Data}, year = {2019}, journal = {IEEE Intelligent Systems}, volume = {34}, number = {4}, pages = {16--23}, url = {https://ieeexplore.ieee.org/document/8844687}, }     2018 Improving Text-to-SQL Evaluation Methodology Catherine Finegan-Dollak*, Jonathan K. Kummerfeld*, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev ACL, 2018  PDF Show Abstract Show BibTeX Data Software Poster    To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.     @InProceedings{acl18sql, author = {Finegan-Dollak\\\\*, Catherine and Kummerfeld\\\\*, Jonathan K. and Zhang, Li and Ramanathan, Karthik and Sadasivam, Sesh and Zhang, Rui and Radev, Dragomir}, title = {Improving Text-to-SQL Evaluation Methodology}, booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2018}, address = {Melbourne, Victoria, Australia}, pages = {351--360}, doi = {10.18653/v1/P18-1033}, url = {https://aclanthology.org/P18-1033.pdf}, arxiv = {https://arxiv.org/abs/1806.09029}, software = {https://www.jkk.name/text2sql-data}, data = {https://www.jkk.name/text2sql-data}, poster = {https://www.aclweb.org/anthology/attachments/P18-1033.Poster.pdf}, }     Factors Influencing the Surprising Instability of Word Embeddings Laura Burdick, Jonathan K. Kummerfeld, Rada Mihalcea NAACL, 2018  PDF Show Abstract Show BibTeX    Despite the recent popularity of word embedding methods, there is only a small body of work exploring the limitations of these representations. In this paper, we consider one aspect of embedding spaces, namely their stability. We show that even relatively high frequency words (100-200 occurrences) are often unstable. We provide empirical evidence for how various factors contribute to the stability of word embeddings, and we analyze the effects of stability on downstream tasks.     @InProceedings{naacl18embeddings, author = {Burdick, Laura and Kummerfeld, Jonathan K. and Mihalcea, Rada}, title = {Factors Influencing the Surprising Instability of Word Embeddings}, booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, year = {2018}, month = {June}, location = {New Orleans, Louisiana, USA}, url = {https://aclanthology.org/N18-1190.pdf}, pages = {2092--2102}, arxiv = {https://arxiv.org/abs/1804.09692}, doi = {10.18653/v1/N18-1190}, }     Effective Crowdsourcing for a New Type of Summarization Task Youxuan Jiang, Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Walter Lasecki NAACL (short), 2018  PDF Show Abstract Show BibTeX    Most summarization research focuses on summarizing the entire given text, but in practice readers are often interested in only one aspect of the document or conversation. We propose 'targeted summarization' as an umbrella category for summarization tasks that intentionally consider only parts of the input data. This covers query-based summarization, update summarization, and a new task we propose where the goal is to summarize a particular aspect of a document. However, collecting data for this new task is hard because directly asking annotators (e.g., crowd workers) to write summaries leads to data with low accuracy when there are a large number of facts to include. We introduce a novel crowdsourcing workflow, Pin-Refine, that allows us to collect highquality summaries for our task, a necessary step for the development of automatic systems.     @InProceedings{naacl18summary, author = {Jiang, Youxuan and Finegan-Dollak, Catherine and Kummerfeld, Jonathan K. and Lasecki, Walter}, title = {Effective Crowdsourcing for a New Type of Summarization Task}, booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}, pages = {628--633}, year = {2018}, month = {June}, location = {New Orleans, Louisiana, USA}, url = {https://aclanthology.org/N18-2099.pdf}, doi = {10.18653/v1/N18-2099}, }     Data Collection for a Production Dialogue System: A Startup Perspective Yiping Kang, Yunqi Zhang, Jonathan K. Kummerfeld, Parker Hill, Johann Hauswald, Michael A. Laurenzano, Lingjia Tang, Jason Mars NAACL (industry), 2018  PDF Show Abstract Show BibTeX Video    Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities. Crowdsourcing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs. Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis, which may not be indicative of impact on trained model performance. In this paper, we present a study of crowdsourcing methods for a user intent classification task in one of our deployed dialogue systems. Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs. Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems.     @InProceedings{naacl18data, author = {Kang, Yiping and Zhang, Yunqi and Kummerfeld, Jonathan K. and Hill, Parker and Hauswald, Johann and Laurenzano, Michael A. and Tang, Lingjia and Mars, Jason}, doi = {10.18653/v1/N18-3005}, title = {Data Collection for a Production Dialogue System: A Startup Perspective}, booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)}, pages = {33--40}, year = {2018}, month = {June}, location = {New Orleans, Louisiana, USA}, url = {https://aclanthology.org/N18-3005.pdf}, video = {https://vimeo.com/277631102}, }     World Knowledge for Abstract Meaning Representation Parsing Charles Welch, Jonathan K. Kummerfeld, Song Feng, Rada Mihalcea LREC, 2018  PDF Show Abstract Show BibTeX    In this paper we explore the role played by world knowledge in semantic parsing. We look at the types of errors that currently exist in a state-of-the-art Abstract Meaning Representation (AMR) parser, and explore the problem of how to integrate world knowledge to reduce these errors. We look at three types of knowledge from (1) WordNet hypernyms and super senses, (2) Wikipedia entity links, and (3) retraining a named entity recognizer to identify concepts in AMR. The retrained entity recognizer is not perfect and cannot recognize all concepts in AMR and we examine the limitations of the named entity features using a set of oracles. The oracles show how performance increases if it can recognize different subsets of AMR concepts. These results show improvement on multiple fine-grained metrics, including a 6% increase in named entity F-score, and provide insight into the potential of world knowledge for future work in Abstract Meaning Representation parsing.     @InProceedings{lrec18amr, author = {Welch, Charles and Kummerfeld, Jonathan K. and Feng, Song and Mihalcea, Rada}, title = {World Knowledge for Abstract Meaning Representation Parsing}, booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)}, year = {2018}, month = {May}, location = {Miyazaki, Japan}, url = {http://www.lrec-conf.org/proceedings/lrec2018/pdf/1085.pdf}, }     2017 Parsing with Traces: An O(\\(n^4\\)) Algorithm and a Structural Representation Jonathan K. Kummerfeld, Dan Klein TACL, 2017  PDF Show Abstract Show BibTeX Software Video    General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.     @InProceedings{tacl17parsing, author = {Kummerfeld, Jonathan K. and Klein, Dan}, title = {Parsing with Traces: An O($n^4$) Algorithm and a Structural Representation}, booktitle = {Transactions of the Association for Computational Linguistics}, doi = {10.1162/tacl_a_00072}, volume = {5}, issn = {2307-387X}, pages = {441--454}, year = {2017}, url = {https://aclanthology.org/Q17-1031.pdf}, arxiv = {https://arxiv.org/abs/1707.04221}, software = {https://www.jkk.name/1ec-graph-parser/}, interview = {https://soundcloud.com/nlp-highlights/46-parsing-with-traces-with-jonathan-kummerfeld}, video = {https://vimeo.com/238235203}, }     Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation Greg Durrett, Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, Rebecca S. Portnoff, Sadia Afroz, Damon McCoy, Kirill Levchenko, Vern Paxson EMNLP, 2017  PDF Show Abstract Show BibTeX Supplementary Material Software    One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own 'fine-grained domain' in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.     @InProceedings{emnlp17forums, author = {Durrett, Greg and Kummerfeld, Jonathan K. and Berg-Kirkpatrick, Taylor and Portnoff, Rebecca S. and Afroz, Sadia and McCoy, Damon and Levchenko, Kirill and Paxson, Vern}, title = {Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation}, arxiv = {https://arxiv.org/abs/1708.09609}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, doi = {10.18653/v1/D17-1275}, year = {2017}, month = {September}, location = {Copenhagen, Denmark}, url = {https://www.aclweb.org/anthology/D17-1275.pdf}, pages = {2588--2597}, software = {https://evidencebasedsecurity.org/forums/}, supplementary = {https://www.aclweb.org/anthology/attachments/D17-1275.Attachment.zip}, }     Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection Youxuan Jiang, Jonathan K. Kummerfeld, Walter S. Lasecki ACL (short), 2017  PDF Show Abstract Show BibTeX Data Video    Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.     @InProceedings{acl17paraphrase, author = {Jiang, Youxuan and Kummerfeld, Jonathan K. and Lasecki, Walter S.}, doi = {10.18653/v1/P17-2017}, title = {Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, location = {Vancouver, Canada}, pages = {103--109}, url = {https://aclanthology.org/P17-2017.pdf}, data = {https://www.aclweb.org/anthology/attachments/P17-2017.Datasets.zip}, slidespdf = {https://www.aclweb.org/anthology/attachments/P17-2017.Presentation.pdf}, video = {https://vimeo.com/234958413}, arxiv = {https://arxiv.org/abs/1704.05753}, }     Tools for Automated Analysis of Cybercriminal Markets Rebecca S. Portnoff, Sadia Afroz, Greg Durrett, Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, Damon McCoy, Kirill Levchenko, Vern Paxson WWW, 2017  PDF Show Abstract Show BibTeX Software    Underground forums are widely used by criminals to buy and sell a host of stolen items, datasets, resources, and criminal services. These forums contain important resources for understanding cybercrime. However, the number of forums, their size, and the domain expertise required to understand the markets makes manual exploration of these forums unscalable. In this work, we propose an automated, top-down approach for analyzing underground forums. Our approach uses natural language processing and machine learning to automatically generate high-level information about underground forums, first identifying posts related to transactions, and then extracting products and prices. We also demonstrate, via a pair of case studies, how an analyst can use these automated approaches to investigate other categories of products and transactions. We use eight distinct forums to assess our tools: Antichat, Blackhat World, Carders, Darkode, Hack Forums, Hell, L33tCrew and Nulled. Our automated approach is fast and accurate, achieving over 80% accuracy in detecting post category, product, and prices.     @InProceedings{www17forums, title = {Tools for Automated Analysis of Cybercriminal Markets}, author = {Portnoff, Rebecca S. and Afroz, Sadia and Durrett, Greg and Kummerfeld, Jonathan K. and Berg-Kirkpatrick, Taylor and McCoy, Damon and Levchenko, Kirill and Paxson, Vern}, booktitle = {Proceedings of 26th International World Wide Web conference}, pages = {657--666}, month = {April}, year = {2017}, location = {Perth, Australia}, url = {https://www.jkk.name/pub/www17forums.pdf}, software = {http://evidencebasedsecurity.org/forums/}, }     2016 Algorithms for Identifying Syntactic Errors and Parsing with Graph Structured Output Jonathan K. Kummerfeld EECS Department, University of California, Berkeley, 2016  PDF Show Abstract Show BibTeX    Representation of syntactic structure is a core area of research in Computational Linguistics, disambiguating distinctions in meaning that are crucial for correct interpretation of language. Development of algorithms and statistical models over the past three decades has led to systems that are accurate enough to be deployed in industry, playing a key role in products such as Google Search and Apple Siri. However, syntactic parsers today are usually constrained to tree representations of language, and performance is interpreted through a single metric that conveys no linguistic information regarding remaining errors.\\n\\nIn this dissertation, we present new algorithms for error analysis and parsing. The heart of our approach to error analysis is the use of structural transformations to identify more meaningful classes of errors, and to enable comparisons across formalisms. For parsing, we combine a novel dynamic program with careful choices in syntactic representation to create an efficient parser that produces graph structured output. Together, these developments allowed us to evaluate the outstanding challenges in parsing and to address a key weakness in current work.\\n\\nFirst, we present a search algorithm that, given two structures, finds a sequence of modifications leading from one structure to the other. We applied this algorithm to syntactic error analysis, where one structure is the output of a parser, the other is the correct parse, and each modification corresponds to fixing one error. We constructed a tool based on the algorithm and analyzed variations in behavior between parsers, types of text, and languages. Our observations shine light on several assumptions about syntactic errors, showing some to be true and others to be false. For example, prepositional phrase attachment errors are indeed a major issue, while coordination scope errors do not hurt performance as much as expected.\\n\\nNext, we describe an algorithm that builds a parse in one syntactic representation to match a parse in another representation. Specifically, we build phrase structure parses from Combinatory Categorial Grammar derivations. Our approach follows the philosophy of CCG, defining specific phrase structures for each lexical category and generic rules for combinatory steps. The new parse is built by following the CCG derivation bottom-up, gradually building the corresponding phrase structure parse. This produced significantly more accurate parses than past work, and enabled us to compare performance of several parsers across formalisms.\\n\\nFinally, we address a weakness we observed in phrase structure parsers: the exclusion of syntactic trace structures for computational convenience. We present an efficient dynamic programming algorithm that constructs the graph structure that has the highest score under an edge-factored scoring function. We define a parse representation compatible with the algorithm, and show how certain linguistic distinctions dramatically impact coverage. We also show various ways to modify the algorithm to improve performance by exploiting properties of observed linguistic structure. This approach to syntactic parsing is the first to cover virtually all structure encoded in the Penn Treebank.     @PhDThesis{thesis16parsing, title = {Algorithms for Identifying Syntactic Errors and Parsing with Graph Structured Output}, author = {Kummerfeld, Jonathan K.}, year = {2016}, month = {Aug}, location = {Berkeley, CA, USA}, school = {EECS Department, University of California, Berkeley}, number = {UCB/EECS-2016-138}, url = {https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-138.html}, In this dissertation, we present new algorithms for error analysis and parsing. The heart of our approach to error analysis is the use of structural transformations to identify more meaningful classes of errors, and to enable comparisons across formalisms. For parsing, we combine a novel dynamic program with careful choices in syntactic representation to create an efficient parser that produces graph structured output. Together, these developments allowed us to evaluate the outstanding challenges in parsing and to address a key weakness in current work. First, we present a search algorithm that, given two structures, finds a sequence of modifications leading from one structure to the other. We applied this algorithm to syntactic error analysis, where one structure is the output of a parser, the other is the correct parse, and each modification corresponds to fixing one error. We constructed a tool based on the algorithm and analyzed variations in behavior between parsers, types of text, and languages. Our observations shine light on several assumptions about syntactic errors, showing some to be true and others to be false. For example, prepositional phrase attachment errors are indeed a major issue, while coordination scope errors do not hurt performance as much as expected. Next, we describe an algorithm that builds a parse in one syntactic representation to match a parse in another representation. Specifically, we build phrase structure parses from Combinatory Categorial Grammar derivations. Our approach follows the philosophy of CCG, defining specific phrase structures for each lexical category and generic rules for combinatory steps. The new parse is built by following the CCG derivation bottom-up, gradually building the corresponding phrase structure parse. This produced significantly more accurate parses than past work, and enabled us to compare performance of several parsers across formalisms. Finally, we address a weakness we observed in phrase structure parsers: the exclusion of syntactic trace structures for computational convenience. We present an efficient dynamic programming algorithm that constructs the graph structure that has the highest score under an edge-factored scoring function. We define a parse representation compatible with the algorithm, and show how certain linguistic distinctions dramatically impact coverage. We also show various ways to modify the algorithm to improve performance by exploiting properties of observed linguistic structure. This approach to syntactic parsing is the first to cover virtually all structure encoded in the Penn Treebank.}, }     2015 An Empirical Analysis of Optimization for Max-Margin NLP Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, Dan Klein EMNLP (short), 2015  PDF Show Abstract Show BibTeX Software Poster    Despite the convexity of structured max-margin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.     @InProceedings{emnlp15learn, title = {An Empirical Analysis of Optimization for Max-Margin NLP}, author = {Kummerfeld, Jonathan K. and Berg-Kirkpatrick, Taylor and Klein, Dan}, doi = {10.18653/v1/D15-1032}, booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2015}, location = {Lisbon, Portugal}, pages = {273--279}, url = {https://www.jkk.name/pub/emnlp15learn.pdf}, poster = {https://www.jkk.name/pub/emnlp15learn_poster.png}, software = {https://github.com/tberg12/murphy}, }     2013 Error-Driven Analysis of Challenges in Coreference Resolution Jonathan K. Kummerfeld, Dan Klein EMNLP, 2013  PDF Show Abstract Show BibTeX Software Slides    Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.     @InProceedings{emnlp13analysis, title = {Error-Driven Analysis of Challenges in Coreference Resolution}, author = {Kummerfeld, Jonathan K. and Klein, Dan}, booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, month = {October}, year = {2013}, location = {Seattle, Washington, USA}, pages = {265--277}, software = {https://www.jkk.name/berkeley-coreference-analyser/}, url = {https://aclanthology.org/D13-1027.pdf}, slides = {https://www.jkk.name/pub/emnlp13analysis_keynote.key}, slidespdf = {https://www.jkk.name/pub/emnlp13analysis_slides.pdf}, }     An Empirical Examination of Challenges in Chinese Parsing Jonathan K. Kummerfeld, Daniel Tse, James R. Curran, Dan Klein ACL (short), 2013  PDF Show Abstract Show BibTeX Software Slides    Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.     @InProceedings{acl13analysis, title = {An Empirical Examination of Challenges in Chinese Parsing}, author = {Kummerfeld, Jonathan K. and Tse, Daniel and Curran, James R. and Klein, Dan}, booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, location = {Sofia, Bulgaria}, pages = {98--103}, month = {August}, year = {2013}, software = {https://www.jkk.name/berkeley-parser-analyser/}, url = {https://aclanthology.org/P13-2018.pdf}, slides = {https://www.jkk.name/pub/acl13analysis_keynote.key}, slidespdf = {https://www.jkk.name/pub/acl13analysis_slides.pdf}, }     High-velocity Clouds in the Galactic All Sky Survey. I. Catalog Vanessa A. Moss, Naomi M. McClure-Griffiths, Tara Murphy, D. J. Pisano, Jonathan K. Kummerfeld, James R. Curran The Astrophysical Journal Supplement Series, 2013  PDF Show Abstract Show BibTeX    We present a catalogue of high-velocity clouds (HVCs) from the Galactic All Sky Survey (GASS) of southern-sky neutral hydrogen, which has 57 mK sensitivity and 1 km/s velocity resolution and was obtained with the Parkes Telescope. Our catalogue has been derived from the stray-radiation corrected second release of GASS. We describe the data and our method of identifying HVCs and analyse the overall properties of the GASS population. We catalogue a total of 1693 HVCs at declinations \u003c 0 deg, including 1111 positive velocity HVCs and 582 negative velocity HVCs. Our catalogue also includes 295 anomalous velocity clouds (AVCs). The cloud line-widths of our HVC population have a median FWHM of ~19 km/s, which is lower than found in previous surveys. The completeness of our catalogue is above 95% based on comparison with the HIPASS catalogue of HVCs, upon which we improve with an order of magnitude in spectral resolution. We find 758 new HVCs and AVCs with no HIPASS counterpart. The GASS catalogue will shed an unprecedented light on the distribution and kinematic structure of southern-sky HVCs, as well as delve further into the cloud populations that make up the anomalous velocity gas of the Milky Way.     @Article{astro13clouds, title = {High-velocity Clouds in the Galactic All Sky Survey. I. Catalog}, author = {Moss, Vanessa A. and McClure-Griffiths, Naomi M. and Murphy, Tara and Pisano, D. J. and Kummerfeld, Jonathan K. and Curran, James R.}, volume = {209}, number = {1}, pages = {12}, publisher = {IOP Publishing}, journal = {The Astrophysical Journal Supplement Series}, year = {2013}, url = {http://iopscience.iop.org/0067-0049/209/1/12}, arxiv = {https://arxiv.org/abs/1309.4104}, }     2012 Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output Jonathan K. Kummerfeld, David Hall, James R. Curran, Dan Klein EMNLP, 2012  PDF Show Abstract Show BibTeX Software Slides    Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.     @InProceedings{emnlp12analysis, title = {Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output}, author = {Kummerfeld, Jonathan K. and Hall, David and Curran, James R. and Klein, Dan}, booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning}, year = {2012}, pages = {1048--1059}, month = {July}, location = {Jeju Island, South Korea}, software = {https://www.jkk.name/berkeley-parser-analyser/}, url = {https://aclanthology.org/D12-1096.pdf}, slides = {https://www.jkk.name/pub/emnlp12analysis_keynote.key}, slidespdf = {https://www.jkk.name/pub/emnlp12analysis_slides.pdf}, }     Robust Conversion of CCG Derivations to Phrase Structure Trees Jonathan K. Kummerfeld, Dan Klein, James R. Curran ACL (short), 2012  PDF Show Abstract Show BibTeX Software Slides    We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.     @InProceedings{acl12conversion, title = {Robust Conversion of CCG Derivations to Phrase Structure Trees}, author = {Kummerfeld, Jonathan K. and Klein, Dan and Curran, James R.}, booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, year = {2012}, pages = {105--109}, month = {July}, location = {Jeju Island, South Korea}, software = {https://www.jkk.name/berkeley-ccg2pst/}, url = {https://aclanthology.org/P12-2021.pdf}, slides = {https://www.jkk.name/pub/acl12conversion_keynote.key}, slidespdf = {https://www.jkk.name/pub/acl12conversion_slides.pdf}, }     2011 Mention Detection: Heuristics for the OntoNotes annotations Jonathan K. Kummerfeld, Mohit Bansal, David Burkett, Dan Klein CoNLL Shared Task, 2011  PDF Show Abstract Show BibTeX Poster    Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.     @InProceedings{conll11coreference, title = {Mention Detection: Heuristics for the OntoNotes annotations}, author = {Kummerfeld, Jonathan K. and Bansal, Mohit and Burkett, David and Klein, Dan}, booktitle = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task}, year = {2011}, pages = {102--106}, month = {June}, location = {Portland, Oregon, USA}, url = {https://aclanthology.org/W11-1916.pdf}, poster = {https://www.jkk.name/pub/conll11coreference_poster.pdf}, }     2010 Faster Parsing by Supertagger Adaptation Jonathan K. Kummerfeld, Jessika Roesner, Tim Dawborn, James Haggerty, James R. Curran, Stephen Clark ACL, 2010  PDF Show Abstract Show BibTeX Software    We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.     @InProceedings{acl10adapt, title = {Faster Parsing by Supertagger Adaptation}, author = {Kummerfeld, Jonathan K. and Roesner, Jessika and Dawborn, Tim and Haggerty, James and Curran, James R. and Clark, Stephen}, booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics}, month = {July}, year = {2010}, location = {Uppsala, Sweden}, pages = {345--355}, software = {http://downloads.schwa.org/acl10adapt_fast_news_model.tar.bz2}, slidespdf = {https://www.jkk.name/pub/acl10adapt_slides.pdf}, url = {https://aclanthology.org/P10-1036.pdf}, }     Spatiotemporal Hierarchy of Relaxation Events, Dynamical Heterogeneities, and Structural Reorganization in a Supercooled Liquid Raphael Candelier, Asaph Widmer-Cooper, Jonathan K. Kummerfeld, Olivier Dauchot, Giulio Biroli, Peter Harrowell, David R. Reichman Physical Review Letters, 2010  PDF Show Abstract Show BibTeX    We identify the pattern of microscopic dynamical relaxation for a two-dimensional glass-forming liquid. On short time scales, bursts of irreversible particle motion, called cage jumps, aggregate into clusters. On larger time scales, clusters aggregate both spatially and temporally into avalanches. This propagation of mobility takes place along the soft regions of the systems, which have been identified by computing isoconfigurational Debye-Waller maps. Our results characterize the way in which dynamical heterogeneity evolves in moderately supercooled liquids and reveal that it is astonishingly similar to the one found for dense glassy granular media.     @Article{prl10chemistry, title = {Spatiotemporal Hierarchy of Relaxation Events, Dynamical Heterogeneities, and Structural Reorganization in a Supercooled Liquid}, author = {Candelier, Raphael and Widmer-Cooper, Asaph and Kummerfeld, Jonathan K. and Dauchot, Olivier and Biroli, Giulio and Harrowell, Peter and Reichman, David R.}, journal = {Physical Review Letters}, volume = {105}, number = {13}, pages = {135702}, doi = {10.1103/PhysRevLett.105.135702}, numpages = {4}, year = {2010}, month = {September}, publisher = {American Physical Society}, url = {http://prl.aps.org/abstract/PRL/v105/i13/e135702}, arxiv = {https://arxiv.org/abs/0912.0193}, }     Morphological Analysis Can Improve a CCG Parser for English Matthew Honnibal, Jonathan K. Kummerfeld, James R. Curran CoLing, 2010  PDF Show Abstract Show BibTeX    Because English is a low morphology language, current statistical parsers tend to ignore morphology and accept some level of redundancy. This paper investigates how costly such redundancy is for a lexicalised grammar such as CCG.\\n\\nWe use morphological analysis to split verb inflectional suffixes into separate tokens, so that they can receive their own lexical categories. We find that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can benefit from morphological analysis, so long as the analysis is correct.     @InProceedings{coling10morph, title = {Morphological Analysis Can Improve a CCG Parser for English}, author = {Honnibal, Matthew and Kummerfeld, Jonathan K. and Curran, James R.}, booktitle = {Proceedings of the 23rd International Conference on Computational Linguistics}, year = {2010}, pages = {445--453}, location = {Beijing, China}, month = {August}, url = {https://aclanthology.org/C10-2051.pdf}, We use morphological analysis to split verb inflectional suffixes into separate tokens, so that they can receive their own lexical categories. We find that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can benefit from morphological analysis, so long as the analysis is correct.}, }     2009 Adaptive Supertagging for Faster Parsing Jonathan K. Kummerfeld The University of Sydney, 2009  PDF Show Abstract Show BibTeX Poster    Statistical parsers are crucial for tackling the grand challenges of Natural Language Processing. The most effective approaches to these tasks are data driven, but parsers are too slow to be effectively used on large data sets. State-of-the-art parsers generally cannot process more than one sentence a second, and the fastest cannot process more than fifty sentences a second. The situation is even worse when they are applied outside of the domain of their training data. The fastest systems have two components, a parser, which has time complexity O(n3) and a supertagger, which has linear time complexity. By shifting work from the parser to the supertagger we dramatically improve speed.\\n\\nThis work demonstrates several major novel ideas that improve parsing efficiency. The core idea is that the tags chosen by the parser are gold standard data for its supertagger. This leads to the second surprising conceptual development, that decreasing tagging accuracy can improve parsing performance. To demonstrate these ideas required extensive development of the C\u0026C supertagger, including imple- mentation of more efficient estimation algorithms and parallelisation of the training process. This was particularly challenging as the C\u0026C supertagger is a state-of-the-art high performance system designed with a focus on speed rather than flexibility.\\n\\nI was able to significantly improve performance on the standard evaluation corpus by using the parser to generate extremely large new resources for supertagger training. I have also shown that these methods provide significant benefits on another domain, Wikipedia text, without the cost of generating human annotated data sets. These parsing performance gains occur while supertagging accuracy decreases.\\n\\nDespite extensive use of supertaggers to improve parsing efficiency there has been no comprehensive study of the interaction between a supertagger and a parser. I present the first systematic exploration of the relationship, show the potential benefits of understanding it, and demonstrate a novel algorithm for optimising the parameters that define it.\\n\\nI have constructed models that process newspaper text 86% faster than previously, and Wikipedia text 30% faster, without any loss in accuracy and without the aid of extra gold standard resources in either domain. This work will lead directly to improvements in a range of Natural Language Processing tasks by enabling the use of far more parsed data.     @PhDThesis{thesis09adapt, title = {Adaptive Supertagging for Faster Parsing}, author = {Kummerfeld, Jonathan K.}, school = {The University of Sydney}, year = {2009}, location = {Sydney, Australia}, url = {https://www.jkk.name/pub/thesis09adapt_thesis.pdf}, poster = {https://www.jkk.name/pub/thesis09adapt_poster.pdf}, slidespdf = {https://www.jkk.name/pub/thesis09adapt_slides.pdf}, This work demonstrates several major novel ideas that improve parsing efficiency. The core idea is that the tags chosen by the parser are gold standard data for its supertagger. This leads to the second surprising conceptual development, that decreasing tagging accuracy can improve parsing performance. To demonstrate these ideas required extensive development of the C\u0026C supertagger, including imple- mentation of more efficient estimation algorithms and parallelisation of the training process. This was particularly challenging as the C\u0026C supertagger is a state-of-the-art high performance system designed with a focus on speed rather than flexibility. I was able to significantly improve performance on the standard evaluation corpus by using the parser to generate extremely large new resources for supertagger training. I have also shown that these methods provide significant benefits on another domain, Wikipedia text, without the cost of generating human annotated data sets. These parsing performance gains occur while supertagging accuracy decreases. Despite extensive use of supertaggers to improve parsing efficiency there has been no comprehensive study of the interaction between a supertagger and a parser. I present the first systematic exploration of the relationship, show the potential benefits of understanding it, and demonstrate a novel algorithm for optimising the parameters that define it. I have constructed models that process newspaper text 86% faster than previously, and Wikipedia text 30% faster, without any loss in accuracy and without the aid of extra gold standard resources in either domain. This work will lead directly to improvements in a range of Natural Language Processing tasks by enabling the use of far more parsed data.}, }     Faster parsing and supertagging model estimation Jonathan K. Kummerfeld, Jessika Roesner, James R. Curran ALTA, 2009  PDF Show Abstract Show BibTeX    Parsers are often the bottleneck for data acquisition, processing text too slowly to be widely applied. One way to improve the efficiency of parsers is to construct more confident statistical models. More training data would enable the use of more sophisticated features and also provide more evidence for current features, but gold standard annotated data is limited and expensive to produce.\\n\\nWe demonstrate faster methods for training a supertagger using hundreds of millions of automatically annotated words, constructing statistical models that further constrain the number of derivations the parser must consider. By introducing new features and using an automatically annotated corpus we are able to double parsing speed on Wikipedia and the Wall Street Journal, and gain accuracy slightly when parsing Section 00 of the Wall Street Journal.     @InProceedings{alta09tagging, title = {Faster parsing and supertagging model estimation}, author = {Kummerfeld, Jonathan K. and Roesner, Jessika and Curran, James R.}, booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2009}, year = {2009}, pages = {62--70}, location = {Sydney, Australia}, month = {December}, url = {https://aclanthology.org/U09-1009.pdf}, slidespdf = {https://www.jkk.name/pub/alta09tagging_slides.pdf}, We demonstrate faster methods for training a supertagger using hundreds of millions of automatically annotated words, constructing statistical models that further constrain the number of derivations the parser must consider. By introducing new features and using an automatically annotated corpus we are able to double parsing speed on Wikipedia and the Wall Street Journal, and gain accuracy slightly when parsing Section 00 of the Wall Street Journal.}, }     2008 The densest packing of AB binary hard-sphere homogeneous compounds across all size ratios Jonathan K. Kummerfeld, Toby S Hudson, Peter Harrowell The Journal of Physical Chemistry B, 2008  PDF Show Abstract Show BibTeX    This paper considers the homogeneous packing of binary hard spheres in an equimolar stoichiometry, and postulates the densest packing at each sphere size ratio. Monte Carlo simulated annealing optimizations are seeded with all known atomic inorganic crystal structures, and the search is performed within the degrees of freedom associated with each homogeneous AB structure type. Structures isopointal to the FeB structure type are found to have the highest packing fraction at all sphere size ratios. The optimized structures match or improve on the best previously demonstrated packings of this type, and show that compound structures can pack more densely than segregated close-packed structures at all radius ratios less than 0.62.     @Article{chem08packing, title = {The densest packing of AB binary hard-sphere homogeneous compounds across all size ratios}, author = {Kummerfeld, Jonathan K. and Hudson, Toby S and Harrowell, Peter}, journal = {The Journal of Physical Chemistry B}, month = {August}, year = {2008}, volume = {112}, issue = {35}, pages = {10773--10776}, url = {http://pubs.acs.org/doi/abs/10.1021/jp804953r}, }     Classification of Verb Particle Constructions with the Google Web1T Corpus Jonathan K. Kummerfeld, James R. Curran ALTA, 2008  PDF Show Abstract Show BibTeX Poster    Manually maintaining comprehensive databases of multi-word expressions, for example Verb-Particle Constructions (VPCs), is infeasible. We describe a new classifier for potential VPCs, which uses information in the Google Web1T corpus to perform a simple linguistic constituency test. Specifically, we consider the fronting test, comparing the frequencies of the two possible orderings of the given verb and particle. Using only a small set of queries for each verb-particle pair, the system was able to achieve an F-score of 78.4% in our evaluation while processing thousands of queries a second.     @InProceedings{alta08vpc, title = {Classification of Verb Particle Constructions with the Google Web1T Corpus}, author = {Kummerfeld, Jonathan K. and Curran, James R.}, booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2008}, month = {December}, year = {2008}, location = {Hobart, Australia}, pages = {55--63}, volume = {6}, url = {https://aclanthology.org/U08-1008.pdf}, poster = {https://www.jkk.name/pub/alta08vpc_poster.eps}, }     Non-Archival 2020 NOESIS II: Predicting Responses, Identifying Success, and Managing Complexity in Task-Oriented Dialogue Chulaka Gunasekara, Jonathan K. Kummerfeld, Luis Lastras, Walter S. Lasecki AAAI Wokshop: Dialogue System Technology Challenges, 2020  PDF Show Abstract Show BibTeX Data    Real-world conversation often involves more than two participants and complex conversation structures, but most datasets for dialogue research simplify the task to make it more tractable. This shared task built on prior tasks for goal-oriented dialogue, moving towards more realistic settings. Seventeen teams participated in the primary task, predicting the next utterance in a multi-party conversation, and several teams participated in supplementary tasks. All of the datasets have been publicly released, providing a standard benchmark for future work in this space.     @InProceedings{ws-aaai-dstc20task2, title = {NOESIS II: Predicting Responses, Identifying Success, and Managing Complexity in Task-Oriented Dialogue}, author = {Gunasekara, Chulaka and Kummerfeld, Jonathan K. and Lastras, Luis and Lasecki, Walter S.}, year = {2020}, booktitle = {8th Edition of the Dialog System Technology Challenges at AAAI 2019}, month = {January}, url = {https://www.jkk.name/pub/dstc20task2.pdf}, data = {https://github.com/dstc8-track2/NOESIS-II/}, }     Qualification Labour: A Fair Wage Isn’t Enough if Workers Need to Do 5,000 Low Paid Tasks to Qualify for Your Task Jonathan K. Kummerfeld HComp (Work in Progress), 2020  PDF Show Abstract Show BibTeX    Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S.~federal minimum wage. Meanwhile, research on collecting high quality annotations (e.g.~for Natural Language Processing) suggests using qualifications such as a minimum number of previously completed tasks. If most requesters who pay fairly use this kind of minimum qualification, then workers may be forced to complete a substantial amount of poorly paid work for other requesters before they can earn a fair wage. This paper (1) explores current conventions for the threshold, (2) discusses possible alternatives, and (3) presents a study of correlation between approved work and work quality.     @InProceedings{hcomp20fair, title = {Qualification Labour: A Fair Wage Isn't Enough if Workers Need to Do 5,000 Low Paid Tasks to Qualify for Your Task}, author = {Kummerfeld, Jonathan K.}, year = {2020}, month = {October}, booktitle = {The eighth AAAI Conference on Human Computation and Crowdsourcing}, url = {https://www.jkk.name/pub/hcomp20fair.pdf}, }     2019 DSTC7 Task 1: Noetic End-to-End Response Selection Chulaka Gunasekara, Jonathan K. Kummerfeld, Lazaros Polymenakos, Walter S. Lasecki AAAI Wokshop: Dialogue System Technology Challenges, 2019  PDF Show Abstract Show BibTeX Data    Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.     @InProceedings{ws-aaai-dstc19task1, title = {DSTC7 Task 1: Noetic End-to-End Response Selection}, author = {Gunasekara, Chulaka and Kummerfeld, Jonathan K. and Polymenakos, Lazaros and Lasecki, Walter S.}, year = {2019}, booktitle = {7th Edition of the Dialog System Technology Challenges at AAAI 2019}, month = {January}, url = {https://www.jkk.name/pub/ws18dstc_task1.pdf}, alt-url = {http://workshop.colips.org/dstc7/papers/dstc7_task1_final_report.pdf}, data = {https://ibm.github.io/dstc-noesis/public/index.html}, }     The Eighth Dialog System Technology Challenge Seokhwan Kim, Michel Galley, Chulaka Gunasekara, Sungjin Lee, Adam Atkinson, Baolin Peng, Hannes Schulz, Jianfeng Gao, Jinchao Li, Mahmoud Adada, Minlie Huang, Luis Lastras, Jonathan K. Kummerfeld, Walter S. Lasecki, Chiori Hori, Anoop Cherian, Tim K. Marks, Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta NeurIPS Workshop: Conversational AI: Today's Practice and Tomorrow's Potential, 2019  PDF Show Abstract Show BibTeX Data    This paper introduces the Eighth Dialog System Technology Challenge. In line with recent challenges, the eighth edition focuses on applying end-to-end dialog technologies in a pragmatic way for multi-domain task-completion, noetic response selection, audio visual scene-aware dialog, and schema-guided dialog state tracking tasks. This paper describes the task definition, provided datasets, and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks.     @InProceedings{ws-neurips-convai19dstc, title = {The Eighth Dialog System Technology Challenge}, author = {Kim, Seokhwan and Galley, Michel and Gunasekara, Chulaka and Lee, Sungjin and Atkinson, Adam and Peng, Baolin and Schulz, Hannes and Gao, Jianfeng and Li, Jinchao and Adada, Mahmoud and Huang, Minlie and Lastras, Luis and Kummerfeld, Jonathan K. and Lasecki, Walter S. and Hori, Chiori and Cherian, Anoop and Marks, Tim K. and Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav}, year = {2019}, booktitle = {NeurIPS Workshop: Conversational AI: Today's Practice and Tomorrow's Potential}, url = {https://arxiv.org/abs/1911.06394}, location = {Vancouver, Canada}, month = {December}, arxiv = {https://arxiv.org/abs/1911.06394}, data = {https://github.com/dstc8-track2/NOESIS-II/}, }     Training Data Voids: Novel Attacks Against NLP Content Moderation Jordan S. Huffaker, Jonathan K. Kummerfeld, Walter S. Lasecki, Mark S. Ackerman CSCW Workshop: Volunteer Work: Mapping the Future of Moderation Research, 2019  PDF Show Abstract Show BibTeX        @InProceedings{ws-cscw19voids, title = {Training Data Voids: Novel Attacks Against NLP Content Moderation}, author = {Huffaker, Jordan S. and Kummerfeld, Jonathan K. and Lasecki, Walter S. and Ackerman, Mark S.}, year = {2019}, booktitle = {CSCW Workshop: Volunteer Work: Mapping the Future of Moderation Research}, url = {https://www.jkk.name/pub/ws19voids.pdf}, location = {Austin, TX}, month = {November}, }     DSTC7 Task 1: Noetic End-to-End Response Selection Chulaka Gunasekara, Jonathan K. Kummerfeld, Lazaros Polymenakos, Walter S. Lasecki ACL Workshop: NLP for Conversational AI, 2019  PDF Show Abstract Show BibTeX Data        @InProceedings{ws-acl-convai19dstc7, title = {DSTC7 Task 1: Noetic End-to-End Response Selection}, author = {Gunasekara, Chulaka and Kummerfeld, Jonathan K. and Polymenakos, Lazaros and Lasecki, Walter S.}, year = {2019}, booktitle = {Proceedings of the First Workshop on NLP for Conversational AI}, pages = {60--67}, url = {https://www.aclweb.org/anthology/W19-4107}, location = {Florence, Italy}, month = {August}, doi = {10.18653/v1/W19-4107}, data = {https://ibm.github.io/dstc-noesis/public/index.html}, }     2018 Dialog System Technology Challenge 7 Koichiro Yoshino, Chiori Hori, Julien Perez, Luis Fernando D'Haro, Lazaros Polymenakos, Chulaka Gunasekara, Walter S. Lasecki, Jonathan K. Kummerfeld, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, Xiang Gao, Huda Alamari, Tim K. Marks, Devi Parikh, Dhruv Batra NeurIPS Workshop: Conversational AI: Today's Practice and Tomorrow's Potential, 2018  PDF Show Abstract Show BibTeX Data    This paper introduces the Seventh Dialog System Technology Challenges (DSTC), which use shared datasets to explore the problem of building dialog systems. Recently, end-to-end dialog modeling approaches have been applied to various dialog tasks. The seventh DSTC (DSTC7) focuses on developing technologies related to end-to-end dialog systems for (1) sentence selection, (2) sentence generation and (3) audio visual scene aware dialog. This paper summarizes the overall setup and results of DSTC7, including detailed descriptions of the different tracks and provided datasets. We also describe overall trends in the submitted systems and the key results. Each track introduced new datasets and participants achieved impressive results using state-of-the-art end-to-end technologies.     @InProceedings{ws-neurips-convai18dstc, title = {Dialog System Technology Challenge 7}, author = {Yoshino, Koichiro and Hori, Chiori and Perez, Julien and D'Haro, Luis Fernando and Polymenakos, Lazaros and Gunasekara, Chulaka and Lasecki, Walter S. and Kummerfeld, Jonathan K. and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill and Gao, Xiang and Alamari, Huda and Marks, Tim K. and Parikh, Devi and Batra, Dhruv}, year = {2018}, booktitle = {NeurIPS Workshop: Conversational AI: Today's Practice and Tomorrow's Potential}, url = {https://arxiv.org/abs/1901.03461}, location = {Montreal, Quebec, Canada}, month = {December}, arxiv = {https://arxiv.org/abs/1901.03461}, data = {https://ibm.github.io/dstc-noesis/public/index.html}, }     2009 Large-Scale Syntactic Processing: Parsing the Web Stephen Clark, Ann Copestake, James R. Curran, Yue Zhang, Aurelie Herbelot, James Haggerty, Byung-Gyu Ahn, Curt Van Wyk, Jessika Roesner, Jonathan K. Kummerfeld, Tim Dawborn Johns Hopkins University, 2009  PDF Show Abstract Show BibTeX    Scalable syntactic processing will underpin the sophisticated language technology needed for next generation information access. Companies are already using nlp tools to create web-scale question answering and 'semantic search' engines. Massive amounts of parsed web data will also allow the automatic creation of semantic knowledge resources on an unprecedented scale. The web is a challenging arena for syntactic parsing, because of its scale and variety of styles, genres, and domains.\\n\\nThe goals of our workshop were to scale and adapt an existing wide-coverage parser to Wikipedia text; improve the efficiency of the parser through various methods of chart pruning; use self-training to improve the efficiency and accuracy of the parser; use the parsed wiki data for an innovative form of bootstrapping to make the parser both more efficient and more accurate; and finally use the parsed web data for improved disambiguation of coordination structures, using a variety of syntactic and semantic knowledge sources.\\n\\nThe focus of the research was the C\u0026C parser (Clark and Curran, 2007c), a state-of-the-art statistical parser based on Combinatory Categorial Grammar (ccg). The parser has been evaluated on a number of standard test sets achieving state-of-the-art accuracies. It has also recently been adapted successfully to the biomedical domain (Rimell and Clark, 2009). The parser is surprisingly efficient, given its detailed output, processing tens of sentences per second. For web-scale text processing, we aimed to make the parser an order of magnitude faster still. The C\u0026C parser is one of only very few parsers currently available which has the potential to produce detailed, accurate analyses at the scale we were considering.     @TechReport{report09jhu, title = {Large-Scale Syntactic Processing: Parsing the Web}, author = {Clark, Stephen and Copestake, Ann and Curran, James R. and Zhang, Yue and Herbelot, Aurelie and Haggerty, James and Ahn, Byung-Gyu and Wyk, Curt Van and Roesner, Jessika and Kummerfeld, Jonathan K. and Dawborn, Tim}, year = {2009}, institution = {Johns Hopkins University}, url = {https://www.jkk.name/pub/report09jhu.pdf}, The goals of our workshop were to scale and adapt an existing wide-coverage parser to Wikipedia text; improve the efficiency of the parser through various methods of chart pruning; use self-training to improve the efficiency and accuracy of the parser; use the parsed wiki data for an innovative form of bootstrapping to make the parser both more efficient and more accurate; and finally use the parsed web data for improved disambiguation of coordination structures, using a variety of syntactic and semantic knowledge sources. The focus of the research was the C\u0026C parser (Clark and Curran, 2007c), a state-of-the-art statistical parser based on Combinatory Categorial Grammar (ccg). The parser has been evaluated on a number of standard test sets achieving state-of-the-art accuracies. It has also recently been adapted successfully to the biomedical domain (Rimell and Clark, 2009). The parser is surprisingly efficient, given its detailed output, processing tens of sentences per second. For web-scale text processing, we aimed to make the parser an order of magnitude faster still. The C\u0026C parser is one of only very few parsers currently available which has the potential to produce detailed, accurate analyses at the scale we were considering.}, }      ","categories":"","description":"These are my publications and prepints.\n","excerpt":"These are my publications and prepints.\n","ref":"/pubs/","tags":"","title":"Publications"},{"body":"I am not currently teaching. In 2023, I will be teaching “Comp 5046: Natural Language Processing”. I encourage third year students with a Distinction average in CS subjects to take the course!\nPrevious Talks:\n University of Wisconsin-Madison, March 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” Yale University, February 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” 3M / MModal, February 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” Google, February 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” University of Maryland, College Park, February 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” George Mason University, February 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” University of Arizona, Fev 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” Northeastern University, February 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” Virginia Tech, February 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” University of Sydney, January 2021 “You Are What You Train On:\u2028Creating Robust Natural Language Interfaces” University of California, Berkeley, NLP Seminar, February 2018 “Representing Online Conversation Structure with Graphs” Midwest Speech and Language Days \u0026 CL Colloquium, May 2017, May 2018 “Modeling Discourse Structure with Conversation Graphs” University of Macquarie, Centre for Language Tech., June 2016, August 2017 “Parsing with Graph Structured Output” “Extracting Structured Information from Noisy Online Text” Sydney Humans, Language, Technology MeetUp, May 2016, August 2017 “Linguistic structure \u0026 parsing in practice” “Location and Language; Analysing Cybercriminal Markets” University of Sydney, May 2016 “Algorithms for Parsing with Graph Structured Output” Information Sciences Institute, Natural Language Seminar, March 2016 “Capturing More Linguistic Structure with\u2028Graph-Structured Parsing” CommonCrawl, Big Open Data Hackathon, May 2014 “Tasks and Tools in NLP” Cambridge Computer Laboratory, August 2013 “Where did it all go wrong? New Tools for Automatic Error Analysis in NLP” Berkeley Syntax Circle, December 2012 “What’s wrong with this parse tree?”  Instructor:\n Artificial Intelligence, Berkeley, Summer 2014  Teaching Assistant:\n Artificial Intelligence, Berkeley, Fall 2011 Artificial Intelligence (Advanced), Sydney, Fall 2009 Informatics (Advanced), Sydney, Fall 2009  Guest Lecturer:\n DATA3406: Human-in-the-Loop Data Analytics, Semester 2, 2021 “Human-in-the-Loop Natural Language Processing” DATA1002: Informatics: Data and Computation, Semester 2, 2021 “Natural Language Processing” Conversational AI, Michigan, Spring 2020 “NLP: overview, non-DNN approaches, computational linguistics” “NLP: Special topics (research)” STEM Posse, Michigan, Summer 2019 “Workshop building language models” The Anatomy of Natural Language Processing Systems, Michigan, Winter 2018 “The Sapphire Dialogue Project” The Beauty and Joy of Computing, Berkeley, Summer 2014 “Artificial Intelligence” Applied Natural Language Processing, Berkeley, Fall 2013 “Live: Coding a parser to understand the challenges of parsing” Proseminar in the Digital Humanities, Berkeley, Spring 2013 “The latest in Entity Recognition and Syntactic Parsing”  ","categories":"","description":"","excerpt":"I am not currently teaching. In 2023, I will be teaching “Comp 5046: …","ref":"/teaching/","tags":"","title":"Teaching"},{"body":" One-Endpoint Crossing Graph Parser   A range of tools related to one-endpoint crossing graphs - parsing, format conversion, and evaluation. https://jkk.name/1ec-graph-parser      CCG to PST   A tool for converting CCG derivations into PTB-style phrase structure trees. https://jkk.name/berkeley-ccg2pst      Colaboratoy Notebook for Coreference Resolution with SpanBERT   A notebook that (1) sets up the SpanBERT code and model, and (2) runs inference on text you provide. https://colab.research.google.com/drive/1SlERO9Uc9541qv6yH26LJz5IM9j7YVra      Coreference Error Analysis   A tool for classifying errors in coreference resolution. https://jkk.name/berkeley-coreference-analyser      Neural POS tagging   Implementations of a POS tagger in DyNet, PyTorch, and Tensorflow, visualised to show the overall picture and make comparisons easy. https://jkk.name/neural-tagger-tutorial/      Parse Error Analysis   A tool for classifying mistakes in the output of parsers. https://jkk.name/berkeley-parser-analyser      SLATE: A Super-Lightweight Annotation Tool for Experts   A terminal-based text annotation tool in Python. https://jkk.name/slate      Text to SQL Baseline   A simple LSTM-based model that uses templates and slot-filing to map questions to SQL queries. https://jkk.name/text2sql-data/systems/baseline-template/      Adaptive CCG Supertagging Model   A model for the C\u0026C supertagger that gives the same results with smaller beam sizes, enabling faster parsing. https://www.dropbox.com/s/aunrfzys03wco41/acl10adapt_fast_news_model.tar.bz2?dl=0     ","categories":"","description":"","excerpt":" One-Endpoint Crossing Graph Parser   A range of tools related to …","ref":"/software/","tags":"","title":"Software"},{"body":" IE/NER from Cybercriminal Forums   Forum posts with annotations of products. https://evidencebasedsecurity.org/forums/#data      DSTC 7 track 1: Next Utterance Selection   Data from Noetic End-to-End Response Selection Challenge. Dialogue from Ubuntu tech support and Michigan course advising. https://ibm.github.io/dstc-noesis/public/index.html      DSTC 8 track 2: Next Utterance Selection   Data from NOESIS II: Predicting Responses, Identifying Success, and Managing Complexity in Task-Oriented Dialogue. Dialogue from Ubuntu tech support and Michigan course advising. https://github.com/dstc8-track2/NOESIS-II/      IRC Disentanglement   Annotation of IRC messages with reply-to structure, which disentangles simultaneous conversations. The largest such annotated resource. https://jkk.name/irc-disentanglement/      Crowdsourced Paraphrases   Paraphrases collected while conducting experiments on factors influencing crowd performance. https://aclanthology.org/anthology/attachments/P/P17/P17-2017.Datasets.zip      Spine and Arc version of the Penn Treebank   Code to convert the standard Penn Treebank into a version where each word is assigned a spine of non-terminals, and arcs to indicate attachments from one spine to another. https://jkk.name/1ec-graph-parser/format-conversion      Text to SQL datasets   A collection of datasets containing questions in English paired with SQL queries for a provided database. Our version homogenises the style of the SQL and corrects errors in previous versions of the data. https://jkk.name/text2sql-data     ","categories":"","description":"","excerpt":" IE/NER from Cybercriminal Forums   Forum posts with annotations of …","ref":"/data/","tags":"","title":"Datasets"},{"body":"","categories":"","description":"","excerpt":"","ref":"/reading-notes/","tags":"","title":"Reading Notes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/advice/","tags":"","title":"Advice"},{"body":"My work depends on generous support from a range of sources. If you or your organisation is interested in sponsoring projects in my group, please contact me (jkummerf@umich.edu).\nCurrent  Discovery Early Career Researcher Award (DECRA)    Previous  AI Key Scientific Challenges Program, 2017    Data Science Research Grant, 2019    ","categories":"","description":"","excerpt":"My work depends on generous support from a range of sources. If you or …","ref":"/sponsor/","tags":"","title":"Sponsor"},{"body":"My office is at (TBD)\nFor questions on code / data, please raise an issue in the relevant GitHub project.\nFor other questions, email me at jkummerf@umich.edu\nYou can also find me online here:\n  Email    Twitter    LinkedIn    GitHub    ACL Anthology    Semantic Scholar    Google Scholar    DBLP    ArXiv    OrcID    ","categories":"","description":"","excerpt":"My office is at (TBD)\nFor questions on code / data, please raise an …","ref":"/contact/","tags":"","title":"Contact"},{"body":"Semantic parsing datasets are small because they are expensive to produce (logical forms don’t occur naturally and writing them down takes time). The idea here is to do semi-supervised learning by implementing both a parser and a generator, which are trained together as a form of autoencoder where the intermediate representation is natural language.\nThe architecture has four LSTMs:\n Bidirectional LSTM over a logical form. One directional LSTM attending to the first LSTM’s hidden states, generating a sentence. Bidirectional LSTM over the sentence generated by the second LSTM. One directional LSTM attending to the third LSTM’s hidden states, generating a logical form.  Usually a component like the second LSTM would choose the max word at each position (or use beam search), but here they want this whole thing to be differentiable, so the distribution over words is used. At evaluation time only the second half (3+4) is used, with the test sentence as input.\nWith this structure, a loss function is defined that compares the input to (1) and the output of (4), which in both cases is a logical form. As a result, they don’t need (logical form, sentence) pairs to train, they can use automatically generated logical forms. Of course, with only logical forms it would do something random with the intermediate representation, so some supervised examples are also needed (in which case the two halves are trained independently).\nThe results are not state-of-the-art, but good on all three tasks (Geoquery, NLmaps, SAIL), and on two they show am improvement over training (3+4) with only supervised data. Varying the amount of training data gives a less clear picture. On Geoquery with 5-25% of the data, this approach clearly helps, particularly if the queries are real rather than generated (which is a realistic scenario), but then there is no improvement for 50% or 75%, and at 100% the improvement is small. On NLmaps there was no generator, and the differences at different data %s seem like noise. SAIL has the most clear benefit, though it’s a particularly small dataset, consisting of paths in just four maps.\nThis is a cool idea that seems effective in certain situations. The generator is key, and it’s possible that performance on GeoQuery would be higher with a more sophisticated one (e.g. a tree structured generator, rather than the ngram model used here). One idea mentioned in the conclusion is to try reversing the setup (3-4-1-2) and training with natural language examples that have no logical form. How to tradeoff the different data scenarios seems like an interesting challenge!\nCitation Paper\n@InProceedings{kovcisky-EtAl:2016:EMNLP2016, author = {Ko\\v{c}isk\\'{y}, Tom\\'{a}\\v{s} and Melis, G\\'{a}bor and Grefenstette, Edward and Dyer, Chris and Ling, Wang and Blunsom, Phil and Hermann, Karl Moritz}, title = {Semantic Parsing with Semi-Supervised Sequential Autoencoders}, title: = {Semantic Parsing with Semi-Supervised Sequential Autoencoders}, booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2016}, address = {Austin, Texas}, publisher = {Association for Computational Linguistics}, pages = {1078--1087}, url = {https://aclanthology.org/D16-1116} } ","categories":"","description":"By training a parser and language generation system together, we can use semantic parses without associated sentences for training (the sentence becomes a latent representation that is being learnt).","excerpt":"By training a parser and language generation system together, we can …","ref":"/reading-notes/old-blog/2017-10-09_parsing-autoencoder/","tags":"","title":" Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)"},{"body":"Interpreting the behaviour of statistical models in NLP has been hard for a long time, but it has gotten even harder with nonlinear models. The simplest method so far in NLP has been to look at the attention distributions in sequence to sequence models, but that doesn’t provide everything we need and obviously only applies when the model has attention. For looking at the dynamics of the hidden state in an LSTM the Harvard NLP group built a cool visualisation, but what about structured outputs?\nThis paper considers sequence to sequence models and determines which parts of the input were most important for determining each part of the output. The steps are:\n Use a variational autoencoder to get perturbed versions of the input Use logistic regression to get scores for every output symbol indicating how sensitive it is to variations in parts of the input Create a bipartite graph between inputs and outputs, then find high weight components in the graph  These components serve as the representation of which parts of the input determine which parts of the output. Experiments show results that match with past observations and intuitions, which is good for supporting the effectiveness of the method, but it’s a shame this didn’t uncover any exciting new patterns.\nCitation Paper\n@InProceedings{alvarezmelis-jaakkola:2017:EMNLP2017, author = {Alvarez-Melis, David and Jaakkola, Tommi}, title = {A causal framework for explaining the predictions of black-box sequence-to-sequence models}, title: = {A causal framework for explaining the predictions of black-box sequence-to-sequence models}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {412--421}, url = {https://aclanthology.org/D17-1042} } ","categories":"","description":"To explain structured outputs in terms of which inputs have most impact, treat it as identifying components in a bipartite graph where weights are determined by perturbing the input and observing the impact on outputs.","excerpt":"To explain structured outputs in terms of which inputs have most …","ref":"/reading-notes/old-blog/2017-12-05_explainingpredictions/","tags":"","title":"A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)"},{"body":"Attention - a weighted average over a set of vectors representing context - has consistently produced positive results. Here we see an example of how it can be applied in the case of modeling a threaded discussion.\nAttention is applied in two ways. First, over a fixed set of vectors. This is intended to provide a mechanism to choose between several different sub-models contained within a single model. Put differently, the vectors provide a set of latent representations that capture each of the different types of posts in the subreddit. Second, attention over the current utterance is used in the process of predicting responses (at training time only). This provides an additional source of input to the model, by forcing it to explain the response utterances using the same representations as a source of information.\nThe application is a new task, using values assigned to posts = upvotes - downvotes (i.e. Reddit karma). Predicting the specific value is hard, so the task is split into 7 binary decisions about whether a post has a score higher or lower than some value. On this task the new approach provides consistent gains, though overall performance remains low (53 - 56%). Confusingly though, one of the figures (number 4) seems to suggest that it was a single multi-way decision, not a set of binary decisions. I’m also curious about the data, in particular what the distribution of scores is. The paper mentions it is Zipfian, but surely it would be something double-sided with a massive peak at 0 and a rapid drop in either direction?\nOverall, this is further evidence of the versatility of the idea of attention!\nCitation Paper\n@InProceedings{cheng-fang-ostendorf:2017:EMNLP2017, author = {Cheng, Hao and Fang, Hao and Ostendorf, Mari}, title = {A Factored Neural Network Model for Characterizing Online Discussions in Vector Space}, title: = {A Factored Neural Network Model for Characterizing Online Discussions in Vector Space}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2286--2296}, url = {https://aclanthology.org/D17-1242} } ","categories":"","description":"A proposal for how to improve vector representations of sentences by using attention over (1) fixed vectors, and (2) a context sentence.","excerpt":"A proposal for how to improve vector representations of sentences by …","ref":"/reading-notes/old-blog/2017-10-16_forumrnn/","tags":"","title":"A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)"},{"body":"This post is about my own paper to appear at ACL later this month. What is interesting about this paper will depend on your research interests, so that’s how I’ve broken down this blog post.\nA few key points first:\n Data and code are available on Github. The paper is also available. The general-purpose span labeling and linking annotation tool we used is also appearing at ACL. Check out DSTC 8 Track 2, which is based on this work.  You study discourse We investigated discourse structure when multiple conversations are occurring in the same stream of communication. In our case, the stream is a technical support channel for Ubuntu on Internet Relay Chat (IRC). We annotated each message with which message(s) it was a response to. As far as we are aware, this is the first large-scale corpus with this kind of discourse structure in synchronous chat. Here is an example from the data, with annotations marked by edges and colours:\nWe don’t frame the paper as being about reply-structure though. Instead, we focus on a byproduct of these annotations - conversation disentanglement. Given our graph of reply-structure, each connected component is a single conversation (as shown by each colour in the example). The key prior work on the disentanglement problem is Elsner and Charniak (2008), who released the largest annotated resource for the task, with 2,500 messages manually separated into conversations. We annotated their data with our annotation scheme and 75,000 additional messages.\nWe built a set of simple models for predicting reply-structure and did some analysis of assumptions about discourse from prior disentanglement work, but there is certainly more scope for study here. One direction would be to develop better models for this task. Another would be to study patterns in the data to understand how people are able to follow the conversation.\nYou work on dialogue There has been a lot of work recently using the Ubuntu dataset from Lowe et al., (2015), which was produced by heuristically disentangling conversations from the same IRC channel we use. Their work opened up a fantastic research opportunity by providing 930,000 conversations for training and evaluating dialogue systems. However, they were unable to evaluate the quality of their conversations because they had no annotated data.\nUsing our data, we found that only 20% of their conversations are a true prefix of a conversation (since their next utterance classification task cuts the conversation off part-way, being a true prefix is all that matters). Many conversations are missing messages, and some have extra messages from other conversations. Unsurprisingly, our trained model does better, producing conversations that are a true prefix 81% of the time. We also noticed that their heuristic was incorrectly linking messages far apart in time. This is not tested by our evaluation set, so we constructed this figure, which shows the problem is quite common:\nThe purple results are based on the output of our model over the entire Ubuntu IRC logs. That output is the basis of DSTC 8 Track 2. Once the competition finishes (October 20th, 2019) we will release all of the conversations.\nYou am interested in studying online communities This is not my area of expertise, but our data and models could enable the exploration of interesting questions. For example:\n What is the structure of the community? By looking at who asks for help and who responds we could see patterns of behaviour. How does a community evolve over time? This data spans 15 years, during which there were many Ubuntu releases, Stackoverflow was created, other Ubuntu forums were created, etc. It seems likely that those events and more would be reflected in the data.  It would be interesting to apply the model to other communities, but that would require additional in-domain data to get good results. We have no plans to collect additional data at this stage, and for other channels there are copyright questions that might be difficult to resolve (the Ubuntu channels have an open access license).\nYou mainly care about neural network architectures We experimented with a bunch of ideas that didn’t improve performance, so our final model is very simple (a feedforward network with features representing the logs and sentences represented by averaging and max-pooling GloVe embeddings). Maybe that means there is an opportunity for you to improve on our results with a fancy model? One of our motivations for making such a large new resource was to make it possible to train sophisticated models.\nAcknowledgments This project has been going since I started at Michigan as a postdoc funded by a grant from IBM. The final paper is the result of collaboration with a large group of people from Michigan and IBM. Thank you!\nCitation Paper\n@InProceedings{acl19disentangle, author = {Kummerfeld, Jonathan K. and Gouravajhala, Sai R. and Peper, Joseph and Athreya, Vignesh and Gunasekara, Chulaka and Ganhotra, Jatin and Patel, Siva Sankalp and Polymenakos, Lazaros and Lasecki, Walter S.}, title = {A Large-Scale Corpus for Conversation Disentanglement}, title: = {A Large-Scale Corpus for Conversation Disentanglement}, booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, location = {Florence, Italy}, month = {July}, year = {2019}, url = {https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf}, arxiv = {https://arxiv.org/abs/1810.11118}, software = {https://jkk.name/irc-disentanglement}, data = {https://jkk.name/irc-disentanglement}, } ","categories":"","description":"","excerpt":"This post is about my own paper to appear at ACL later this month. …","ref":"/reading-notes/old-blog/2019-07-10_disentanglement/","tags":"","title":"A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)"},{"body":"The classic NER system is a model that has a lot of curated features, like lists of people, and does inference by choosing the top scoring tag sequence for the whole sentence, using Viterbi decoding. The neural version swaps the curated features for word vectors, and viterbi inference for an LSTM (maybe with beam search). This paper makes the argument that in reality people are very good at identifying an entity in isolation, so why do global decoding for the best tag sequence?\nGiven that perspective, they make a model that scores every span of the sentence independently using a feedforward network. To get an input representing context, they use a weighted sum of word embeddings, where the weights decay exponentially further from the span (FOFE = Fixed-size Ordinally Forgetting Encoding). The authors point out that this gives a fixed length encoding that could be reversed to recover the original sequence (assuming arbitrary precision floating point numbers). Thinking about the calculation though, a word ten positions away is having its vector scaled down by a factor of a thousand, so it probably has negligible impact on the decision. They also apply this idea to the characters of the span itself in both directions.\nOne tradeoff with the independent classification idea is that it can select overlapping spans. This is a benefit in one sense, because it naturally handles nested entities (e.g. “[Member of the Order of [Australia]]\"), but for partially overlapping spans we have to decide which to keep. Their solution is to sort by model score and keep the higher scoring option.\nThe experiments show this is comparable with previous work using LSTMs. There were a few things I found interesting in the results:\n The FOFE encoding for characters is far worse than a CNN encoding when on their own, but give similar gains when combined with word level features. Since the FOFE essentially ignores the centre of long spans, this suggests they are both learning some representation of prefixes and suffixes. They don’t try it, but this model seems very amenable to gazetteers, which may be a way to further boost performance. They have an in-house dataset of 10,000 manually labeled documents (!), but it only gives a 3% gain on the KBP evaluation.  Citation Paper\n@InProceedings{xu-jiang-watcharawittayakul:2017:Long, author = {Xu, Mingbin and Jiang, Hui and Watcharawittayakul, Sedtawut}, title = {A Local Detection Approach for Named Entity Recognition and Mention Detection}, title: = {A Local Detection Approach for Named Entity Recognition and Mention Detection}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1237--1247}, url = {https://aclanthology.org/P17-1114} } ","categories":"","description":"Effective NER can be achieved without sequence prediction using a feedforward network that labels every span with a fixed attention mechanism for getting contextual information.","excerpt":"Effective NER can be achieved without sequence prediction using a …","ref":"/reading-notes/old-blog/2017-12-01_nonsequencener/","tags":"","title":"A Local Detection Approach for Named Entity Recognition and Mention Detection (Xu et al., 2017)"},{"body":"My previous post discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions. This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.\nThe core new idea is a filtering process in which workers identify incorrect answers for a task. This is the first step of a three stage process:\n Five workers iteratively filter the options for a label (either for a predicate or argument) until there are only three. Five workers select the correct answer. If the workers disagree or any of them indicates uncertainty, ask an expert to annotate the example.  To make this work, we use an automatic system for identifying spans for predicates and arguments. This is better than going straight to the second step because it makes the set of labels less overwhelming, focusing effort on the subtle distinctions between the options.\nThis mixture of crowd and expert effort achieves high accuracy (94%) while only having 12% of examples annotated by experts. The cost is about 52 cents per label for crowd work plus the cost of the expert.\nIt’s a little tricky to compare the cost with prior work. Comparing to other work on SRL, we spend more on the crowd, but less on experts. Whether that trade-off is worth it will depend on the cost of experts. In practise, our experts are often members of the research team and so their time is a stronger constraint than the crowdsourcing budget. In that case, our approach comes out ahead as we can get more data annotated per unit of expert effort (by a factor of four). The QA-SRL work is quite a bit cheaper, at 54 cents per predicate with 2.9 roles on average (which would be ~$2 + expert effort for our approach), but the type of annotations collected are quite different, with ours providing labels from the sense inventory in Propbank.\nI see a range of interesting potential improvements for future work. First, bringing in methods of worker training in order to improve their accuracy and so reduce the need for duplicate effort. Second, combining with ideas from other work, such as having a model deciding whether examples are easy or hard and changing how they are processed accordingly, or using QA-SRL annotation to inform the process.\nCitation Paper\nMy Tweet\n@InProceedings{emnlp-findings20srl, title = {A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels}, title: = {A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels}, author = {Jiang, Youxuan and Zhu, Huaiyu and Kummerfeld, Jonathan K. and Li, Yunyao and Lasecki, Walter}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, shortvenue = {Findings of EMNLP}, month = {November}, year = {2020}, location = {Online}, url = {https://www.jkk.name/pub/emnlp-findings20srl.pdf}, abstract = {Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95\\% accuracy for predicate labels and 93\\% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56\\% to 14\\% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.}, } ","categories":"","description":"My [previous post](https://www.jkk.name/post/2020-09-25_crowdqasrl/) discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions. This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.","excerpt":"My [previous post](https://www.jkk.name/post/2020-09-25_crowdqasrl/) …","ref":"/reading-notes/old-blog/2020-10-04_crowdsrl/","tags":"","title":"A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)"},{"body":"To construct word vectors from multi-domain data, use a separate vector for each domain and add a loss term to encourage them to agree. Here the loss is an l2 norm, weighted by a factor that depends on the frequency of the words in the two domains. The factor is the harmonic mean of the normalised frequency in each domain (so the lower frequency dominates the factor, pulling it lower). Across a range of tasks this consistently performs better than other approaches.\nCitation Paper\n@InProceedings{yang-lu-zheng:2017:EMNLP2017, author = {Yang, Wei and Lu, Wei and Zheng, Vincent}, title = {A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings}, title: = {A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2898--2904}, url = {https://aclanthology.org/D17-1312} } ","categories":"","description":"To leverage out-of-domain data, learn multiple sets of word vectors but with a loss term that encourages them to be similar.","excerpt":"To leverage out-of-domain data, learn multiple sets of word vectors …","ref":"/reading-notes/old-blog/2017-12-12_multidomainwordvector/","tags":"","title":"A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings (Yang et al., 2017)"},{"body":"Over the last few years interest has risen in parsing structures other than projective trees (including my dissertation!). There are now a range of different datasets with annotations for syntactic and/or semantic structure that include discontinuous constituents and graphs. This paper looks at UCCA, a proposed formalism that is somewhat similar to SRL, with non-terminals included to allow for easier handling of cases like coordination.\nThe parser is a transition based, with a transition system that covers all the structural phenomena in UCCA: non-terminals, discontinuous spans, and multiple parents. The key to consistent multiple parents is distinguishing the addition of edges that are the primary parent (to prevent multiple being added). To get discontinuity, they use a swap operation. They consider a range of models, including both linear and neural network examples.\nThe dataset is relatively small, with only 4,268 training sentences, and the task is hard, so performance is relatively low (50 - 75 for primary edges, 20-50 for others). The neural model consistently beats the linear ones, particularly for the non-primary edges. Comparing to other standard parsers (retrained on this data), the ability to generate the full space of structures makes a big difference.\nIt would be interesting to see coverage of this data for one-endpoint crossing graphs. If it is high, then my own parser could be applied fairly directly!\nCitation Paper\n@InProceedings{hershcovich-abend-rappoport:2017:Long, author = {Hershcovich, Daniel and Abend, Omri and Rappoport, Ari}, title = {A Transition-Based Directed Acyclic Graph Parser for UCCA}, title: = {A Transition-Based Directed Acyclic Graph Parser for UCCA}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1127--1138}, url = {https://aclanthology.org/P17-1104} } ","categories":"","description":"Parsing performance on the semantic structures of UCCA can be boosted by using a transition system that combines ideas from discontinuous and constituent transition systems, covering the full space of structures.","excerpt":"Parsing performance on the semantic structures of UCCA can be boosted …","ref":"/reading-notes/old-blog/2017-11-16_ucca/","tags":"","title":"A Transition-Based Directed Acyclic Graph Parser for UCCA (Hershcovich et al., 2017)"},{"body":"Discourse parsing for Rhetorical Structure Theory is difficult partly because it involves a range of relation types at different scales (within and between sentences) and partly because there is relatively little annotated data available. To deal with the limited data, this paper breaks the task into two parts: (1) identify relations, (2) assign labels. Their system is state-of-the-art, and an ablation shows that the division of tasks helps performance. They also divide up the labeling step to have different classifiers for within sentences, between sentences in the same paragraph, and between paragraphs, which also helps a little.\nI find the second improvement surprising, since an expanded feature set for a single classifier would be able to emulate their multi-classifier model, while having the advantage of sharing information between classes. The first improvement is more intuitive (a denser space makes for an easier problem), though I wonder whether this will be one point on the back-and-forth that usually occurs between sequential and joint models (with joint models usually winning in the end). This paper also continues the trend of transition-based inference applying effectively to tasks, which makes sense if our models are getting good enough that search errors are not a major issue.\nCitation Paper\n@InProceedings{wang-li-wang:2017:Short, author = {Wang, Yizhong and Li, Sujian and Wang, Houfeng}, title = {A Two-Stage Parsing Method for Text-Level Discourse Analysis}, title: = {A Two-Stage Parsing Method for Text-Level Discourse Analysis}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {184--188}, url = {https://aclanthology.org/P17-2029} } ","categories":"","description":"Breaking discourse parsing into separate relation identification and labeling tasks can boost performance (by dealing with limited training data).","excerpt":"Breaking discourse parsing into separate relation identification and …","ref":"/reading-notes/old-blog/2017-11-17_twostagediscourseparsing/","tags":"","title":"A Two-Stage Parsing Method for Text-Level Discourse Analysis (Wang et al., 2017)"},{"body":"Most effective summarisation systems are extractive, selecting the most important sentences in a document and sticking them together. Clearly that is not how people write summaries, but creating abstractive summaries means generating fluent language. At the same time, most datasets are based on news text, where the first few sentences are a strong baseline summary (by design, as journalists need to assume that the reader could stop at any point). This paper introduces several ideas to get state-of-the-art results on summarisation using an abstractive system.\nThere are three core new ideas, one for decoding and two for the model. The idea in decoding is a beam search in which the score is increased when adding bigrams that occur in the source but are not in the output. In the model, they propose a new form of attention based on PageRank, similar to previous methods used for ranking sentences in summarisation. For every pair of sentences plus the current decoder hidden vector, a similarity score is calculated ($h_1 M h_2$), where $M$ is a matrix of parameters. This produces a matrix of similarities, which they run PageRank on with initialisation set so that all weight starts on the decoder hidden vector. That produces a score for each input sentence, which is normalised to get attention values. The second idea is that they don’t want to attend to the same sentence multiple times, so before normalising they subtract the previous score for that sentence (with it capped at 0 to avoid negative values).\nTogether, these lead to state of the art results, beating both extractive and abstractive systems. Though in human evaluation using the first three sentences as a summary remains a very strong baseline, only slightly behind this system on informativeness and ahead on coherence and fluency. Ablation shows that the decoding idea has the biggest impact, but the graph based attention does help. Interestingly, if the score in decoding is extremely biased to focus on the bigram addition aspect performance only decreases a little. That may reflect the nature of the metric, which is based on ngram overlap.\nThere are also a bunch of little details that may be crucial, like adding markers for entities (which seems like a possible space for a more elegant solution). I’m not sure the beam search scoring idea has applications beyond summarisation, but thee modified attention might!\nCitation Paper\n@InProceedings{tan-wan-xiao:2017:Long, author = {Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo}, title = {Abstractive Document Summarization with a Graph-Based Attentional Neural Model}, title: = {Abstractive Document Summarization with a Graph-Based Attentional Neural Model}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1171--1181}, url = {https://aclanthology.org/P17-1108} } ","categories":"","description":"Neural abstractive summarisation can be dramatically improved with a beam search that favours output that matches the source document, and further improved with attention based on PageRank, with a modification to avoid attending to the same sentence more than once.","excerpt":"Neural abstractive summarisation can be dramatically improved with a …","ref":"/reading-notes/old-blog/2017-11-29_abstractivesummarisation/","tags":"","title":"Abstractive Document Summarization with a Graph-Based Attentional Neural Model (Tan et al., 2017)"},{"body":"Academic posters need to convey the core point of your work, without overwhelming the viewer with detail (they can read the paper for that). A few general principles:\n Be consistent in style Keep text short Make sure content is large enough to read  There are two general stages in making a poster:\n Find an effective layout. Put all the key figures and a rough version of the text down and move them around. Don’t worry about the details at this point, and come up with a bunch of different options, then stand back and compare them. Fine tune. Many of the notes below are about this - getting the poster to look polished by cleaning up details.  What to include  Title and authors. Some indication of the institution. Key figures that explain your contribution. A brief explanation of key prior work. A QR code to the paper / software / data. If you do include one, make sure it is big, so people can scan it from a distance.  What to leave out  References, people can read the paper for them. Giant results tables. Experiments that aren’t critical for the core story.  Layout There are many approaches to designing posters. Some effective approaches are:\n A collection of boxes. A single key takeaway sentence that is really big, then additional information on the sides. A single graphic / thing that captures the key idea in the paper.  General notes:\n Don’t go too close to the edges (might get cut off in printing). Use space in proportion to how much something matters. This means university logos should be small, while the key figures explaining what you did should be large. If you have a box containing text, make sure the space between the text and the edge of the box is the same on all four sides. If you have multiple similarly shaped objects (e.g. a set of boxes containing text) make sure they are either aligned or far from aligned (being a few pixels off looks sloppy). This applies to text, shapes, figures, tables - anything that occurs more than once. Keep the spacing between sections consistent in width, and if there are both vertical and horizontal gaps, keep them consistent too.  Figures  Make sure all the text is big enough (axis labels, values, the legend, etc). Consider showing a subset of the results to make graphs easier to read.  Misc  Text should be fully justified, meaning the lines are a consistent width. For posters this can often look bad at first because words have to be spaced out too much. In that case, edit the text to make it fit nicely. Do not use ALL CAPS Strike a balance between monotone and a disco ball of colour. Check spelling and grammar carefully.  Printing For a local event, print anywhere and carry it to the event.\nFor an event you have to fly to, either:\n Print at home, buy a poster tube, and carry the poster with you. Order a fabric poster. You can pack this in your luggage, just remember to unwrap it when you arrive and try to steam it (e.g. use a hotel iron, or hang it in the bathroom so the steam from the shower gets wrinkles out). Print in the place you are going. In the US there are FedEx stores everywhere and you can order the poster to be printed online (just make sure to do it early enough, a week is generally safe). Outside of the US you should search online. In either case, make sure you have a planned time to pick up the poster at least a day before you present (that way if something goes wrong you have time to get it reprinted, usually for an additional fee).  I’ve done all of these and like (3) the best. Fabric posters have more muted colours, and carrying a poster both ways is a pain.\n","categories":"","description":"How do make a poster that effectively conveys the contributions of your work.\n","excerpt":"How do make a poster that effectively conveys the contributions of …","ref":"/advice/posters/","tags":"","title":"Academic Posters"},{"body":"This is another paper concerned with the challenge of sparsity in AMR parsing, specifically that there are an enormous number of output symbols in the parse trees and most are seen infrequently. The system they develop is based on the encoder-decoder with attention approach, which has previously done poorly for AMR, partially because of sparsity.\nTheir solution is to merge certain types of symbols into groups (dates, named entities, rare verbs, constants, etc) and have a standard way to map from the surface form to the output symbol. This is an alternative to the approach from the paper I wrote about last week. They also introduce a completely separate idea, which is a different way to take an AMR graph and turn it into a linear sequence. This change is necessary to make the output follow the form their model generates - a sequence (though there has been work on tree based LSTMs on the output side, so AMR could be directly generated, and I believe there has been some work on applying that to AMR).\nTogether these changes do substantially improve performance over previous encoder-decoder based work for AMR. However, there is still a substantial gap between the system and state-of-the-art, presumably because of the additional resources that other systems indirectly use by running external systems for NER, dependency parsing, etc. Given the recent success of multi-task learning with neural nets, it would be interesting to see if those resources could be used here to further boost performance. It may also be productive to combine these ideas with the graph abstraction ideas from AMR alignment paper.\nCitation Paper\n@InProceedings{peng-EtAl:2017:EACLlong1, author = {Peng, Xiaochang and Wang, Chuan and Gildea, Daniel and Xue, Nianwen}, title = {Addressing the Data Sparsity Issue in Neural AMR Parsing}, title: = {Addressing the Data Sparsity Issue in Neural AMR Parsing}, booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers}, month = {April}, year = {2017}, address = {Valencia, Spain}, publisher = {Association for Computational Linguistics}, pages = {366--375}, url = {https://aclanthology.org/E17-1035} } ","categories":"","description":"Another paper looking at the issue of output symbol sparsity in AMR parsing, though here the solution is to group the consistent but rare symbols (rather than graph fragments like the paper last week). This drastically increases neural model performance, but does not reach the level of hybrid systems.","excerpt":"Another paper looking at the issue of output symbol sparsity in AMR …","ref":"/reading-notes/old-blog/2017-10-18_neuralamr/","tags":"","title":"Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)"},{"body":"Assigning a probability distribution over the next word or character in a sequence (language modeling) is a useful component of many systems, such as speech recognition and translation. Recently neural networks have come to dominate in performance, with a range of clever innovations in network structure. This paper is not about new models, but rather explores the current evaluation and how well carefully tuned baseline models can do.\nThe key observations for me were:\n There are issues with the PTB dataset for character-level evaluation - it removes all punctuation, makes numbers ‘N’, and removes rare words (i.e. it is a character-level version of the token-level task). Given that the original Penn Treebank exists, I would have been interested to see a comparison with the PTB without any simplification. The other dataset, enwik8, makes sense as a testing ground for compression algorithms, but is a little odd for modeling language, since it is the first 100 million bytes of a Wikipedia XML dump. The paper does have another dataset, WikiText, which sounds good, but then there is no character-level evaluation! The LSTM is able to achieve ~SotA results for character-level modeling. The key seems to be careful design of the softmax that produces the final probability distribution: (1) rare words are clustered and represented by a single value in the distribution calculation, and (2) word vectors are shared between input and output. Dropout matters more than the network design, and multiple forms of dropout should be tuned jointly. This comes from analysis of a set of models trained with random variation in hyperparameters.  Citation Paper\n@Article{2018arXiv180308240M, author = {Merity, S. and Shirish Keskar, N. and Socher, R.}, title = {An Analysis of Neural Language Modeling at Multiple Scales}, title: {An Analysis of Neural Language Modeling at Multiple Scales}, journal = {ArXiv e-prints}, year = {2018}, url = {https://arxiv.org/abs/1803.08240}, } ","categories":"","description":"Assigning a probability distribution over the next word or character in a sequence (language modeling) is a useful component of many systems...","excerpt":"Assigning a probability distribution over the next word or character …","ref":"/reading-notes/old-blog/2018-04-16_lm_analysis/","tags":"","title":"An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)"},{"body":"Am I getting the most out of the time I put into conferences? This year NAACL and ACL ran mentoring programs to help newer members of the community and in the process of giving advice I started to question whether my own approach to conferences was effective. Most online advice is aimed at students attending for the first time. What about a more experienced researcher? I’ve fallen into certain patterns without stepping back to think about whether they are effective and what could be better.\nDuring sessions There are four main options during a session in the main conference:\n Attend a talk. Pro: Typically the most exciting work is presented in talks. Con: Most talks are poorly presented. Even in a good talk, it is easy to sit and listen but not take it in. Attend the poster session. Pro: Easy to spend more or less time on each poster. Con: Easy to start skimming titles and figures without learning much (particularly when tired). Talk to people. Pro: A great way to (1) learn about what other people are doing / thinking about right now, (2) make connections / network, (3) make specific people aware of your own work. Con: Sometimes conversations become just small talk, which is not always worthwhile (Edit: see clarification in the postscript). It can also be hard to approach new people. Relax. Pro: Being focused and engaged is tiring, a break can be rejuvenating. Con: Missing out on work being presented.  There is no perfect fixed combination of these. The strategy I want to try next time is:\n  Over breakfast, choose 2-3 talks from each session that cover must-see work for my interests. Look through posters for the topics I have an interest in and flag must-see items. If the conference provides breakfast, eat elsewhere to do this and then show up for the last 30 minutes to chat.\n  When a session starts, if I’m in an interesting conversation, keep talking. Otherwise, go to the talks selected that morning and drop by posters in between.\n  Take notes in the conference handbook on every talk I attend and every poster I do more than read the title of.\n  This minimises the number of decisions during the day and focuses on the work I am most interested in. Of the options above, taking a break isn’t included. I’m hoping this approach (and more below) will make that unnecessary in general.\nDuring breaks At the first few conferences I went to, I was constantly meeting new people because I didn’t know anyone. Now, I tend to talk to the same people at every conference. I do want to catch up with those people, but it means I’m not making the most of the diverse group the conference brings together.\nWho should I be trying to meet? As a student, I was most interested in meeting faculty working in my area. Now, I want to look for (1) the students doing work I am excited about and (2) faculty at places I intend to apply to. My reasoning on the students is that (1) they will have more to say about their work than their advisor, (2) they are the future of the field, and (3) if there is scope for collaboration then it will be easier to get their advisor on board if they are excited than vice versa.\nHow should I try to meet people? Conferences are so big these days that simply hoping to bump into someone won’t work. One solution is to contact people ahead of time and plan to meet during a specific break. The same idea can apply to lunch. Usually I have just joined lunch groups in an ad hoc way, but having a plan for the nucleus of a group and then picking up more people on the day would be more effective.\nIn the evening I always go to the conference receptions (ie. Welcome / Social) and plan to continue. The key questions in my mind are about industry hosted events and when to go to sleep.\nI usually don’t get invited to industry events these days. One thing I could do differently is be more proactive in talking to people from the companies hosting events to tell them I am interested. When I am invited, I think it is worth going as it’s another chance to meet people in a setting where it is easy to join and leave conversations (like conference breaks).\nSleep is crucial for conferences. I’ve held that opinion for a while, but on reflection I have not gone far enough. It’s easy to keep hanging out and stay up late then set an alarm to make the morning session. Almost all people need 7-8 hours of sleep, not counting the time before we fall asleep, and the specific sleep hours need to be consistent (see “Why We Sleep” by Matthew Walker). There are two options here: either plan to go back to the hotel at 10pm to be up at 7am, or plan to miss the morning session, staying out till 12am, waking up at 9am. On the topic of sleep, I’m noticing jet lag more as I get older and while the university won’t pay for lodging before the conference, I should seriously consider it anyway (but not tire myself out by doing in a million tourist activities).\nWorkshop days Previously I’ve jumped between workshops, trying to squeeze in every keynote talk from someone whose work I am interested in. In future, I plan to take go to a single workshop all day. Reflecting on the keynotes I’ve seen, most are just conference talks stitched together. I’m not learning a lot that is new. In contrast, being at a single workshop means engaging with a sub-community.\nConclusion In many ways, the plans outlined above are very similar to what I do already. I am eager to see how the changes work out and also hope that having stepped back like this I will feel less uncertain about my choices at the next conference.\nWhile putting this together I found this series of tweets from Chinmay Kulkarni interesting. Our opinions differ on certain points, so check them out: https://twitter.com/chinmay/status/988410612316286976?s=20\nPostscript: Responses After sharing this on Twitter, there was some interesting discussion. For posterity, I’m summarising that and other comments I got here:\nGeneral\n  Be willing to leave a poster, talk, or conversation. It may feel polite to stay, but time is valuable, so once something no longer seems interesting, move on to something else. This is one reason to sit on the aisle in a talk.\n  Note taking is important because there is simply too much happening to remember. One suggestion was to track who you talked to and what it was about.\n  Consider going to random things and trying to understand them. This can lead to unexpected links to your own work and may be more interesting (as everything is new).\n  I clarified my small talk point above. I definitely see it is valuable, but if every conversation is small talk then you are missing an opportunity to have a conversation you couldn’t have outside of a conference.\n  People vary in their preferences regarding staying up late or not. Some see it is valuable time to connect. Others get the same from morning activities like running groups.\n  Talks vs. Posters\n  Talks can give a lot of content in a brief period and convey the presenter’s view of the most important idea.\n  Staying for a complete talk session can expose you to work that is relevant to your interests, but you might have otherwise missed.\n  Talks are often recorded now, so you can watch them later instead (but be honest with yourself about whether you will).\n  Demos and Industry track presentations may have content not in the paper.\n  Attending a talk is a nice way to connect with someone. It means you are guaranteed to find them and there is a starting place for conversation.\n  Interactions at posters can make you feel part of the community in a way talks don’t.\n  As the community grows we may need to explore other structures. For example, at RSS, work is presented as both a four minute talk and a poster session (NAACL tried a 1-minute-madness at least once, which is a similar idea).\n  ","categories":"","description":"Am I getting the most our of time at conferences? This post was a way for me to think through that question and come up with strategies.","excerpt":"Am I getting the most our of time at conferences? This post was a way …","ref":"/reading-notes/old-blog/2019-09-22_conferenceapproach/","tags":"","title":"Approaching Conferences"},{"body":"This paper brings together work on neural dependency parsing with the idea of non-terminal spines as a way to represent constituency structure. Within the transition parsing inference process they can naturally fit the generation of a new spines by gradually building up the spine, which makes for a very elegant inference process.\nSurprisingly, it doesn’t seem to matter what head choices are used to generate the spines (they tried leftmost word, rightmost word, and two standard schemes). This contrasts with my own observations that the choice of head had a big impact (0.5 F) on accuracy. I think the incrementally-built spines are the key difference. Decisions about higher up in the spine are difficult to make when looking at a single word, but with the incremental construction there is information about a larger context.\nCitation Paper\n@InProceedings{ballesteros-carreras:2017:IWPT, author = {Ballesteros, Miguel and Carreras, Xavier}, title = {Arc-Standard Spinal Parsing with Stack-LSTMs}, title: = {Arc-Standard Spinal Parsing with Stack-LSTMs}, booktitle = {Proceedings of the 15th International Conference on Parsing Technologies}, month = {September}, year = {2017}, address = {Pisa, Italy}, publisher = {Association for Computational Linguistics}, pages = {115--121}, url = {https://aclanthology.org/W17-6316} } ","categories":"","description":"Stack-LSTM models for dependency parsing can be adapted to constituency parsing by considering spinal version of the parse and adding a single 'create-node' operation to the transition-based parsing scheme, giving an elegant algorithm and competitive results.","excerpt":"Stack-LSTM models for dependency parsing can be adapted to …","ref":"/reading-notes/old-blog/2017-11-07_spineparsinglstm/","tags":"","title":"Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)"},{"body":"Recurrent neural networks like LSTMs and GRUs have limited scope for parallelisation because each step depends on the one before it. This architecture also means that many steps of computation separate two words that are far apart, making it difficult to capture long-distance relations. A range of approaches have been used to try to address these issues, such as convolutional structures and other forms of recurrence (e.g. QRNNs). The idea in this work is to use attention, applied multiple times, to get a network that is fast while still capturing positional information.\nTo explain the structure I put together the figure below, which captures the network structure with a few simplifications:\nThere are a few ideas being brought together here:\n Positional encoding, which is a vector of the same length as the word representation, but that depends only on the position in the input. Here they use $f(pos, dim) = sin(pos / 10000^{2 dim / d_w})$ for even dimensions and the cosine equivalent for odd dimensions (where $d_w$ is the number of dimensions. Multi-head attention, where rather than running attention once on the full vector, multiple linear transforms are applied to get smaller vectors. Scaled dot product attention, the equation is shown in the figure, the key new idea is to rescale by the square root of the dimensionality so that larger vectors don’t produce excessively sharp distributions after the softmax is applied. The more general form of this described in the paper has keys ($K$), queries ($Q$) and values ($V$), but the network uses the same vector for the key and value. I show the query with a solid line and the values/keys with dotted lines. The matrix $V$ is formed by using the $v$ vectors as rows, while $Q$ is formed by duplicating $q$ in every row. Note, on the left hand side attention is over all input words, while on the right it is only over the words produced so far (ie. content to the left). Layer normalisation, a way to rescale weights to keep vector outputs in a nice range, from Ba, Kiros and Hinton (ArXiv 2016). Other details, (1) When the outputs are words, the vectors used to initially represent each input word are also used to represent the outputs and in the final linear transformation (though with some rescaling). (2) They use a formula I haven’t seen before to adjust the learning rate during training, (3) dropout in several places and label smoothing are used for regularization.  Simplifications in the figure:\n For multi-head attention I only show two transforms, while in practise they used 8. The shaded regions are duplicated 6 times to create a stack, with the output of one region acting as the input to the next copy of it. The links from left to right are always from the top of the input stack. The musical repeat signs indicate that the structure is essentially the same. On the output side this isn’t quite true since the attention boxes only take inputs to their left (since output to the right doesn’t exist when they are being calculated).  In terms of experiments, it works at least as well if not better than prior approaches, and is a lot faster for machine translation (no speed numbers are given for parsing). There is also some nice analysis of what it ends out using the attention mechanism to focus on for each word. It seems like it can provide a way to effectively disambiguate the sense of a word based on its context.\nCitation ArXiv Paper\nGoogle also has some blog posts up about the paper and about the library they released.\n@article{arxiv:1706.03762, author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia}, title = {Attention Is All You Need}, title: = {Attention Is All You Need}, journal = {ArXiv}, year = {2017}, url = {http://arxiv.org/abs/1706.03762}, } ","categories":"","description":"To get context-dependence without recurrence we can use a network that applies attention multiple times over both input and output (as it is generated).","excerpt":"To get context-dependence without recurrence we can use a network that …","ref":"/reading-notes/old-blog/2017-10-20_onlyattention/","tags":"","title":"Attention Is All You Need (Vaswani et al., ArXiv 2017)"},{"body":"Attention, a weighted average over vectors with weights determined based on context (usually decoder state), has proven effective in many NLP tasks. There are several variants, and this paper adds new types that address the question of how to apply attention to different sources at the same time, such as text and an image.\nThey consider three general versions:\n Concatenation, just do attention separately then concatenate the vectors from the input sources Flat, do the weighted average over all of the inputs Hierarchical, do attention separately, but then combine the vectors with another phase of attention  They also explore two variations that are orthogonal to the list above:\n The first step and the last step in attention both involve the input vectors being multiplied by a weight matrix. Should that matrix be shared for the two steps, or different? (the first informs the decision of what to give high weight in the average, the second determines what is being averaged over) sentinel gates, a modification to the way the inputs and context vector are combined that allow one or the other to be ignored.  They consider two tasks, (1) translation when both an image and source sentence are given, (2) post-editing a translated sentence with the original source given. The results show fairly clear trends, though the systems are not great compared to baselines (worse than a text only baseline for the first, and only slightly better than a direct MT system for the second). The trends are that hierarchical is best, the sentinel doesn’t help, and it is better to not share weights (though I wonder if that would be true when controlling for the total number of parameters).\nCitation Paper\n@InProceedings{libovicky-helcl:2017:Short, author = {Libovick\\'{y}, Jind\\v{r}ich and Helcl, Jind\\v{r}ich}, title = {Attention Strategies for Multi-Source Sequence-to-Sequence Learning}, title: = {Attention Strategies for Multi-Source Sequence-to-Sequence Learning}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {196--202}, url = {https://aclanthology.org/P17-2031} } ","categories":"","description":"To apply attention across multiple input sources, it is best to apply attention independently and then have a second phase of attention over the summary vectors for each source.","excerpt":"To apply attention across multiple input sources, it is best to apply …","ref":"/reading-notes/old-blog/2017-11-21_multiinputattention/","tags":"","title":"Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)"},{"body":"It is difficult to predict how well a model will work in the real world. Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to. This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories. Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.\nThe paper organises these tests along two axes. One is the type of test:\n Invariance: Giving the same answer when changes are made that should not impact the model prediction. Directional: Giving an answer that differs in a way that matches the intended impact of a change. Minimum Function Tests: A range of other tests that consider specific cases.  The other axis is the linguistic property being varied:\n Vocabulary Change Named Entity Variation Temporal Shift Negation Semantic Role Swap Various Other Changes  For example, an invariance test on vocabulary would be that replacing words with their synonyms should not change the result.\nThe paper tests the idea on (1) sentiment analysis on SST-2, (2) identifying matching questions on QQP, and (3) machien comprehension on SQuAD. Researchers / developers using the method are more effective at finding issues than those asked to write tests without this framework to approach the problem.\nUnderstanding system errors has been an interest of mine for a long time now (back to my 2012 parsing paper) and from my experience with startups it is definitely challenging to develop effective tests for NLP models. I’m curious to see how this approach works out when used iteratively. When users modify their model or data to address the problems do they actually fix them or just overfit to the new set of tests? Another open question is how to apply these to problems with more structured output (e.g. text-to-SQL). Some would easily apply, e.g. invariance tests, while others would be more difficult.\nCitation Paper\n@inproceedings{ribeiro-etal-2020-beyond, title = \"Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist\", title: \"Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist\", author = \"Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\", month = \"jul\", year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2020.acl-main.442\", doi = \"10.18653/v1/2020.acl-main.442\", pages = \"4902--4912\", abstract = \"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.\", } ","categories":"","description":"It is difficult to predict how well a model will work in the real world. Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to. This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories. Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.","excerpt":"It is difficult to predict how well a model will work in the real …","ref":"/reading-notes/old-blog/2020-09-03_checklist/","tags":"","title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)"},{"body":"Natural language interfaces to computer systems are an exciting area with new workshops (WNLI at ACL and IntEx-SemPar at EMNLP), a range of datasets (including my own work on text-to-SQL), and many papers. Most work focuses on either (1) commands for simple APIs, (2) generating a database query, or (3) generating general purpose code. This paper considers an interesting application: interaction with data visualisation tools.\nUsing the full flexibility of these tools is a tall order, so this work focuses on commands to modify style parameters of a figure. For that setting, the problem can be framed as task-oriented dialogue in which each style parameter (e.g. x-axis font size) is a slot that needs to be defined. Using this framing of the problem, the paper presents a new dataset of 3,200 conversations in which a person modifies the style of a plot. These were collected on Mechanical Turk by having one worker describe a target plot and another worker manipulating values for parameters to match it. There are 12 plot types with 3-13 properties, with the target plot randomly generated. Baseline approaches do fairly well, but far short of a human (either another worker or one of the authors).\nIt’s a large resource with high agreement between annotators and the paper presents detailed analysis and helpful examples. One experiment I’d be curious to see is results with a fixed number of training examples per plot type (or per slot type). Histograms and scatter plots appear particularly difficult in the breakdown of results by plot type, but they are also the types with the fewest examples (a tenth as many as the type with the most).\nI find this general topic exciting because it brings together several areas of NLP and it seems feasible to create a useful system in the near future. Hopefully there will be progress on models for this dataset and development of additional resources. In particular, there was a decision here to limit generation to slot-values, which is powerful, but does not capture the full flexibility of matplotlib (at least not without further work on representing more features this way). Arbitrary code generation would be a fantastic extension, though creating the data would require some creativity as the approach used here wouldn’t directly work.\nCitation Paper\n@inproceedings{shao-nakashole-2020-chartdialogs, title = \"{C}hart{D}ialogs: {P}lotting from {N}atural {L}anguage {I}nstructions\", title: \"{C}hart{D}ialogs: {P}lotting from {N}atural {L}anguage {I}nstructions\", author = \"Shao, Yutong and Nakashole, Ndapa\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\", month = \"jul\", year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2020.acl-main.328\", doi = \"10.18653/v1/2020.acl-main.328\", pages = \"3559--3574\", abstract = \"This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61{\\%} plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.\", } ","categories":"","description":"Natural language interfaces to computer systems are an exciting area with new workshops ([WNLI](https://aclanthology.org/volumes/2020.nli-1/) at ACL and [IntEx-SemPar](https://intex-sempar.github.io/) at EMNLP), a range of datasets (including my own work on [text-to-SQL](/publication/acl18sql/)), and many papers. Most work focuses on either (1) commands for simple APIs, (2) generating a database query, or (3) generating general purpose code. This paper considers an interesting application: interaction with data visualisation tools.","excerpt":"Natural language interfaces to computer systems are an exciting area …","ref":"/reading-notes/old-blog/2020-09-07_chartdialogs/","tags":"","title":"ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)"},{"body":"Most work in NLP uses datasets with a diverse set of speakers. In practise, everyone speaks / writes slightly differently and our models would be better if they accounted for that. This has been the motivation for a line of work by Charlie Welch that I’ve been a collaborator on (in CICLing 2019, IEEE Intelligent Systems 2019, CoLing 2020, and this paper).\nHere, the question is how to improve language modeling for a new user of a service who voluntarily provided some demographic information, but you have no other data for. Our solution is a language model that (1) has a separate word embedding space for each individual demographic value, and (2) forms a word embedding for a given user by composing the embeddings for their demographics. In experiments on Reddit, this leads to improvements in performance for all demographic groups.\nIn the process, we also developed a way to extract demographics of Reddit users. Prior work has either inferred demographics or looked at flairs (labels in user profiles). We use self-reported information in posts, such as “I am a [blah]”. We use simple regular expressions, which are enough to get two or more demographic values for 61,000 users. There is also relatively little overlap with a flair based method (less than 0.5% of ours are in a set based on flairs).\nIt is important to note that a range of ethical issues exist around the use of demographics in machine learning. We discuss a range of issues in the paper, but I also wanted to mention a few here. First, to collect our data, we identified self-reported demographics in Reddit text. This avoids some of the problems with inferring demographics, but it does mean our sample is biased (it only contains people who wish to publicly share demographics online). Second, we must consider how our work may be used. There is a potential positive (improved performance for specific groups), but also the risk that in order to use our ideas developers require users to disclose information or try to infer it automatically. Third, there is the risk that our work is interpreted as implying that how someone speaks is a consequence of their demographics. For more detailed discussion of these and other issues see the “Limitations and Ethical Considerations” section of the paper.\nCitation Paper\n@InProceedings{emnlp20demographics, title = {Compositional Demographic Word Embeddings}, title: = {Compositional Demographic Word Embeddings}, author = {Welch, Charles and Kummerfeld, Jonathan K. and P{\\'e}rez-Rosas, Ver{\\'o}nica and Mihalcea, Rada}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2020}, location = {Online}, url = {https://arxiv.org/pdf/2010.02986.pdf}, abstract = {Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.}, } ","categories":"","description":"Most work in NLP uses datasets with a diverse set of speakers. In practise, everyone speaks / writes slightly differently and our models would be better if they accounted for that. This has been the motivation for a line of work by [Charlie Welch](http://cfwelch.com/) that I've been a collaborator on (in [CICLing 2019](https://www.jkk.name/publication/cicling19personal), [IEEE Intelligent Systems 2019](https://www.jkk.name/publication/ieee19personal/), [CoLing 2020](https://www.jkk.name/publication/coling20personal/), and this paper).","excerpt":"Most work in NLP uses datasets with a diverse set of speakers. In …","ref":"/reading-notes/old-blog/2020-10-10_demographicembeddings/","tags":"","title":"Compositional Demographic Word Embeddings (Welch et al., EMNLP 2020)"},{"body":"Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments. Over the last few years, Luke Zettlemoyer’s Group has been exploring using question-answer pairs to represent this structure. This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank. However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.\nI got three main things out of this paper:\n It shifted my approach to crowdsourcing to consider workers more like traditional expert annotators. It reinforced the idea that small shifts in crowd workflows can have a major impact on annotation quality. QA-SRL can capture roles not covered by PropBank.  The work also provides a new dataset that will be useful for future work on this problem, and useful benchmarks of systems and measurements of data quality. Expanding on the three points above:\nCrowd workers: The paper argues in favour of putting more time into training workers. Most of the work I’ve seen in NLP for crowdsourcing (including my own) focuses on modifying task design or using ML post-processing to improve results. Here, they run a large-scale qualification task and filter workers based on their performance, then train those workers by paying them to read a set of instructions (23 text-dense slides) and do two small annotation rounds with feedback after each one. This increases the upfront cost, but reduces the cost of annotation by reducing the need for multiple annotations of each item. The paper doesn’t provide quite enough detail to quantify the cost. We do know that to get to 11 workers they needed to train 30 workers at a cost of 2 hours each plus 30 minutes of researcher time each. If we assume 60 workers did the preliminary round, each taking 5 minutes, and that workers cost $12 / hour ($10 to the workers, $2 to Amazon), that’s almost $800 plus 15 hours of researcher time. For a large annotation effort, the savings during annotation will make that worth it (or, as in this case, it will lead to higher quality data). I am curious which aspect was more important though - filtering the pool of workers, or training workers.\nWorkflow impact: In previous QA-SRL work, one worker wrote a question and its answers and two workers checked the question and independently added answers. Here, two workers independently write a question+answer and a third work consolidates the annotations into a final annotation. The cost for a label is about the same (54c / predicate vs. 51c / predicate), but coverage is considerably higher. The design space for crowd workflows is huge and this is another example of how important it is to explore. It’s also possible that the changes in recruitment and training were more critical than the workflow shift, but the study didn’t include evaluation with only one or the other.\nQA-SRL vs. PropBank: This may be less surprising to someone who works more on SRL, but they found their approach captured many implicit roles that PropBank does not. Specifically, of 100 annotated arguments that were not in PropBank, 68 were valid implicit arguments. I’m curious about what those implicit arguments are capturing. Maybe targeted re-annotation could be used to add them to PropBank (identifying relevant sentences by trace parsing).\nCitation Paper\nCode\nMy Tweet\n@inproceedings{roit-etal-2020-controlled, title = \"Controlled Crowdsourcing for High-Quality {QA}-{SRL} Annotation\", title: \"Controlled Crowdsourcing for High-Quality {QA}-{SRL} Annotation\", author = \"Roit, Paul and Klein, Ayal and Stepanov, Daniela and Mamou, Jonathan and Michael, Julian and Stanovsky, Gabriel and Zettlemoyer, Luke and Dagan, Ido\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\", month = \"jul\", year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2020.acl-main.626\", doi = \"10.18653/v1/2020.acl-main.626\", pages = \"7008--7013\", abstract = \"Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.\", } ","categories":"","description":"Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments.  Over the last few years, [Luke Zettlemoyer's Group](https://www.cs.washington.edu/people/faculty/lsz/) has been exploring using question-answer pairs to represent this structure.  This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank.  However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.","excerpt":"Semantic Role Labeling captures the content of a sentence by labeling …","ref":"/reading-notes/old-blog/2020-09-25_crowdqasrl/","tags":"","title":"Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)"},{"body":"This paper proposes two techniques for speeding up neural network execution on GPUs:\n Reduce computation when doing matrix-multiply by removing rows. Reduce communication on the GPU by halving the number of bits used to represent numbers.  Either of these gives a speed up of ~1.5x and together they give ~2x, across a range of different computer vision tasks+models.\nCore ideas in detail The first idea, reducing work by eliminating parts of the computation, has been considered before. In the past, however, the focus was on saving memory in models, and so the most common strategy was to move to a sparse matrix where weights close to zero are dropped. Here the focus is on speed and they show that while the sparse approach saves memory it can end up being slower because of hardware behaviour. Instead, they eliminate entire rows of the matrix, which means there is less computation, but it remains dense (and therefore fast). Rows are identified by measuring correlation between outputs and greedily eliminating rows that correlate highly with the rest of the output.\nThe natural question to ask is whether this hurts performance. First, they do two things to avoid problems, (1) a scale factor is used to make sure the outputs are of the same range that they would have been with the full matrix, and (2) they restart training to fine-tune the network once pruning is set up. With high enough pruning accuracy does fall, but speed ups can be gained before that is a problem (the exact point depends on the task).\nThe second idea relates to numerical representation, and is motivated by measurements of where the bottlenecks are in communication. Many AI researchers have tried switching to 16 bit representations to save space and time, but here they develop a different floating point encoding that gives more bits to the exponent, and fewer to the mantissa.\nThoughts  It would be interesting to see the interaction of this work with the investigation of networks without non-linear functions that can still learn non-linear behaviour because of numerical approximations. In the context of language, the weight reduction approach would be interesting to analyse. Specifically, what do we lose in our word vectors depending on the task? I’ve always had some interest in making things faster. It would be interesting to know where the remaining bottlenecks are (after applying these changes).  Citation @InProceedings{Hill:MICRO:2017, author = {Hill, Parker and Jain, Animesh and Hill1, Mason and Zamirai, Babak and Hsu, Chang-Hong and Laurenzano, Michael A. and Mahlke, Scott and Tang, Lingjia and Mars, Jason}, title = {DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission}, title: {DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission}, booktitle = {The 50th Annual IEEE/ACM International Symposium on Microarchitecture}, year = {2017}, } ","categories":"","description":"GPU processing can be sped up ~2x by removing low impact rows from weight matrices, and switching to a specialised floating point representation.","excerpt":"GPU processing can be sped up ~2x by removing low impact rows from …","ref":"/reading-notes/old-blog/2017-10-05-deftnn/","tags":"","title":"DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)"},{"body":"Active learning doesn’t seem to get much attention in NLP, probably because of fear that developing data based on the errors of one model will introduce a particular sampling bias. This paper is a nice example of a problem it can be applied to that doesn’t raise that issue: detecting all the errors in a system’s output.\nThe scenario is that you have a bunch of models for doing a task (e.g. POS tagging) and a new dataset with no labeled data, which you would like to label. Having a person label the data would take a long time and doesn’t take advantage of these systems. At the same time, we can’t just run the systems and use their output because they aren’t perfect, particularly out of domain. We could run the systems and check their output, which could be faster than annotating directly, but would still take a long time. If we don’t mind having some errors, we can check just some output, but how do we decide what to check?\nThis paper applies the generative model from MACE to build a generative model of system outputs. The model is:\n For each example, sample the true label with a uniform prior Then, for each classifier, sample from a Bernoulli distribution to decide if they are good or not A good classifier returns the true label, a not good classifier samples from a multinomial over the options  Since we don’t know the parameters of the model, or the true labels, use expectation maximisation to learn.\nThis work takes that model, trains it and uses it to identify the sample that is most uncertain. A person annotates it, the correct label replaces one of the system predictions, and EM is run again. This is repeated until either there appear to be no more errors, or annotators run out of time.\nHow well does it work? The main metric is precision: how many of the instances asked for annotation actually have errors. For POS tagging on WSJ text, the taggers initially get 2.5% of words wrong. To get that down to 1.1% the precision is 33%, and to get it to 0.65% precision is 17.6%. On an out of domain dataset, the error rate is 10% initially, and is down to 5% with a precision of 50%. Put differently, in a dataset of 25,000 tokens, with 2,500 errors, after checking 2,500 tokens, there are only 1,250 errors (another 2,500 checks brings it down to 730). It also works well for NER, and consistently does better than the alternative they compare to (consider the taggers a committee and find the examples with highest entropy, i.e. greatest disagreement).\nThis seems like a natural fit for prodigy and something that could be broadly useful.\nCitation Paper\n@InProceedings{rehbein-ruppenhofer:2017:Long, author = {Rehbein, Ines and Ruppenhofer, Josef}, title = {Detecting annotation noise in automatically labelled data}, title: = {Detecting annotation noise in automatically labelled data}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1160--1170}, url = {https://aclanthology.org/P17-1107} } ","categories":"","description":"When labeling a dataset automatically there are going to be errors, but we can use a generative model and active learning to guide effort to checking the examples most likely to be incorrect.","excerpt":"When labeling a dataset automatically there are going to be errors, …","ref":"/reading-notes/old-blog/2017-10-13_errordetection/","tags":"","title":"Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)"},{"body":"Language is bursty, with rare words occurring in clumps, the simplest example being an unusual name that occurs a lot in one news article, but not in other articles. This paper is about how to modify a neural language model to take this into consideration, by adapting the model over time.\nThe main idea is to have one model of overall word usage (global) and a separate model that shifts over time to take into consideration the current text (local). The idea of adapting is not new (as the paper makes clear), but the key here is an update rule that is a modified form of RMSprop, combining the local and global models. It also seems like performing the updates after every 5 words is important, balancing frequency with informativeness (though no ablation of frequencies is presented). Conveniently, this is orthogonal to many other ideas and can essentially be stapled on top of a range of sequential architectures, consistently leading to improvements.\nOne question left open is how this would work in generation. The paper describes how it could be applied and could provide improvements, but it also seems likely to risk the repetitive outputs seen in many dialogue systems.\nCitation ArXiv Paper\n@ARTICLE{2017arXiv170907432K, author = {Krause, B. and {Kahembwe}, E. and {Murray}, I. and {Renals}, S.}, title = \"{Dynamic Evaluation of Neural Sequence Models}\", title: \"{Dynamic Evaluation of Neural Sequence Models}\", journal = {ArXiv e-prints}, archivePrefix = \"arXiv\", eprint = {1709.07432}, keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language}, year = 2017, month = sep, adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170907432K}, adsnote = {Provided by the SAO/NASA Astrophysics Data System}, } ","categories":"","description":"Language model perplexity can be reduced by maintaining a separate model that is updated during application of the model, allowing adaptation to short-term patterns in the text.","excerpt":"Language model perplexity can be reduced by maintaining a separate …","ref":"/reading-notes/old-blog/2017-10-30_neuralsequence/","tags":"","title":"Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)"},{"body":"This paper from 2011 explores the relationship between transition based parsing and dynamic programming based parsing. They show how to convert common dependency parsing systems (Arc-Standard and Arc-Eager) into dynamic programs, and how doing the reverse on a dynamic program gives the Arc-Hybrid approach (which has since been used in many places, and is now joined by additional systems like Arc-Swift).\nThe benefit of this transformation is that we can find exact answers without massive beams. The drawback is that the feature set is restricted. This paper is theoretical, so it doesn’t give a direct measure of this tradeoff, though follow up work shows that avoiding search errors is indeed beneficial.\nWith all of the positive results using neural networks for multi-task learning, one thought this work leads to is whether we could treat different inference methods as different tasks. In other words, have a single model encoding the input, then have multiple inference algorithms with different extensions of that model, all trained simultaneously. The variation in available context for the different algorithms may force generality in the core representation shared across them.\nCitation Paper\n@InProceedings{kuhlmann-gomezrodriguez-satta:2011:ACL-HLT2011, author = {Kuhlmann, Marco and G\\'{o}mez-Rodr\\'{i}guez, Carlos and Satta, Giorgio}, title = {Dynamic Programming Algorithms for Transition-Based Dependency Parsers}, title: = {Dynamic Programming Algorithms for Transition-Based Dependency Parsers}, booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, month = {June}, year = {2011}, address = {Portland, Oregon, USA}, publisher = {Association for Computational Linguistics}, pages = {673--682}, url = {https://aclanthology.org/P11-1068} } ","categories":"","description":"Transition based algorithms can be transformed into dynamic programs by defining sequences of actions that correspond to the same overall transformation.","excerpt":"Transition based algorithms can be transformed into dynamic programs …","ref":"/reading-notes/old-blog/2017-10-24_dynamictransition/","tags":"","title":"Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)"},{"body":"This work presents a system that parses sentences and identifies grammatical errors simultaneously. It’s an intuitive combination - a syntactic model should assign higher probability to a parse for a fixed version of a sentence than the one with a mistake.\nThey build on an incremental ‘easy-first’ dependency parsing approach. Easy-First parsing starts with the set of words in the sentence and allows an edge to be created between any adjacent pair of words. Once an edge is created, the child is hidden beneath its parent, so now the parent is effectively adjacent to a word slightly further away. Then the process repeats, until there is only one word left (the root of the sentence). In a way it is like following a dynamic program, but with only a single state that ties together multiple cells.\nThe change in this paper is the addition of actions that insert a word, delete a word, or alter a word. To make it work, there are constraints to avoid cycles of repeated actions (e.g. insert-delete-insert-delete…), and on the sets of allowed word substitutions. To produce additional training data, a tool is used to inject errors into grammatical text. On error detection, this approach does lead to improvements, though it changes a relatively small number of the sentences. On dependency parsing it is (unsurprisingly) worse than a baseline system on grammatical text. It does perform better on ungrammatical text, though the data is generated using the same process as the training data, creating a bias in the system’s favour.\nCitation Paper\n@InProceedings{sakaguchi-post-vandurme:2017:Short, author = {Sakaguchi, Keisuke and Post, Matt and Van Durme, Benjamin}, title = {Error-repair Dependency Parsing for Ungrammatical Texts}, title: = {Error-repair Dependency Parsing for Ungrammatical Texts}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {189--195}, url = {https://aclanthology.org/P17-2030} } ","categories":"","description":"Grammatical error correction can be improved by jointly parsing the sentence being corrected.","excerpt":"Grammatical error correction can be improved by jointly parsing the …","ref":"/reading-notes/old-blog/2017-11-22_errorrepairparsing/","tags":"","title":"Error-repair Dependency Parsing for Ungrammatical Texts (Sakaguchi et al., 2017)"},{"body":"A common argument in favour of neural networks is that they do not require ‘feature engineering’, manually defining functions that produce useful representations of the input data (e.g. a function that checks if a word is in a list of cities and returns 1 or 0). This paper argues that there is in fact still value in such functions.\nThe task is named entity recognition and the model is a CRF with a bidirectional LSTM using character and word embeddings. The functions in this case are (1) part of speech tags, (2) word shapes, and (3) gazetteers. Importantly, as well as receiving these as inputs, the model has to predict them as outputs (in both cases using predictions, not gold values). The improvement on the test set is substantial, ~0.8 F1. Ablation indicates that POS tags and word shape are particularly important, and having both the input and output is important. Interestingly, the shift on the development set is more marginal, ~0.3 F1, and the ablation doesn’t show as clear trends.\nOverall, my takeaway is that these kinds of features (which are not very hard to define) are worth the effort. However, there are a few more values I would have liked to see:\n Multi-task learning (they kind of get at this with one ablation, but it is on non-gold output) Cross-validation results (given the difference between dev and test) ELMo (the paper argues that it is orthogonal, which is reasonable, but I’m still curious)  Citation Paper\n@InProceedings{Wu:2018:EMNLP, author = {Wu, Minghao and Liu, Fei and Cohn, Trevor}, title = {Evaluating the Utility of Hand-crafted Features in Sequence Labelling}, title: = {Evaluating the Utility of Hand-crafted Features in Sequence Labelling}, booktitle = {EMNLP}, year = {2018}, url = {https://arxiv.org/abs/1808.09075}, } ","categories":"","description":"A common argument in favour of neural networks is that they do not require 'feature engineering', manually defining functions that produce useful representations of the input data (e.g. a function...","excerpt":"A common argument in favour of neural networks is that they do not …","ref":"/reading-notes/old-blog/2018-09-04_featureengineering/","tags":"","title":"Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu et al., 2018)"},{"body":"There is a lot of interest in dialogue agents, but a lot of work sits at one of two extremes: either chit-chat agents that just chat, or task-oriented agents that aim to call a specific API for the user. This work is about trying to integrate a range of systems from both categories, to get something more general purpose as a result.\nThe core approach is a hybrid system that switches between different agents behind the scenes (an approach taken by a number of Alexa Prize teams). The innovation here is that crowd workers will help with the decision (both suggesting things to say and voting on which response to use), and their votes will be used to learn a model to (partially) replace the people over time.\nUnfortunately, the improvement from a learned model of votes is only small (saves only 14% of the crowd effort), and the automated responses are rarely chosen (12% of the time). That said, it seems like an interesting design with a lot of subtle decisions that require more exploration - the sets of agents (4-6 here, mostly narrow types), the voting scheme (only 1 or 2 votes needed here), choosing which agent responses to show (here, the proportion of previously accepted messages from this agent), and so on. That choice of which responses to show is particularly tricky, as with this scheme a very domain specific agent might get voted down too much initially and never be chosen when the appropriate time comes. One potentially interesting alternative would be to let the crowd workers choose which agent’s response to see, and possibly even post-edit slightly.\nNote - This post is the first of a (hopefully) regular series again. However, rather than keeping it weekday-ly, I plan to do three times a week, at least until the ACL deadline.\nCitation Paper\n@InProceedings{blah, title = {Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time}, title: {Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time}, author = {Huang, Ting-Hao (Kenneth) and Chang, Joseph Chee and Bigham, Jeffrey P.}, booktitle = {CHI}, year = {2018}, url = {https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf}, } ","categories":"","description":"For a more flexible dialogue system, use the crowd to propose and vote on responses, then introduce agents and a model for voting, gradually learning to replace the crowd.","excerpt":"For a more flexible dialogue system, use the crowd to propose and vote …","ref":"/reading-notes/old-blog/2018-01-28_crowdassistant/","tags":"","title":"Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)"},{"body":"Virtually all systems trained using data have trouble when applied to datasets that differ even slightly - even switching from Wall Street Journal text to New York Times text can hurt parsing performance slightly. Extensive work has explored how to adapt to new domains (including one of my own), but generally these approaches only made up a fraction of the gap in performance.\nThis paper shows two interesting new approaches to this issue:\n Use ELMo, a type of word representation trained on massive amounts of text. Train a span-based parser with partial annotations.  The first is straightforward, and further demonstrates the effectiveness of ELMo. To give a sense of how much this helps, the Charniak parser goes from 92 on the WSJ to 85 on the Brown corpus, while this model goes from 94 to 90. The second idea takes advantage of a recent parsing model with a simple approach:\n Independently assign a score to every span of a sentence, indicating whether it is part of the parse. Find the maximum scoring set of spans using a dynamic program.  The structure of the scoring step allows for a convenient form of partial annotations. Simply label the tricky spans in a sentence (e.g. to indicate where a prepositional phrase attaches / does not attach). During training on partially annotated sentences, only the labeled spans are used to update the model. This gives dramatic gains across multiple datasets.\nCitation Paper\n@InProceedings{Joshi:2018:ACL, author = {Joshi, Vidur and Peters, Matthew and Hopkins, Mark}, title = {Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples}, title: = {Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples}, booktitle = {ACL}, year = {2018}, url = {https://arxiv.org/abs/1805.06556}, } ","categories":"","description":"Virtually all systems trained using data have trouble when applied to datasets that differ even slightly - even switching from Wall Street...","excerpt":"Virtually all systems trained using data have trouble when applied to …","ref":"/reading-notes/old-blog/2018-06-12_parseradaptation/","tags":"","title":"Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples (Vidur Joshi et al., 2018)"},{"body":"Humor is an incredibly difficult problem, as this paper makes clear in its background section. Most work has considered very specific types of jokes (e.g. “that’s what she said”, or pairs of words that sound similar to form riddles). This work contributes (1) a new task, (2) an evaluation method, and (3) an example system.\nThe task is Mad Libs, where a story has some words removed and people choose new words to make the story funny. If you are familiar with the normal version, one key difference is that here people have access to the complete story when they are choosing their words. A set of 40 ‘stories’ were written based on Simple Wikipedia articles, and workers on Mechanical Turk wrote words to fill them, with filtering based on judging by other workers.\nThe evaluation method involved recruiting a set of judges on Mechanical Turk and asking a series of questions to measure humour for a given response. As well as judging the overall story, they were asked to select which words contributed the most. By aggregating these selections as votes, each word was scored as funny or not.\nThe system is a linear classifier with a range of features, including scores from a language model. On its own, it performs very poorly, but using it as a filter to restrict the space of words a person can choose from actually leads to better performance than people on their own. Of course, it’s difficult to analyse the source of improvement; The authors theorise that it is because it prevents people from selecting words that only they would see is funny. Another interpretation is that the constraint gives them a smaller space to think about and so they can find more interesting plays on words.\nFinally, as a non-expert in this area, this paper had some nice discussion of the tradeoffs between different ways of generating humour (incongruous vs. coherent content strategies).\nCitation Paper\n@InProceedings{hossain-EtAl:2017:EMNLP2017, author = {Hossain, Nabil and Krumm, John and Vanderwende, Lucy and Horvitz, Eric and Kautz, Henry}, title = {Filling the Blanks (hint: plural noun) for Mad Libs Humor}, title: = {Filling the Blanks (hint: plural noun) for Mad Libs Humor}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {649--658}, url = {https://aclanthology.org/D17-1068}, } ","categories":"","description":"A new task and associated evaluation method plus system for Mad Libs - filling in missing words in a story in a funny way. While the system does poorly, using it as a first pass with human rerankers produces funnier stories than people alone.","excerpt":"A new task and associated evaluation method plus system for Mad Libs - …","ref":"/reading-notes/old-blog/2017-10-06-madlibs/","tags":"","title":"Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)"},{"body":"Another paper about a dataset of dialogues, but this time with structure. Like the paper from yesterday, the aim is a dataset of task-oriented conversations, but with more complexity than prior work. The difference is that this work includes a structured representation of the state of the conversation: frames.\nA frame is essentially a tuple describing a query, e.g. (Destination: Sydney, Origin: Ann Arbor, price: 1500 USD). There are multiple frames in a dialogue (a departure from DSTC tasks), and utterances are labeled with dialogue acts that capture modifications to the frames as well as references to them. This structure sounds fairly general, though the focus here was on vacation planning, where the user is buying a package. The setup doesn’t maximise the potential complexity though, as there are a small number of set packages available, rather than the complex tradeoffs of flight+hotel combinations that exist in practise. Looking at the example dialogues in the paper, it has complete sentences of some complexity. One thing I’m still curious about is disagreements between annotators, as for the complete task the score was 0.62 +/- 5 (with dialogue acts being trickier than slot values, and no scores for frame references on their own).\nComparing to the Stanford dataset this is smaller (11k vs. 1.4k), but has more turns per dialogue (11 vs. 15) and probably longer turns too, judging by the examples. The tasks are completely different, but both come with small tables of information that are private to the two participants and required for almost every turn in the conversation. Evaluating on both could be a great way to show the flexibility of a dialogue system, but the lack of frames for the Stanford data and the difficulty of running a human evaluation for this data limits the feasible types of multi-domain experiments.\nCitation Paper\n@InProceedings{elasri-EtAl:2017:W17-55, author = {El Asri, Layla and Schulz, Hannes and Sharma, Shikhar and Zumer, Jeremie and Harris, Justin and Fine, Emery and Mehrotra, Rahul and Suleman, Kaheer}, title = {Frames: a corpus for adding memory to goal-oriented dialogue systems}, title: = {Frames: a corpus for adding memory to goal-oriented dialogue systems}, booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue}, month = {August}, year = {2017}, address = {Saarbrucken, Germany}, publisher = {Association for Computational Linguistics}, pages = {207--219}, url = {https://aclanthology.org/W17-5526} } ","categories":"","description":"A new dialogue dataset that has annotations of multiple plans (frames) and dialogue acts that indicate modifications to them.","excerpt":"A new dialogue dataset that has annotations of multiple plans (frames) …","ref":"/reading-notes/old-blog/2017-11-09_framesdataset/","tags":"","title":"Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)"},{"body":"Abstract Meaning Representation (AMR) structures represent sentence meaning with labeled nodes (concepts) that are related to the words in the sentence, but not explicitly linked to them. This is a problem for most parsing algorithms, which need a way to efficiently decompose the structure in order to learn how to generate it. In dependency parsing there are no abstract nodes to generate, in constituency parsing there is a very small set of node types, and for CCG, TAG, etc the labels come from a constrained space. The solution for many AMR parsers is to have a process for generating the concepts as a first step towards parsing, and to automatically align the training data to guide this concept generation stage.\nThe first idea in this paper is about the set of AMR concepts. Some concepts are easy to link, as the concept clearly maps to a single word in the sentence. Around a quarter of concepts have a more complex relation, where a set of concepts link to a set of words, for example, named entities. The idea for these is to identify common subgraphs by abstracting some lexical items. For example, a teacher and a worker both get mapped to a person concept that is the ARG0 of the lexical item (teach, or work in this case). This can allow for the generation of entirely novel concepts (e.g. “concept”-er), giving a 0.6 boost to recall for CAMR simply by making these additional concepts available. Using a bidirectional LSTM with a character CNN to generate features on likely concepts, there is a gain of 1.0 F1 for the parser.\nThe second idea is to improve the alignments used to train concept generation by taking into consideration the graph structure. To use an aligner developed for machine translation the graph needs to be turned into a linear sequence, but that can lead to strange jumps. The idea here is to take that into consideration by modifying the calculation of the cost of distortion (i.e. jumping) to be reshaped based on the graph structure. For optimal alignment quality they consider aligning in either direction, directly changing the distance metric in the English-AMR direction, and just rescaling it to be less sensitive when appropriate for AMR-English. This is definitely higher precision than prior approaches, but lower recall. It’s hard to tell whether this helps, since the evaluation doesn’t separate it out from the first idea (results in section 5.3 are not on the same dataset as 5.1).\nGiven how separate this is from CAMR, it would be interesting to see if it helps other systems similarly. With concept identification at 83 F there is still plenty of scope for improvement, though there is no analysis of which types of concepts remain the most problematic.\nCitation Paper\n@InProceedings{wang-xue:2017:EMNLP2017, author = {Wang, Chuan and Xue, Nianwen}, title = {Getting the Most out of AMR Parsing}, title: = {Getting the Most out of AMR Parsing}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {1268--1279}, url = {https://aclanthology.org/D17-1130}, } ","categories":"","description":"Two ideas for improving AMR parsing: (1) take graph distance into consideration when generating alignments, (2) during parsing, for concept generation, generate individual concepts in some cases and frequently occurring subgraphs in other cases.","excerpt":"Two ideas for improving AMR parsing: (1) take graph distance into …","ref":"/reading-notes/old-blog/2017-10-12_amralignment/","tags":"","title":"Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)"},{"body":"This paper is a detailed analysis of a surprisingly effective simple idea: train a machine translation system with sentence pairs from multiple languages, adjusting the input to have an extra token at the end that says what the target language is. To deal with class imbalance, data is oversampled to have all language pairs be equally represented (though even without that, it works fairly well).\nThe biggest advantage of this approach is that a single model can handle translation between many pairs, rather than needing $O(n^2)$ models for $n$ languages. The performance is slightly lower on average, but the single model can manage with far fewer parameters. In one example, twelve models are combined into a single model with as many parameters as one of the twelve, and the results are lower by just 0.76 BLEU on average. Another advantage of the model is the ability to handle code-switched language, though they didn’t have evaluation datasets to get an quantitative measure of accuracy.\nHaving this model also opens up the possibility of translating between pairs of languages with no parallel training data (A -\u003e B). As long as there is data (A -\u003e C) and (D -\u003e B), sentences from A can be fed in with B as the target language. For closely related languages this works very well, and in particular, better than going via another language such that there is data for the two language pairs. For example, going from Portuguese to Spanish with the multilingual model scores 24.75, whereas going via English scores 21.62 and a model with explicit training data gets 31.50. Going between less related languages is less successful, with direct Spanish to Japanese scoring 9.14, and going via English scoring 18.00. One thing I wish the paper had is more exploration of this result - what does it get right when scoring 9.14? For the time being at least, going via a third language still seems necessary, and presumably the best language to use is whichever one the performance is highest on.\nCitation Paper\nArXiv version which appears to be the same aside from one extra figure of the model architecture.\nAs an aside, it is interesting to see the timeline for this paper:\n November 2016, Submission to ArXiv and in the TACL submission batch March 2017, TACL revision batch October 2017, TACL published  @article{TACL1081, author = {Johnson, Melvin and Schuster, Mike and Le, Quoc and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viégas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey}, title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation}, title: = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation}, journal = {Transactions of the Association for Computational Linguistics}, volume = {5}, year = {2017}, issn = {2307-387X}, url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1081}, pages = {339--351} } ","categories":"","description":"A translation model trained on sentence pairs from a mixture of languages can do very well across all of the languages, and even generalise somewhat to new pairs of the languages. That's useful as one model can do the work of $O(n^2)$ models, and with a fraction of the parameters.","excerpt":"A translation model trained on sentence pairs from a mixture of …","ref":"/reading-notes/old-blog/2017-10-11_multimt/","tags":"","title":"Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)"},{"body":"Word vectors are great for common words, but what about rare words? People can have a fairly good understanding of a word given only a few instances, but it’s fairly standard to turn all words with a frequency of less than 5 into UNK when learning word vectors.\nOne simple approach is to add up word vectors from the context of the rare word and use that as the representation. This paper proposes using a tweaked version of word2vec: keep vectors for frequent words fixed, increase the learning rate, use a fixed width context window, initialise with the additive approach, and only subsample by discarding frequent words. All of those make sense, though I am curious whether it would be better to just decrease subsampling or disable it entirely.\nThe results are mixed, with the improvement over the additive approach data dependent. That might partly reflect the tasks though - something downstream like POS tagging would have been interesting, particularly since the LSTM may already be capturing contextual information that covers what the additive approach has, but not what this adds. Ultimately this is not a solution to this problem, but it’s an idea to keep in mind.\nCitation Paper\n@InProceedings{herbelot-baroni:2017:EMNLP2017, author = {Herbelot, Aur\\'{e}lie and Baroni, Marco}, title = {High-risk learning: acquiring new word vectors from tiny data}, title: = {High-risk learning: acquiring new word vectors from tiny data}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {304--309}, url = {https://aclanthology.org/D17-1030} } ","categories":"","description":"The simplest way to learn word vectors for rare words is to average their context. Tweaking word2vec to make greater use of the context may do slightly better, but it's unclear.","excerpt":"The simplest way to learn word vectors for rare words is to average …","ref":"/reading-notes/old-blog/2017-12-07_rarewordvectors/","tags":"","title":"High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)"},{"body":"The standard approach in crowdsourcing is to have a fixed number of workers annotate each instance and then aggregate annotations in some way (possibly with experts resolving disagreements). This paper proposes a way to dynamically allocate workers.\nThe process is as follows:\n Get two workers to annotate an example. If they agree, assign the label. For disagreements, ask additional annotators to label it until a simple majority annotation is reached or a limit is reached. For cases where the limit is reached, use some aggregation approach / experts.  I really like this idea - it’s simple to apply and the intuition for why it should work is clear. Unfortunately, the experiments in the paper do not do the comparison I am most interested in: real data, with multiple annotation strategies applied. The simulated study supports the effectiveness, but that means buying a range of assumptions about annotator behaviour (e.g. that all errors are equally likely and all workers have the same pattern of behaviour). There is a large-scale experiment with real data in which the approach collects 3.74 labels per instance on average (with a mimimum of 3) and only 5% of cases not reaching a consensus. That seems very good!\nCitation Paper\n@inproceedings{sun-etal-2020-improving, title = \"Improving Human-Labeled Data through Dynamic Automatic Conflict Resolution\", title: \"Improving Human-Labeled Data through Dynamic Automatic Conflict Resolution\", author = \"Sun, David Q. and Kotek, Hadas and Klein, Christopher and Gupta, Mayank and Li, William and Williams, Jason D.\", booktitle = \"Proceedings of the 28th International Conference on Computational Linguistics\", month = \"dec\", year = \"2020\", address = \"Barcelona, Spain (Online)\", publisher = \"International Committee on Computational Linguistics\", url = \"https://aclanthology.org/2020.coling-main.316\", pages = \"3547--3557\", abstract = \"This paper develops and implements a scalable methodology for (a) estimating the noisiness of labels produced by a typical crowdsourcing semantic annotation task, and (b) reducing the resulting error of the labeling process by as much as 20-30{\\%} in comparison to other common labeling strategies. Importantly, this new approach to the labeling process, which we name Dynamic Automatic Conflict Resolution (DACR), does not require a ground truth dataset and is instead based on inter-project annotation inconsistencies. This makes DACR not only more accurate but also available to a broad range of labeling tasks. In what follows we present results from a text classification task performed at scale for a commercial personal assistant, and evaluate the inherent ambiguity uncovered by this annotation strategy as compared to other common labeling strategies.\", } ","categories":"","description":"","excerpt":"The standard approach in crowdsourcing is to have a fixed number of …","ref":"/reading-notes/old-blog/2020-12-10_dynamicannoallocation/","tags":"","title":"Improving Human-Labeled Data through Dynamic Automatic Conflict Resolution (Sun, et al., CoLing 2020)"},{"body":"This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 10 million+ words of text, but only 1 GPU for training?\nThe impact of tying, freezing, and pretraining It is standard practise to tie the input and output embeddings of language models (i.e., use the same weights in both places), training them together and initialising them randomly. Several papers have shown that this improves results by providing more frequent updates to the input embeddings. But if you have data available for pretraining it is less clear that this is the right approach. To explore this I’m going to use a few symbols:\nHere are the results of training an AWD-LSTM with all variations of these parameters, evaluated on the standard LM development set of the PTB (Std) and a variation that has actual words instead of unk (Rare):\nLight blue shows the standard configuration and light red shows our proposal. The table is ranked by performance on Std and has four clear sections:\n(a) Frozen random output embeddings.\n(b) Frozen pretrained output embeddings.\n(c) Frozen random input embeddings.\n(d) Various configurations.\nI was surprised by the dramatic difference between input and output embeddings here. Freezing the output embeddings, even with a good embedding space, leads to terrible performance. In contrast, freezing input embeddings is fine if they are pretrained, and has a far smaller impact when they are random.\nEvaluating with rare words, the big picture is mostly the same, but pretraining has a bigger impact. One interesting difference is that the top five models all use pretrained input embeddings, with a large gap from there to the next results. At the same time, pretraining the output embeddings seems to have only a small impact (when holding all other variables fixed). Finally, the best results freeze the input embeddings. Our explanation is that embeddings become inconsistent when they aren’t frozen. The vectors for words in the training set are moved but the ones seen only in pretraining stay where they are, leading to an inconsistent embedding space.\nThe paper then goes through a series of experiments to explore this, varying data domain, similarity of pretraining data, and more. Here I’m going to jump straight to the final results. The table below considers a dataset with 43 million in-domain tokens for pretraining and 7 million for LM training. The other models are the standard AWD-LSTM, an n-gram language model, and two version of GPT-2 (without finetuning):\nFor word level prediction perplexity is reduced by 4. However, if we train and test with BPE there is no improvement (see the SHA-RNN paper for some issues with comparing BPE and word evaluation). So if your application works with BPE this finding isn’t useful, but for word-level modeling it probably is.\nA few notes about this work:\n A natural next step would be to explore ways to train the language model with more data. Modifying the AWD-LSTM code to support training sets larger than GPU memory could render pretraining unnecessary (though at the cost of much longer training). In some experiments (not in the paper), we found that when the pretraining set and training set were the same, pretraining didn’t improve performance, but it did speed up training. Properties of evaluation datasets have shaped the direction of work on language modeling. It’s important to think beyond the hyperparameters that are easy to vary (e.g., hidden vector dimensions) when adapting a model for a new scenario. Writing robust research code is hard. We tried getting several other models to run with our variations, but going beyond reproducing results to actually modifying code proved hard. Even for the AWD-LSTM, we failed to reproduce results except when we went back to one of the earliest releases. This paper was saved by author response. The initial reviews were 3.5, 2.5, 3.5 and based on the response and reviewer discussion the 2.5 went to a 4. The response contained answers to reviewer questions, including a bunch of statistics about the data that are now in the final paper. I have always been a fan of author response. It can lead to more informed acceptance decisions and more useful feedback to authors. To achieve that, both authors and reviewers need to engage with it though. In particular, reviewers need to give something of substance to be responded to and they need to carefully read and consider the response.  Citation Paper\n@InProceedings{emnlp20lm, title = {Improving Low Compute Language Modeling with In-Domain Embedding Initialisation}, title: = {Improving Low Compute Language Modeling with In-Domain Embedding Initialisation}, author = {Welch, Charles and Mihalcea, Rada and Kummerfeld, Jonathan K.}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2020}, url = {https://www.jkk.name/pub/emnlp20lm.pdf}, abstract = {Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.}, } Tables in written form Table with training variations Each section is presented separately below, with the model described using five words followed by the result on the standard data and the result on the data with rare words.\nFirst section:\n tied frozen dice frozen dice, 680, 1120 untied frozen dice frozen dice, 680, 1120 untied unfrozen dice frozen dice, 680, 431 untied unfrozen train frozen dice, 220, 372 untied frozen train frozen dice, 218, 360  Second section:\n untied frozen dice frozen train, 121, 202 untied unfrozen dice frozen train, 95.0, 170 untied unfrozen train frozen train, 91.3, 147 tied frozen train frozen train, 90.7, 136 untied frozen train frozen train, 90.7, 136  Third section:\n untied frozen dice unfrozen dice, 82.2, 143 untied frozen dice unfrozen train, 81.4, 142  Fourth section:\n untied unfrozen dice unfrozen dice, 65.3, 120 untied unfrozen dice unfrozen train, 64.1, 113 untied unfrozen train unfrozen dice, 62.5, 105 untied unfrozen train unfrozen train, 61.7, 98.5 untied frozen train unfrozen train, 61.6, 97.1 tied unfrozen dice unfrozen dice, 61.3, 112 untied frozen train unfrozen dice, 61.1, 98.1 tied unfrozen train unfrozen train, 59.8, 98.7  Final results table Models with word level evaluation, giving development results then test results:\n N-Gram, 92.3, 95.0 Baseline AWD-LSTM, 52.8, 53.5 Our approach, 49.0, 49.4  Models with BPE evaluation:\n N-Gram, 56.7, 55.3 GPT-2 (112m), 46.4, 43.8 Baseline AWD-LSTM, 37.8, 36.7 Our approach, 38.3, 37.2 GPT-2 (774m), 32.5, 33.7  Acknowledgements Dice Icon by Andrew Doane from the Noun Project. Fire and Snowflake Icons by Freepik from www.flaticon.com.\n","categories":"","description":"This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 50 million+ words of text, but only 1 GPU for training?","excerpt":"This paper explores two questions. First, what is the impact of a few …","ref":"/reading-notes/old-blog/2020-09-29_pretraininglm/","tags":"","title":"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)"},{"body":"Shift-reduce constituency parsing incrementally builds the parse either bottom-up or top-down. The difference is whether a non-terminal is placed on the stack before or after the words that it spans. This corresponds to two forms of depth-first traversal of the tree: pre-order or post-order.\nThe idea in this paper is to do an in-order traversal, which in a binary tree means traversing the left child of a node, then the node, then its right child. In this context that means putting the non-terminal symbol on the stack after the first word it spans, but before the rest. The model follows the stack-LSTM approach of Dyer et al., with non-terminals always fed into the LSTM first during composition, regardless of where it was inserted into the stack.\nThis leads to a 0.5 F1 gain on standard parsing metrics, with no hyperparameter tuning. High-level error analysis seems to show it just does better everywhere. I wonder whether further gains could be realised with a label-sensitive ordering.\nCitation Paper\n@article{TACL1199, author = {Liu, Jiangming and Zhang, Yue }, title = {In-Order Transition-based Constituent Parsing}, title: {In-Order Transition-based Constituent Parsing}, journal = {Transactions of the Association for Computational Linguistics}, volume = {5}, year = {2017}, issn = {2307-387X}, url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1199}, pages = {413--424} } ","categories":"","description":"Using in-order traversal for transition based parsing (put the non-terminal on the stack after its first child but before the rest) is consistently better than pre-order / top-down or post-order / bottom-up traversal.","excerpt":"Using in-order traversal for transition based parsing (put the …","ref":"/reading-notes/old-blog/2017-11-14_inorderparsing/","tags":"","title":"In-Order Transition-based Constituent Parsing (Liu et al., 2017)"},{"body":"When we crowdsource data for tasks like SRL and sentiment analysis we only care about accuracy. For tasks where workers write new content, such as paraphrasing and creating questions, we also care about data diversity. If our data is not diverse then models trained on it will not be robust in the real world. The core idea of this paper is to encourage creativity by constraining workers.\nWe use three steps:\n Collect some data. Create a taboo list of words / phrases based on the data collected. Return to step 1, but tell workers they can’t use things in the taboo list.  We explored this idea for task-oriented dialogue. We identified taboo words using an SVM with a bag-of-words to identify common words associated with specific intents or slot values. Depending on the taboo word, we got quite different paraphrases. For example, for the sentence “What is the capital of Florida?” we collected paraphrases with various taboo words:\n   Taboo Word Paraphrases      what city is the state capital of florida   florida what is the capital of the sunshine state   capital where is the seat of government in florida   what tell me the name of florida’s capital    These examples show interesting variations, but to see if the variations are significant we tried collecting new test sets for five intent classification datasets and four slot filling datasets. With just two taboo words, a BERT based model trained on the original dataset did considerably worse on our new data. The drop varied from 2 to 33 points, with a median of 9. This indicates that we are capturing ways of expressing these intents that are not well covered by the original data.\nAddressing this issue is simple - train on data collected with our method! Interestingly, this approach is complementary to the outlier-based approach from our NAACL 2019 paper. Examples collected using one approach are hard for models trained on data from the other. Fortunately, training with data from a mix of the two leads to strong results on both.\nI’m particularly excited about this work because the general idea could be applied in so many ways:\n Change the task. Vary the type of taboo item (e.g. phrases instead of words). Try other ways of selecting taboo items. Use a different mapping from taboo items to tasks.  In fact, more specific versions of this idea have already been used. Luis von Ahn’s ESP game for image captioning used a taboo list. Each image had a list of complete labels previously assigned to the image. New labels could not match an existing label. That work predates this paper by 16 years, but hasn’t had much traction in the NLP community, possibly because of the limitation of requiring complete matches on labels (making it impractical for sentences or even phrases). I’m hopeful that our more general version will be useful in a range of crowdsourcing efforts.\nCitation Paper\n@InProceedings{emnlp20taboo, title = {Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness}, title: = {Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness}, author = {Larson, Stefan and Zheng, Anthony and Mahendran, Anish and Tekriwal, Rishi and Cheung, Adrian and Guldan, Eric and Leach, Kevin and Kummerfeld, Jonathan K.}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2020}, location = {Online}, url = {https://www.jkk.name/pub/emnlp20taboo.pdf}, } ","categories":"","description":"When we crowdsource data for tasks like SRL and sentiment analysis we only care about accuracy.  For tasks where workers write new content, such as paraphrasing and creating questions, we also care about data diversity.  If our data is not diverse then models trained on it will not be robust in the real world.  The core idea of this paper is to encourage creativity by constraining workers.","excerpt":"When we crowdsource data for tasks like SRL and sentiment analysis we …","ref":"/reading-notes/old-blog/2020-10-12_taboo/","tags":"","title":"Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)"},{"body":"This paper considers the task of identifying named entities in a sentence and the relations between them. The contribution is a way of formulating the task as tagging, so a bi-directional LSTM can be applied.\nThe tags are like in NER (Begin, Inside, End, Single, Outside), but rather than Person, Location, etc, they label each entity with the relation it is participating in, and whether it is in role one or two for the relation. Applying a two layer bidirectional LSTM to this set up gets to state-of-the-art precision on news data. To get SotA F-score they modify the loss to place less weight on Outside tags, which raises recall at the cost of precision.\nOne catch with this approach is handling multiple relations of the same type. The solution here is to link pairs that are closest together (unclear what they do for nesting). That doesn’t handle overlapping relations, which the authors say is particularly common in the BioInfer data (I’m curious how much it is hurting here too). It’s unclear how this could be addressed without a radical redesign, since extending the tag scheme could lead to sparsity issues.\nI was not familiar with this data, so I looked back to the original paper the annotated test data came from: Hoffman et al., (2011). There is no dev set, only a 395 sentence test set, so the standard practise is to use random 10% samples of the test data for development. Also, if I understand it correctly, the data was annotated by manually confirming the output of systems, which means it will have recall errors. If interest in this data grows, going back and annotating more seems worthwhile.\nCitation Paper\n@InProceedings{zheng-EtAl:2017:Long, author = {Zheng, Suncong and Wang, Feng and Bao, Hongyun and Hao, Yuexing and Zhou, Peng and Xu, Bo}, title = {Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme}, title: = {Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1227--1236}, url = {https://aclanthology.org/P17-1113} } ","categories":"","description":"By encoding the relation type and role of each word in tags, an LSTM can be applied to relation extraction with great success.","excerpt":"By encoding the relation type and role of each word in tags, an LSTM …","ref":"/reading-notes/old-blog/2017-11-30_taggingrelations/","tags":"","title":"Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme (Zheng et al., 2017)"},{"body":"Automatically generating high quality meeting notes and summaries would be awesome, but involves solving many challenges. Here, they assume speech recognition is already done and we also know the structure over utterances indicating which previous utterance each is a response to. The task is to label each of those utterance-utterance pairs with a type (e.g. elaboration) and to select the key phrase of each utterance.\nTwo datasets are used, the AMI and ICSO meeting corpora, which have all of the required information. The new idea here is to jointly model the choice of link label and the key phrase, which is intuitive. To show the value of joint modeling they run a version of the system with the same linear model, but with independent inference, which performs quite a bit worse.\nOne neat follow up is that by combining the key phrases into a list you get a form of summary. According to automatic metrics it is quite a bit better than running the summarisation system they compare to, though it’s still a long way from a human summary.\nCitation Paper\n@InProceedings{qin-wang-kim:2017:Long, author = {Qin, Kechen and Wang, Lu and Kim, Joseph}, title = {Joint Modeling of Content and Discourse Relations in Dialogues}, title: = {Joint Modeling of Content and Discourse Relations in Dialogues}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {974--984}, url = {https://aclanthology.org/P17-1090} } ","categories":"","description":"Identifying the key phrases in a dialogue at the same time as identifying the type of relations between pairs of utterances leads to substantial improvements on both tasks.","excerpt":"Identifying the key phrases in a dialogue at the same time as …","ref":"/reading-notes/old-blog/2017-11-03_discourserelations/","tags":"","title":"Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)"},{"body":" I am recruiting PhD students to start in 2022!\nSee my recruiting page for more information.\n   My work: I aim to make language a complementary part of the interface for any application (e.g. from desktop spreadsheets to mobile apps). This involves solving a range of fascinating challenges through new methods in Artificial Intelligence and Crowdsourcing.\nInterests: Executable Semantic Parsing (e.g., text-to-SQL) Dialogue Crowdsourcing Human-in-the-Loop Systems / Hybrid Intelligence\nMe: I am currently a Postdoctoral Research Fellow at the University of Michigan. In mid-2022, I will start as a Senior Lecturer (ie., research tenure-track) at the University of Sydney.\n                               News September, 2021: Congratulations to Laura and Andrew for their EMNLP and Findings of EMNLP papers, on the stability of embeddings in different languages, and an architecture for interpretable models built out of smaller, more focused models.\nAugust, 2021: I’ve been awarded a DECRA Fellowship by the Australian Research Council.\n","categories":"","description":"Jonathan K. Kummerfeld's website, with links to research papers, code, and data.\n","excerpt":"Jonathan K. Kummerfeld's website, with links to research papers, code, …","ref":"/","tags":"","title":"Jonathan K. Kummerfeld's Homepage"},{"body":"Since word2vec was released there have been a series of X2vec papers, though none have had the success of word vectors. In this case the idea is to represent entities and chunks of text (words, sentences, paragraphs).\nEntities are represented with vectors. To get the vector for a chunk of text, they:\n Sum word vectors for the text. Rescale to be of unit length. Multiply by a weight matrix and add a bias.  Then to learn these, negative log likelihood is used, where the probability is defined as a softmax over the dot product between entity and text vectors. The data is a portion of Wikipedia annotated with entities as indicated by links (plus they say the entity the page is about is implicitly part of every sentence).\nWith these new vectors in hand, they try textual similarity, with strong results. They also build a very simple entity linking system, a feed-forward network with these representations plus a few other features, and beat all prior work. Similarly They apply the same modeling approach to Quizball QA, also with strong results.\nThe simplicity and effectiveness of the model really is impressive. Some qualitative examples are included, but hard to find trends in. It does seem like a more reasonable vector learning approach than skip-thought and other similar approaches that rely only on text context - the entities provide something different, but clearly closely related. That said, I feel like more ablation is needed to see what role each of these pieces is playing (are they learning better vectors, or using them in a way that is more effective? Or both?).\nCitation Paper\n@article{TACL1065, author = {Yamada, Ikuya and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu }, title = {Learning Distributed Representations of Texts and Entities from Knowledge Base}, title: {Learning Distributed Representations of Texts and Entities from Knowledge Base}, journal = {Transactions of the Association for Computational Linguistics}, volume = {5}, year = {2017}, issn = {2307-387X}, url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1065}, pages = {397--411} } ","categories":"","description":"Vectors for words and entities can be learned by trying to model the text written about the entities. This leads to word vectors that score well on similarity tasks and entity vectors that produce excellent results on entity linking and question answering.","excerpt":"Vectors for words and entities can be learned by trying to model the …","ref":"/reading-notes/old-blog/2017-11-15_entityvectors/","tags":"","title":"Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)"},{"body":"Task-oriented dialogue systems are often focused on a very narrow task, to the point where the state can be described completely with a tuple (e.g. preferences for a restaurant). This paper sets up a more challenging task with more complex language use, while still having a specific goal and directly relevant structured information. They collected 11,000 dialogues, where two people have private lists of friends and are trying to identify which friend they have in common. While this is a lot of data, the mechanical turk workers are clearly moving fast, with dialogues taking 1.5 minutes on average, and in 18% of cases they get the friend wrong.\nThe algorithmic contribution is that the lists of people are represented as a graph, where nodes are properties like company and hobby. The graph is used to generate vectors for each person by running a form of message passing over its structure. During generation, the LSTM uses attention over these vectors to inform the output choice.\nA few interesting things in the output:\n There are cases where the output is incorrect, as in, says a fact about the structured information / knowledge base that is false. Evaluation is tricky, and over the metrics they consider sometimes this wins, but sometimes the baseline system (rules) does better. In particular, success on bot-bot evaluation doesn’t seem to clearly transfer to bot-human experiments. The utterances are very fluent, but that may be because it’s essentially copying from the training data. It looks like there is diversity in the dataset, but a lot of utterances do fit a template of “I have X who Y”  Citation Paper\n@InProceedings{he-EtAl:2017:Long4, author = {He, He and Balakrishnan, Anusha and Eric, Mihail and Liang, Percy}, title = {Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings}, title: = {Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1766--1776}, abstract = {We study a \\emph{symmetric collaborative dialogue} setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.}, url = {https://aclanthology.org/P17-1162} } ","categories":"","description":"During task-oriented dialogue generation, to take into consideration a table of information about entities, represent it as a graph, run message passing to get vector representations of each entity, and use attention.","excerpt":"During task-oriented dialogue generation, to take into consideration a …","ref":"/reading-notes/old-blog/2017-11-08_graphdialogue/","tags":"","title":"Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)"},{"body":"Usually when we learn, we have a curriculum designed to incrementally build understanding. It seems reasonable that the same idea could be useful for machine learning, and indeed there is a large body of work on the topic. This paper explores the specific question of whether a curriculum can help develop task-specific word vectors, and whether we can determine an effective curriculum automatically.\nThey define a linear model with a range of features that characterise a paragraph of text, such as the number of distinct words, the number of prepositional phrases, and the average number of syllables per word. Paragraphs are sorted by the model and used to train word vectors with word2vec. These word vectors are then used as part of a model for a target task, giving a score that indicates the quality of the curriculum. Based on this score the weights for the model are updated, using a form of Bayesian optimisation.\nOne really nice aspect of this paper is the range of tasks considered: sentiment analysis, NER, POS tagging, and parsing. Learning a curriculum does improve performance slightly, and which features are important varies across the tasks (indicating the importance of task-specific curriculums). However, the models are somewhat restricted (as shown by the low absolute performance) because they do not change the word vectors during training. For most of this paper that’s a reasonable decision, as it allows a clearer learning signal, but it would have been interesting to also see the impact on the normal training scenario and a state-of-the-art model. In my experience (and in our soon-to-appear NAACL paper) we find that variations in word vectors can disappear during training for a downstream task.\nCitation Paper\n@InProceedings{tsvetkov-EtAl:2016:P16-1, author = {Tsvetkov, Yulia and Faruqui, Manaal and Ling, Wang and MacWhinney, Brian and Dyer, Chris}, title = {Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning}, title: = {Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning}, booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {August}, year = {2016}, address = {Berlin, Germany}, publisher = {Association for Computational Linguistics}, pages = {130--139}, url = {https://aclanthology.org/P16-1013} } ","categories":"","description":"Reordering training sentences for word vectors may impact their usefulness for downstream tasks.","excerpt":"Reordering training sentences for word vectors may impact their …","ref":"/reading-notes/old-blog/2018-03-05_curriculum/","tags":"","title":"Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning (Tsvetkov et al., 2016)"},{"body":"The standard way to get high quality annotations is to get labels from multiple people and take a majority vote. Getting multiple annotations costs more, and the quality of annotators can vary considerably (with spamming at one extreme). One way to avoid the quality issue is to restrict who can do the task (must have done X previous tasks with an accept rate of Y), but that limits the pool of available workers. Another approach is to try to estimate the quality of annotator work using a statistical model.\nHere a generative model is used, with the following structure:\n $T_i$, the true label, sampled with a uniform prior over labels $S_{ij}$, a binary variable indicating if the person is spamming or not, sampled as a Bernoulli variable with a Beta prior $A_{ij}$, the annotator’s decision, if they are spamming it is sampled from a multinomial with parameters specific to them (with a Dirichlet prior), otherwise it is the true label  $A$ is observed, but $T$ and $S$ are not, so they use expectation maximization to get both model parameters and variable values. To deal with nonconvexity they use 100 random restarts, deciding which is best based on how well the model describes the data. Note - this model (and the code) was the basis of the error detection paper I wrote about recently.\nFor predicting annotator quality the model is consistently effective across three datasets, though the Beta and Dirichlet priors are key for one (where annotator agreement was high on average). For determining the correct answer it is slightly better than majority vote, though the gains are small. The real advantage comes in deciding whether to discard data, where the choice of what to discard can be guided by the estimate of quality (this is what the error detection paper was doing). A range of synthetic experiments also show positive results, though their design shares the assumptions about behaviour that are baked into the model.\nI found a few results particularly interesting:\n As the number of annotators is decreased, the benefit of this approach over majority vote grows to be quite substantial (the main experiments are for data with 10 annotators). If you do use majority vote, use an odd number of annotators. Switching to an even number mainly seems to create ties. The right number is also very data dependent. Providing gold information as supervision within EM doesn’t help much unless it is quite substantial (20%+ of the data)  Citation Paper\n@InProceedings{hovy-EtAl:2013:NAACL-HLT, author = {Hovy, Dirk and Berg-Kirkpatrick, Taylor and Vaswani, Ashish and Hovy, Eduard}, title = {Learning Whom to Trust with MACE}, title: = {Learning Whom to Trust with MACE}, booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, month = {June}, year = {2013}, address = {Atlanta, Georgia}, publisher = {Association for Computational Linguistics}, pages = {1120--1130}, url = {https://aclanthology.org/N13-1132} } ","categories":"","description":"By using a generative model to explain worker annotations, we can more effectively predict the correct label, and which workers are spamming.","excerpt":"By using a generative model to explain worker annotations, we can more …","ref":"/reading-notes/old-blog/2017-10-19_mace/","tags":"","title":"Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)"},{"body":"Structured information sources have been effectively used for entity detection and typing in linear models with the information encoded as binary features. This paper looks at how to integrate vector representations of structured information into an LSTM. The solution is an additional processing step during output generation, in which the vectors for relevant entities in the structured data are combined with the standard LSTM output (note, they do not affect the cell itself, so the information is not passed on through the recurrence).\nIn this case the structured information is a set of tuples forming a graph of relations between entities, from either NELL or WordNet. The actual encoding of entities is an application of prior work; vectors representing tuples are trained with the objective that the score for any tuple is higher than made-up tuples (where the score is $v_a M_r v_b$ for entities $a$ and $b$ in relation $r$). The set of relevant entities for a particular word in the sentence is obtained by string matching, and then attention is used to combine them. There is also a kind of gating mechanism to choose how big a role the entities play in the prediction, using a combination of the input, hidden state, and cell state.\nThe results are interesting not only because this method helps, but because of how well the standard LSTM does on this task, matching or exceeding prior results. This is even more impressive given how small ACE is (if I remember correctly). The other key observations are that having a sequence level loss (using a CRF) helps, and NELL and WordNet seem to be providing different types of information (as using both leads to further improvements).\nCitation Paper\n@InProceedings{yang-mitchell:2017:Long, author = {Yang, Bishan and Mitchell, Tom}, title = {Leveraging Knowledge Bases in LSTMs for Improving Machine Reading}, title: = {Leveraging Knowledge Bases in LSTMs for Improving Machine Reading}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1436--1446}, url = {https://aclanthology.org/P17-1132} } ","categories":"","description":"Incorporating vector representations of entities from structured resources like NELL and WordNet into the output of an LSTM can improve entity and event extraction.","excerpt":"Incorporating vector representations of entities from structured …","ref":"/reading-notes/old-blog/2017-11-10_kginlstm/","tags":"","title":"Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)"},{"body":"This paper is an extension of the original AlphaGo work on using reinforcement learning to build a Go-player. Interestingly, the changes have simplified the overall model, as well as enabling it to do even better than the previous model, but now without any supervised training.\nOne key change is that there is a single core neural network learning to represent the game state. On top of that there are either a set of layers that produce an evaluation of the quality of a position, or there are a set of layers that place a distribution over moves. This ties in nicely to a lot of work happening at the moment on multi-task learning in NLP and elsewhere.\nGetting into the details, they use monte-carlo tree search to choose actions during training, then update the model to better match the outcomes observed. Starting from a completely random initialisation, the argument for why this works is that at every point in self-play the MCTS informed outcomes are just slightly better than the current model. That edge is enough to provide a useful signal, without being such a drastic shift because in self-play the two sides are closely matched. Interestingly, while the unsupervised model is worse at predicting what expert human players will do in a game, it is still better at predicting which player will win.\nCitation Paper\n@Article{AlphaGoZero, author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis}, title = {Mastering the game of Go without human knowledge}, title: {Mastering the game of Go without human knowledge}, journal = {Nature}, year = {2017}, volume = {550}, issue = {7676}, pages = {354-359}, publisher = {Macmillan Publishers Limited, part of Springer Nature}, doi = {10.1038/nature24270}, url = {http://www.nature.com/nature/journal/v550/n7676/abs/nature24270.html#supplementary-information}, } ","categories":"","description":"By using a single core model to build a game state representation, which then gives input to both state evaluation and move choice, DeepMind are able to apply reinforcement learning with self-play with no supervision and achieve state-of-the-art performance.","excerpt":"By using a single core model to build a game state representation, …","ref":"/reading-notes/old-blog/2017-10-23_alphagozero/","tags":"","title":"Mastering the game of Go without human knowledge (Silver et al., Nature 2017)"},{"body":"NLP tools seem like a natural fit for literary analysis, but the domain shift from news text is large enough to degrade performance to the point where tools are not useful. Here the specific question is how many characters are there in novels? NER + coreference would seem to be enough, but an off-the-shelf system fares poorly (and I doubt improvements in the last few years would change that story).\nThe solution is to craft a kind of coreference system focused on getting all of the characters, but not necessarily every mention. The most interesting new piece is how they identify rare characters: identify arguments of verbs that usually take people. With this tool in hand they analyse patterns of character use over time to test hypotheses from literary analysis.\nAnother key piece of this work was a tool to annotate a collection of books with character occurrences. CHARLES, their tool, is built on top of brat, adding features to help multiple annotators coordinate labels (specifically handling the case of new character identification, which modifies the set of linkable entities).\nFinally, they released the character lists identified for the novels considered (here). It would be interesting to modify a coreference resolution system to process these books, taking advantage of that information!\nCitation Paper\nAnnotation Tool Paper\n@InProceedings{vala-EtAl:2015:EMNLP, author = {Vala, Hardik and Jurgens, David and Piper, Andrew and Ruths, Derek}, title = {Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts}, title: = {Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts}, booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2015}, address = {Lisbon, Portugal}, publisher = {Association for Computational Linguistics}, pages = {769--774}, url = {https://aclanthology.org/D15-1088} } ","categories":"","description":"With some tweaks (domain-specific heuristics), coreference systems can be used to identify the set of characters in a novel, which in turn can be used to do large scale tests of hypotheses from literary analysis.","excerpt":"With some tweaks (domain-specific heuristics), coreference systems can …","ref":"/reading-notes/old-blog/2017-11-06_literarycharacters/","tags":"","title":"Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)"},{"body":"Word2Vec and other approaches provide a single vector representing a word’s meaning, giving words spatially defined relationships capturing relatedness. A natural extension is to consider regions in that space and allow some words to take up larger or smaller regions. Another natural idea is to allow a single word to have multiple representations, to capture the different senses. This paper considers both of those ideas, using multiple gaussian distributions per word.\nUsing gaussians has the nice property that there is a closed form for calculating the amount of overlap between them, which is used as a measure of similarity. Following ideas from word2vec, during learning the aim is to increase similarity between words that occur together and decrease it between random pairs that do not occur together. Once the word representations are learned, KL divergence is used for similarity, along with the standard approaches that only look at the gaussian centres.\nIn practise, two spherical distributions per word is sufficient. Performance is better than word2vec and several other approaches for multi-sense word embeddings. There was one puzzling line about the model suffering larger variance problems, but it was not quantified.\nIt would be very interesting to inject some knowledge, such as from WordNet, to guide the number of gaussians per word, rather than giving them all N. The paper also doesn’t get into details about the learned space, for example, are the two senses often far apart or close together? (in the latter case it is learning a slightly non-linear spatial representation).\nCitation Paper\n@InProceedings{athiwaratkun-wilson:2017:Long, author = {Athiwaratkun, Ben and Wilson, Andrew}, title = {Multimodal Word Distributions}, title: = {Multimodal Word Distributions}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1645--1656}, url = {https://aclanthology.org/P17-1151} } ","categories":"","description":"By switching from representing words as points in a vector space to multiple gaussian regions we can get a better model, scoring higher on multiple word similarity metrics than a range of techniques.","excerpt":"By switching from representing words as points in a vector space to …","ref":"/reading-notes/old-blog/2017-10-26_multimodalwordembeddings/","tags":"","title":"Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)"},{"body":"Several NLP tasks aim to identify information regarding entities, such as when two sections of text are referring to the same thing, or which thing out of a large set (e.g. things in Wikipedia) a piece of text is about. This paper focuses on a subset of entity linking, trying to determine which entity out of a set of candidates is the correct one (in a way a kind of reranker for entity linking).\nThe task is based on a really cool dataset from Google+UMass, which collected text that was hyperlinked to wikipedia articles. The idea is that the text (mention) is probably a reference to the thing the article describes, so it is an easy way to get entity linked data for free. Here, the data is filtered to mentions that aren’t too rare (more than 10 occurrences) and where the mention isn’t used to refer to too many different entities (the two most common entities account for over 10% of occurrences). Then, the set of things that this mention is used to refer to somewhere are treated as a list of candidates, and the task is to choose which one is correct in a given context.\nThe model is of the common style at the moment:\n The context is processed using a recurrent neural network to produce a set of vectors Attention is used to produce vectors that combine the context with a candidate entity A feedforward neural network produces a score that is maxed over to get a final decision  On the wikilinks based dataset this performs quite a bit better than other models, but it is behind on the smaller manually curated datasets used elsewhere (YAGO and PPRforNED, which link entities in the CoNLL 2003 shared task). Interestingly, augmenting the training data for YAGO with data from wikilinks does improve performance. For future users of the wikilinks data there is also some nice analysis at the end of remaining challenges, which are spit between mistakes in the data (unsurprising given the approximate collection process), answers that are too general or specific, tricky cases, and the long tail (which would be even longer without the filtering used in these experiments).\nCitation Paper\n@InProceedings{eshel-EtAl:2017:CoNLL, author = {Eshel, Yotam and Cohen, Noam and Radinsky, Kira and Markovitch, Shaul and Yamada, Ikuya and Levy, Omer}, title = {Named Entity Disambiguation for Noisy Text}, title: = {Named Entity Disambiguation for Noisy Text}, booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)}, month = {August}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {58--68}, url = {https://aclanthology.org/K17-1008} } ","categories":"","description":"The WikiLinks dataset of text mentions that are hyperlinked to wikipedia articles provides a nice testing space for named entity disambiguation, and a neural network using attention over local context does reasonably well.","excerpt":"The WikiLinks dataset of text mentions that are hyperlinked to …","ref":"/reading-notes/old-blog/2017-10-17_nedisambiguation/","tags":"","title":"Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)"},{"body":"In reference games, two players communicate in a shared world with the goal of one learning what the other is referring to. Their small scale and clear success criteria make them a convenient testbed for dialogue agents, going back decades, with recent work focusing on neural approaches. This paper considers a simple game and constrains models in various ways to improve performance and see how their communication varies, a line of work also appearing in recent papers by Jacob Andreas (ACL 2017, EMNLP 2017).\nThe game in this case is to find out two properties of an object, where there are three possible properties, each with four possible values. Given enough flexibility, models will explicitly encode every possible structure of the world as a separate symbol, which does not generalise well. Limiting the vocabulary to one symbol per property and one per value helps, but in this particular game there are only 3 possible questions, and over two turns of dialogue the 12 value words are sufficient to encode the space. Limiting even further, to 4 words for values and providing each turn in isolation to the answerer does lead to some compositionality, but clearly not full compositionality as they still make errors on unseen combinations of the inputs.\nIt’s a short paper, so they can only do so much, but some experiments I am curious about are:\n Decrease the questioner vocabulary to 2. This avoids the problem that the questioner can express the task in one step by saying what is not needed. It’s still doable, by defining an order for questions, e.g. ask about attribute A vs. B first, then in the second step ask about either C or the other option from the first step. This is a little weird as symbols need to mean different things at different time steps, but would be interesting. Increase the number of attributes to 4. This also avoids the task expression problem, by forcing there to be compositionality on the questioner side (watching the video of the talk, someone asked this in the question time, and they didn’t know).  Citation Paper\n@InProceedings{kottur-EtAl:2017:EMNLP2017, author = {Kottur, Satwik and Moura, Jos\\'{e} and Lee, Stefan and Batra, Dhruv}, title = {Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog}, title: = {Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2952--2957}, url = {https://aclanthology.org/D17-1320} } ","categories":"","description":"Constraining the language of a dialogue agent can improve performance by encouraging the use of more compositional language.","excerpt":"Constraining the language of a dialogue agent can improve performance …","ref":"/reading-notes/old-blog/2017-11-13_languagegame/","tags":"","title":"Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog (Kottur et al., 2017)"},{"body":"One reason learning for semantic parsing is difficult is that the datasets are generally small. Assuming some words behave similarly across domains, multi-domain parsing should improve performance by providing more data, which is essentially what this paper finds. They consider several configurations, all based on a sequence to sequence LSTM:\n Train a separate model for every domain. Use a single model. They do three subtypes here, (a) that’s it, (b) add an LSTM input at each step with the domain, (c) give the domain as a token at the start. Use a single encoder model, but a different decoder for each domain. Combine (1) and (3), have two encoders, one that is domain specific and one that is trained on all domains.  The results show that any of these does better than (1), with (2b) doing best. There also seems to be three sections: first the independent models (1), then the models with multiple decoders (3 and 4), then the variants of (2). A natural thing to try would be a version of (4) with a single decoder, in which case the thing that is shared is the output space representation (rather than the input space as the motivation for the paper frames it). From the paper it sounds like very little hyperparameter tuning was tried, which is a shame because it makes it less clear how definitive the results are.\nCitation Paper\n@InProceedings{herzig-berant:2017:Short, author = {Herzig, Jonathan and Berant, Jonathan}, title = {Neural Semantic Parsing over Multiple Knowledge-bases}, title: = {Neural Semantic Parsing over Multiple Knowledge-bases}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {623--628}, url = {https://aclanthology.org/P17-2098} } ","categories":"","description":"Training a single parser on multiple domains can improve performance, and sharing more parameters (encoder and decoder as opposed to just one) seems to help more.","excerpt":"Training a single parser on multiple domains can improve performance, …","ref":"/reading-notes/old-blog/2017-12-05_multidomainparsing/","tags":"","title":"Neural Semantic Parsing over Multiple Knowledge-bases (Herzig et al., 2017)"},{"body":"Games have been a focus of AI research for decades, from Samuel’s checkers program in the 1950s, to Deep Blue playing Chess in the 1990s, and AlphaGo playing Go in the 2010s. All of those are two-player sequential games. In this paper (to appear at NeurIPS), we looked at Diplomacy, a seven player game with simultaneous turns.\nThe paper makes three main contributions:\n A neural model that plays the game. Software to play the game (determining the outcomes of player actions is a non-trivial problem). Experiments with supervised learning and reinforcement learning.  Our paper only considers the version of the game where players can not talk to each other (No Press). Engaging in conversation in the game is a fascinating challenge that will involve a lot more work.\nHow well does the bot play the game? It convincingly beats prior systems designed for the game. Playing against it, I saw an impressive improvement over the course of the project. Early on I won trivially with mostly conservative moves. Later I had to carefully consider my moves, and was unable to win as certain powers (e.g. Austria). Eventually I was unable to beat the bot without playing several times, using observations from one game to inform my strategy in subsequent games. I am not an expert player, but I doubt a human playing one power in the game with no prior knowledge can win against the bot. However, I think a single bot playing against six skilled humans would almost definitely lose (we did not test this setting).\nCitation Paper\n@InProceedings{neurips19diplomacy, author = {Paquette, Philip and Lu, Yuchen and Bocco, Steven and Smith, Max O. and Ortiz-Gagn{\\'e}, Satya and Kummerfeld, Jonathan K. and Pineau, Joelle and Singh, Satinder and Courville, Aaron}, title = {No-Press Diplomacy: Modeling Multi-Agent Gameplay}, title: = {No-Press Diplomacy: Modeling Multi-Agent Gameplay}, booktitle = {Advances in Neural Information Processing Systems 32}, year = {2019}, month = {December}, pages = {}, url = {}, arxiv = {https://arxiv.org/abs/1909.02128}, } ","categories":"","description":"Games have been a focus of AI research for decades, from Samuel's checkers program in the 1950s, to Deep Blue playing Chess in the 1990s, and AlphaGo playing Go in the 2010s. All of those are two-player...","excerpt":"Games have been a focus of AI research for decades, from Samuel's …","ref":"/reading-notes/old-blog/2019-09-13_diplomacynopress/","tags":"","title":"No-Press Diplomacy: Modeling Multi-Agent Gameplay (Paquette et al., 2019)"},{"body":"","categories":"","description":"These are blog posts from my old website.\n","excerpt":"These are blog posts from my old website.\n","ref":"/reading-notes/old-blog/","tags":"","title":"Old Blog Posts"},{"body":"When people read a sentence they form an entire world around it, making inferences about unwritten properties based on their prior knowledge. If we want NLP systems to do the same, we need data to train and test this common sense aspect of language understanding.\nThis paper is about a new dataset of automatically generated sentence pairs with human ratings. The ratings indicate that given the first sentence, the second sentence is either very likely, likely, plausible, technically possible, or impossible. These ratings are crowdsourced, using the median of three ratings per example. The pay rates are fairly low, at $3.45 / hour (1.99c / example and 20.71 seconds / example), though it’s possible that the time is being skewed by outliers, and it’s unclear exactly how pay was determined (does this include Amazon’s cut? Why is it an average cost per example, rather than just the cost?).\nThe main contribution is the novel way of generating the sentences. For each prompt sentence, an argument is chosen, and then a hypothesis is generated in one of three ways (all trained with Gigaword). (1) A sequence-to-sequence model takes the full sentence as input and generates a sentence. (2) The same as (1), but with only the argument provided. (3) A sentence is sampled from templates generated by abstraction of sentences in the training data. Together these produce a diverse set of examples that get a range of ratings, with only ‘likely’ being somewhat rarer. They also labeled some pairs from SNLI and COPA, to enable analysis of how this task compares.\nThey also provide a set of baselines for the new task. Using the baselines, they show that the generated sentences are somewhat more difficult than the pairs from existing datasets. The standard metrics proposed are MSE and Spearman’s Rho (both necessary because otherwise always guessing the middle would get an MSE better than any of the proposed baselines). Interestingly, regression does quite a bit better than a set of one-vs-all SVMs on MSE, and also slightly better on rho (I’m surprised because while there is an ordinal scale, it doesn’t feel like it should have a strong continuous interpretation).\nCitation Paper\n@article{TACL1082, author = {Zhang, Sheng and Rudinger, Rachel and Duh, Kevin and Van Durme, Benjamin }, title = {Ordinal Common-sense Inference}, title: {Ordinal Common-sense Inference}, journal = {Transactions of the Association for Computational Linguistics}, volume = {5}, year = {2017}, keywords = {}, issn = {2307-387X}, url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1082}, pages = {379--395} } ","categories":"","description":"A new task and dataset of 39k examples for common sense reasoning, with a sentence generated for each prompt and a manual label indicating their relation, from very likely to impossible.","excerpt":"A new task and dataset of 39k examples for common sense reasoning, …","ref":"/reading-notes/old-blog/2017-11-27_commonsense/","tags":"","title":"Ordinal Common-sense Inference (Zhang et al., 2017)"},{"body":"Mixture of experts can be seen as an ensemble approach in which we assume that each of our models is effective under different circumstances and so we combine them by switching between which we use to make our decision. From this perspective the idea can be applied to any set of models, but here the idea is to train (1) the expert models, (2) our method of choosing between them, and (3) a set of common model components, all at the same time.\nThe particular set up here is that they modify a series of LSTM layers, adding a new layer in between each pair of LSTMs. The new layer has a set of small feed-forward networks (the experts) and an even simpler network that chooses which expert to use. One big benefit of this is that a lot of computation can be avoided when we know some of the small feed-forward components are going to be ignored. As a result, they can scale up to massive networks while still having reasonable runtimes.\nSome key things to make this all work:\n Enough machines to train it! Also, there is a careful mixture of data and model parallelism during training. Some noise in the expert selection process A loss that directly encourages the use of multiple experts  One thing mentioned in passing is how this relates to a form of dropout (which can be viewed as training a set of overlapping experts, kind of).\nCitation Paper\n@inproceedings{45929, title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, title: {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff}, year = {2017}, booktitle = {ICLR}, URL = {https://openreview.net/pdf?id=B1ckMDqlg}, } ","categories":"","description":"Neural networks for language can be scaled up by using a form of selective computation, where a noisy single-layer model chooses among feed-forward networks (experts) that sit between LSTM layers.","excerpt":"Neural networks for language can be scaled up by using a form of …","ref":"/reading-notes/old-blog/2017-11-01_mixtureofexperts/","tags":"","title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Shazeer et al., 2017)"},{"body":"Training models requires massive amounts of labeled data. We usually sample data iid from the target domain (e.g. newspapers), but it seems intuitive that this means we wast effort labeling samples that are obvious or easy and so not informative during training. Active Learning follows that intuition, labeling data incrementally, selecting the next example(s) to label based on what a model considers uncertain. Lots of work has shown this can be effective for that model, but if the labeled dataset is then used to train another model will it also do well?\nFor text classification this paper finds the answer is no: training model X on iid samples is as good or better than training on samples collected while active learning with model Y. They show this through experiments with four datasets and three models, training on up to 25% of the available data. For named entity recognition the story is different in my opinion - iid is consistently slightly worse, though the gains from active learning are small in all cases (0 to 0.6 point gain for the better model, 0.4 to 1.7 for the weaker model). One caveat is that these models are not state-of-the-art. For CoNLL 2003 NER, many models score around 93, but these models are getting 70-90. On OntoNotes, the best results are close to 90, but these models get 74-85. This is still an interesting result, but I’m left with a few questions:\n This work focused on a low data scenario. What if I have a lot of data? It may be that sampling iid and active learning based samples were similar here because either way the data was capturing the core phenomena. The challenge here is that you can’t run this experiment easily with an existing dataset (unless it is truly massive). How does the sampled data differ from iid data? Is there a significant shift in the distribution of class types? What about using a hybrid approach, with some data sampled iid and other data sampled randomly?  Overall, what I take away from this work is that active learning may not be the right choice for building a small dataset in NLP. For large datasets, building models, or other tasks and domains the conclusions are less clear, though it is certainly worth being aware of the risk that a dataset made with active learning may not be equally useful to all models.\nCitation Paper\nTwitter discussion in 2018) and 2019.\n@inproceedings{lowell-etal-2019-practical, title = \"Practical Obstacles to Deploying Active Learning\", title: \"Practical Obstacles to Deploying Active Learning\", author = \"Lowell, David and Lipton, Zachary C. and Wallace, Byron C.\", booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\", month = \"nov\", year = \"2019\", address = \"Hong Kong, China\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D19-1003\", doi = \"10.18653/v1/D19-1003\", pages = \"21--30\", abstract = \"Active learning (AL) is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. In AL, one iteratively selects training examples for annotation, often those for which the current model is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice, one does not have the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.\", } ","categories":"","description":"Training models requires massive amounts of labeled data.  We usually sample data iid from the target domain (e.g. newspapers), but it seems intuitive that this means we wast effort labeling samples that are obvious or easy and so not informative during training.  Active Learning follows that intuition, labeling data incrementally, selecting the next example(s) to label based on what a model considers uncertain.  Lots of work has shown this can be effective for that model, but if the labeled dataset is then used to train another model will it also do well?","excerpt":"Training models requires massive amounts of labeled data.  We usually …","ref":"/reading-notes/old-blog/2020-09-17_activelearningbrittle/","tags":"","title":"Practical Obstacles to Deploying Active Learning (Lowell, et al., EMNLP 2019)"},{"body":"The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset). Some of these are discussed in my CoNLL Shared Task submission paper, the biggest being the choice to not annotate mentions that are not coreferent. This paper describes a new dataset that has a different set of compromises, specifically:\n A broader definition of coreference (e.g. appositives are coreferent) All mentions annotated Different annotation methods for different subsets of the data (training data is double annotated and then adjudicated, while the development and test data is triple annotated, all pairs of annotations are adjudicated, then the outcomes are merged by voting) A variety of genres, but generally simpler language  The dataset is 10x the size of OntoNotes and freely available, which is fantastic. The source text is 2/3rds the RACE dataset (English reading comprehension exams from China), and 1/3rd scraped websites. Measurements of annotator agreement suggest the annotations are not as consistent as OntoNotes, but still good enough to be a useful resource. I do disagree with one aspect of the paper’s analysis - the results show a substantial gain in performance when providing gold mentions, suggesting to me that it remains an important challenge in coreference resolution. I’m also curious whether my coreference analysis tool would find different patterns in errors on this dataset compared to OntoNotes.\nCitation Paper\nData\n@InProceedings{Chen:EMNLP:2018, author = {Chen, Hong and Fan, Zhenhua and Lu, Hao and Yuille, Alan and Rong, Shu}, title = {PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution}, title: = {PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution}, booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, year = {2018}, publisher = {Association for Computational Linguistics}, pages = {172--181}, location = {Brussels, Belgium}, url = {https://aclanthology.org/D18-1016}, } ","categories":"","description":"The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset).  Some of these are discussed in...","excerpt":"The OntoNotes dataset, which is the focus of almost all coreference …","ref":"/reading-notes/old-blog/2018-11-08_corefdata/","tags":"","title":"PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution (Chen et al., 2018)"},{"body":"Being able to query a database in natural language could help make data accessible to more people. Systems that do this have to solve two challenges: (1) understanding the query and (2) expressing the response in a way the user will understand. Recently there have been papers in the NLP community on the first challenge, but this paper comes from the DB community and considers the second.\nThe approach assumes we have a syntactic parse of the query and an alignment between the parse and the SQL query it corresponds to (they rely on prior work for this query interpretation piece). Given that, the new idea in this paper is to take the database results and use the alignment to insert values for each field into the original parse, and from there into the original question. To avoid extremely long sentences (when there are multiple result rows) they define a procedure to identify ways to summarise results.\nHowever, I’m not convinced by the evaluation. The dataset they use was collected by (1) enumerating the 196 types of queries people could ask using the Microsoft Academic Search service, and (2) a person manually writing a question for each query. As a result, the questions feel very formulaic and also only cover cases that we already have a user-friendly interface for, making it unclear how well this will generalise to more natural data. Still, this work explores an interesting problem and it’s cool to see a direct use of syntactic parsing!\nCitation Paper\n@Article{Deutch:2017, author = {Deutch, Daniel and Frost, Nave and Gilad, Amir}, title = {Provenance for Natural Language Queries}, title: {Provenance for Natural Language Queries}, journal = {Proceedings of the VLDB Endowment}, volume = {10}, number = {5}, month = {Jan}, year = {2017}, issn = {2150-8097}, pages = {577--588}, doi = {10.14778/3055540.3055550}, url = {http://www.vldb.org/pvldb/vol10/p577-deutch.pdf}, } ","categories":"","description":"Being able to query a database in natural language could help make data accessible ...","excerpt":"Being able to query a database in natural language could help make …","ref":"/reading-notes/old-blog/2018-03-08_sql/","tags":"","title":"Provenance for Natural Language Queries (Deutch et al., 2017)"},{"body":"It would be convenient to have a way to represent sentences in a vector space, similar to the way vectors are frequently used to represent input words for a task. Quite a few sentence embeddings methods have been proposed, but none have really caught on. Building on prior work by the same authors, the approach here is to define a neural network that maps a sentence to a vector, then train it with a loss function that measures similarity between the vectors for paraphrases.\nThis paper scales up the approach, using millions of paraphrases, and explores a range of models. To get the paraphrases they use translation (start with a sentence, translate it to another language and back, then assume the translation is a paraphrase). For negative examples they use the sentence that the model currently thinks is most similar other than the correct one (choosing this from a large enough set is key).\nThe best model is very simple - concatenate together the average of word vectors and the average of character trigram vectors. That consistently beats prior work, including convolutional models, and LSTMs. In a way, this is nice as it is a simple way to get a sentence representation! On the other hand, this can’t possibly capture the semantics of a sentence fully since it doesn’t take word order into consideration at all.\nCitation ArXiv Paper\n@ARTICLE{2017arXiv171105732W, author = {Wieting, J. and {Gimpel}, K.}, title = {Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations}, title: = {Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations}, journal = {ArXiv e-prints}, archivePrefix = {arXiv}, eprint = {1711.05732}, primaryClass = {cs.CL}, year = {2017}, month = {November}, url = {https://arxiv.org/abs/1711.05732}, } ","categories":"","description":"With enough training data, the best vector representation of a sentence is to concatenate an average over word vectors and an average over character trigram vectors.","excerpt":"With enough training data, the best vector representation of a …","ref":"/reading-notes/old-blog/2018-01-31_sentencerepfromparaphrases/","tags":"","title":"Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (Wieting et al., 2017)"},{"body":"For any given task, automatic systems are fast, while annotation is accurate. This work is about bridging that gap to provide a way for a team of annotators to produce real-time high quality labels. The specific application is speech transcription, in which automatic systems are not accurate, while average people are slow (experts can transcribe in real time, but are very expensive).\nThe solution is to carefully break up the task and combine annotations back together. To get it to work well there are a range of subtle design decisions:\n People hear the entire audio stream, but with their section at normal volume and the rest quieter. This allows them to focus their effort while still understanding the context. The alignment process combines annotations with guidance from a language model and a model of typos based on keyboard layout. Words are locked in shortly after being typed, to encourage workers to go on rather than revising their own errors.  Follow up work added several more ideas to improve performance:\n Time warping, slowing down to half speed for their section, then going to 1.5x for the rest. Use ASR as well, either as another worker (with very uncorrelated errors), or as a starting point for human editing (or vice versa). Use A* search rather than a greedy algorithm for the alignment.  Performance does not reach the level of a professional, but is far better than ASR. From the paper it’s tricky to see a final cost, but it is certainly far lower than the professional.\nCitation Paper\n@inproceedings{Lasecki:2012:RCG:2380116.2380122, author = {Lasecki, Walter and Miller, Christopher and Sadilek, Adam and Abumoussa, Andrew and Borrello, Donato and Kushalnagar, Raja and Bigham, Jeffrey}, title = {Real-time Captioning by Groups of Non-experts}, title: {Real-time Captioning by Groups of Non-experts}, booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology}, series = {UIST '12}, year = {2012}, isbn = {978-1-4503-1580-7}, location = {Cambridge, Massachusetts, USA}, pages = {23--34}, numpages = {12}, url = {http://doi.acm.org/10.1145/2380116.2380122}, doi = {10.1145/2380116.2380122}, acmid = {2380122}, publisher = {ACM}, address = {New York, NY, USA}, keywords = {captioning, crowdsourcing, deaf, hard of hearing, real-time, text alignment, transcription}, } ","categories":"","description":"By dividing a task up among multiple annotators carefully we can achieve high-quality real-time annotation of data, in this case transcription of audio.","excerpt":"By dividing a task up among multiple annotators carefully we can …","ref":"/reading-notes/old-blog/2017-10-31_realtimecaptioning/","tags":"","title":"Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)"},{"body":"Selectional preferences in this context are about how some verbs are more likely to take certain types of arguments (e.g. people laugh, computers do not). Many papers have added features or structures to coreference systems aiming to get at this kind of information. This paper presents another way of doing it and experiments that probe how useful it is (punchline: not very).\nTheir approach is to parse a large amount of text, producing noun-verb pairs. They learn vector representations of the relations and try to create a single space containing both entities and relations (e.g. Michigan gets a vector, as does attended@dobj). The goal is that entities end up in locations similar to the locations of relations they are selected for.\nFor results, first it seems like these vector similarities do not correlate particularly strongly with being coreferent. It could be that the feature on its own isn’t enough, or this representation might not be capturing it effectively. Adding this to the Stanford coreference system they are able to get slight gains, though the improvement might not be statistically significant.\nI’m not sure exactly how to do this, but it would be neat if a vector at some point of the model could be modified to remove any correlation with these features, and see what that does to performance. If performance remains high, then this actually is an uninformative feature, but if it drops that suggests the model is already learning it.\nCitation Paper\n@InProceedings{heinzerling-moosavi-strube:2017:EMNLP2017, author = {Heinzerling, Benjamin and Moosavi, Nafise Sadat and Strube, Michael}, title = {Revisiting Selectional Preferences for Coreference Resolution}, title: = {Revisiting Selectional Preferences for Coreference Resolution}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {1332--1339}, url = {https://aclanthology.org/D17-1138} } ","categories":"","description":"It seems intuitive that a coreference system could benefit from information about what nouns a verb selects for, but experiments on explicitly adding a representation of it to a neural system does not lead to gains, implying it is already learning them or they are not useful.","excerpt":"It seems intuitive that a coreference system could benefit from …","ref":"/reading-notes/old-blog/2017-12-06_coreferencearguments/","tags":"","title":"Revisiting Selectional Preferences for Coreference Resolution (Heinzerling et al., 2017)"},{"body":"Like the UCCA parser, this paper explores a transition-based neural model for semantic parsing, but for Minimal Recursion Semantics instead of Universal Conceptual Cognitive Annotation. Comparing MRS and UCCA, every word gets a non-terminal symbol in MRS, plus additional non-terminals for phenomena like quantification, while UCCA only introduces them for special cases like linking to a coordination. Both have discontinuous graph structures, creating a challenge for most parsers.\nThe UCCA and MRS parsers extend the basic shift-reduce transitions in different ways. Here, crossing edges can be added with a transition that forms edges between the front of the buffer and a word anywhere in the stack, while the UCCA parser used swapping and a additional reduce actions for graph edges. The models are similar, both using a form of stack-RNN, but with different structures (partly as a result of the different transition schemes). The results in this case are not state-of-the-art, though this task has received more attention, and the data is slightly biased (the parser that does better, ACE, is based on the grammar that was used to determine which sentences to include). However, the system can also be applied to AMR, and does fairly well, better than other neural AMR parsers at the time (and more recent ideas for improvements are large orthogonal).\nCitation Paper\n@InProceedings{buys-blunsom:2017:Long, author = {Buys, Jan and Blunsom, Phil}, title = {Robust Incremental Neural Semantic Graph Parsing}, title: = {Robust Incremental Neural Semantic Graph Parsing}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1215--1226}, url = {https://aclanthology.org/P17-1112} } ","categories":"","description":"A neural transition based parser with actions to create non-local links can perform well on Minimal Recursion Semantics parsing.","excerpt":"A neural transition based parser with actions to create non-local …","ref":"/reading-notes/old-blog/2017-11-20_mrsparser/","tags":"","title":"Robust Incremental Neural Semantic Graph Parsing (Buys et al., 2017)"},{"body":"Semantic parsing datasets generally consist of (question, answer) pairs, where each pair is completely independent of the rest (one exception is ATIS, which has multi-turn conversations, though most work doesn’t use them). In reality, we often ask a series of simple questions that together form a complex one, for example “What flights are available from Detroit to Sydney? And how much is the price if I don’t want to leave before 8am?” This work explores these kinds of sequential questions with a new dataset and algorithm.\nThe dataset was formed by asking crowd workers to rephrase questions from the WikiTableQuestions dataset into sequences of shorter questions. This naturally constrains the types of questions (in particular, they reference a single table only), but covers a range of domains. With 6,066 question sequences, and on average 2.9 questions / sequence, it’s a large dataset by semantic parsing standards. However, there are no logical forms, only the row, column, or cell(s) that contain the answer.\nTo solve the problem, they treat it as choosing a sequence of actions, where each action generate a part of the execution instructions. The model follows the recent approach of considering the contents of the database as part of the calculation (e.g. by taking the dot product of the vector for a cell and the vector for the question).\nThe system has consistently better performance than other QA systems on the new dataset (though no results are shown for the WikiTableQuestions dataset). At only 12.8% of sequences completely correct, there is plenty of scope for improvement. Based on the description of the operators there are definitely additional abilities that would be useful, so this model has potential to improve. That said, it seems difficult to generalise the model to handle more complicated databases with multiple interconnected tables.\nCitation Paper\n@InProceedings{iyyer-yih-chang:2017:Long, author = {Iyyer, Mohit and Yih, Wen-tau and Chang, Ming-Wei}, title = {Search-based Neural Structured Learning for Sequential Question Answering}, title: = {Search-based Neural Structured Learning for Sequential Question Answering}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1821--1831}, url = {https://aclanthology.org/P17-1167} } ","categories":"","description":"A new dataset containing multi-turn questions about a table, and a model that generates a kind of logical form, but scores actions based on the content of the table.","excerpt":"A new dataset containing multi-turn questions about a table, and a …","ref":"/reading-notes/old-blog/2017-10-10_seqqa/","tags":"","title":"Search-based Neural Structured Learning for Sequential Question Answering (Iyyer et al., ACL 2017)"},{"body":"Update After prior work came to light that uses the same non-linearity, this paper was updated to focus more on the search process used to investigate the space. One new takeaway for me was how diverse the effective activation functions were (see figures in the new version of the paper).\nOriginal Post Non-linear functions are the key to the representation power of neural networks. Many different ones have been proposed, though it is difficult to make theoretical claims of their properties and so the choice of which to use is generally empirical. This paper proposes a new non-linearity, $\\text{swish}(x) = x \\cdot \\text{sigmoid}(x)$.\nInterestingly, it was chosen by a combination of exhaustive search and search with reinforcement learning across a range of functions, evaluating on CIFAR-10 with a small model. ReLU variants were consistently second-best to swish variants, and generally the more complicated functions performed worse. They do mention two functions that performed well, but didn’t generalise: $\\text{cos}(x) - x$ and $\\text{max}(x, \\text{tanh}(x))$, which look like this:\nIn a range of experiments in vision and machine translation swish does at least as well or slightly better than the alternatives. It also seems more robust to network depth and to work across different network structures. As for why it works so well, there are two main ideas: (1) it adds smoothness to the ReLU, (2) it has some sensitivity to negative inputs. Both of these seem particularly important at the start of training.\nCitation ArXiv Paper\n@ARTICLE{2017arXiv171005941R, author = {Ramachandran, P. and {Zoph}, B. and {Le}, Q.~V.}, title = \"{Swish: a Self-Gated Activation Function}\", title: \"{Swish: a Self-Gated Activation Function}\", journal = {ArXiv e-prints}, archivePrefix = \"arXiv\", eprint = {1710.05941}, keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning}, year = 2017, month = oct, adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171005941R}, adsnote = {Provided by the SAO/NASA Astrophysics Data System} } ","categories":"","description":"Switching from the ReLU non-linearity, $\\text{max}(0, x)$, to Swish, $x \\cdot \\text{sigmoid}(x)$, consistently improves performance in neural networks across both vision and machine translation tasks.","excerpt":"Switching from the ReLU non-linearity, $\\text{max}(0, x)$, to Swish, …","ref":"/reading-notes/old-blog/2017-10-27_swishactivation/","tags":"","title":"Searching for Activation Functions (Ramachandran et al., 2017)"},{"body":"Getting high quality annotations from crowdsourcing requires careful design. This paper looks at how one annotation a worker does can influence their next annotation, for example:\n When scoring translations, a good example may make the next one look worse in comparison For labeling tasks, we may expect a long sequence of the same label to be rare (the gambler’s fallacy)  To investigate this they fit a linear model with inputs (previous label, gold label, random noise) and see what the coefficients are. Across multiple tasks, there is a non-zero correlation with the previous label. Interestingly, there also seems to be a learning effect for good workers, where over time they become calibrated and show less sequence bias. Fortunately, there is a simple solution - for each worker, give every annotator their documents in a different random order! With that change, averaging over annotations should avoid this bias.\nCitation Paper\n@InProceedings{mathur-baldwin-cohn:2017:EMNLP2017, author = {Mathur, Nitika and Baldwin, Timothy and Cohn, Trevor}, title = {Sequence Effects in Crowdsourced Annotations}, title: = {Sequence Effects in Crowdsourced Annotations}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2860--2865}, url = {https://aclanthology.org/D17-1306} } ","categories":"","description":"Annotator sequence bias, where the label for one item affects the label for the next, occurs across a range of datasets. Avoid it by separately randomise the order of items for each annotator.","excerpt":"Annotator sequence bias, where the label for one item affects the …","ref":"/reading-notes/old-blog/2017-12-08_crowdbias/","tags":"","title":"Sequence Effects in Crowdsourced Annotations (Mathur et al., 2017)"},{"body":"This paper is a follow-up to yesterday’s, where the approach is implemented and evaluated on English and Chinese, with very strong results. The novel contribution is the idea of introducing alternating steps in the dynamic program to do unary steps (not a novel idea in general, but novel in its application to the dynamic programming version of shift-reduce parsing).\nWhat I found interesting here were the clear benefits of the dynamic program (DP) version. One way of viewing this is that the DP gives a more intelligent type of beam, avoiding the issue where the beam is filled with minor variations on a theme. Results are given for various beam sizes in both approaches, but it would be interesting to see a graph where the x-axis is number of items built. I suspect in that situation, the gap would be smaller. On speed, there is the nice theoretical bound of $O(n)$ for this approach, but that obscures a grammar constant related to the item structure.\nCitation Paper\n@InProceedings{mi-huang:2015:NAACL-HLT, author = {Mi, Haitao and Huang, Liang}, title = {Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice}, title: = {Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice}, booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, month = {May--June}, year = {2015}, address = {Denver, Colorado}, publisher = {Association for Computational Linguistics}, pages = {1030--1035}, url = {https://aclanthology.org/N15-1108} } ","categories":"","description":"An implementation of the transition-parsing as a dynamic program idea, leading to fast parsing and strong performance.","excerpt":"An implementation of the transition-parsing as a dynamic program idea, …","ref":"/reading-notes/old-blog/2017-10-25_shiftreducedp/","tags":"","title":"Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice (Mi and Huang, 2015)"},{"body":"The first step in almost any neural network model for language is to look up a vector for each token in the input. These vectors express relations between the words, but it is difficult to know exactly what relations. This work proposes a way to modify a vector space of words to have more interpretable dimensions.\nThe core idea is actually more general, it is a new loss that encourages sparsity in an auto-encoder. In this case the model is very simple: input a word vector, apply an affine transformation and a pointwise nonlinearity, producing a hidden vector, then apply another affine transformation to get the output. The loss is a combination of how well the input and output match (reconstruction loss), plus a function that is minimised when the average activation is below a threshold (average sparsity loss), and the new idea, a loss that is minimised at either 0 or 1 for each hidden value. To get the hidden values to be bounded between 1 and 0, the nonlinearity used is a modified ReLU that stops increasing after reaching 1. After training, the hidden values become the new word vectors.\nTo evaluate interpretability they consider the top 4 words along each dimension, add a random word, and ask a person to identify the odd word out. Using either word2vec or GloVe as the initial vectors and applying this method, the results shown a dramatic difference (~25 vs. ~70). On downstream tasks the story is more mixed. With 1,000 dimensional vectors, there is usually an improvement for GloVe, but not for word2vec, and the differences are generally small. Apparently going up to 2,000 further improves interpretability scores, but ‘at a severe cost’ for the downstream tasks. Going the other direction, to 500, hurts interpretability, and probably doesn’t improve downstream performance (it isn’t mentioned).\nI would be curious to see if taking these new word vectors and applying them to a downstream task like parsing, but letting them change during training, would be beneficial. The general idea of a sparse auto-encoder also seems cool and may have other applications.\nCitation ArXiv Paper\n@ARTICLE{2017arXiv171108792S, author = {Subramanian, A. and {Pruthi}, D. and {Jhamtani}, H. and {Berg-Kirkpatrick}, T. and {Hovy}, E.}, title = {SPINE: SParse Interpretable Neural Embeddings}, title: = {SPINE: SParse Interpretable Neural Embeddings}, journal = {ArXiv e-prints}, archivePrefix = {arXiv}, eprint = {1711.08792}, primaryClass = {cs.CL}, year = {2017}, month = {November}, url = {https://arxiv.org/abs/1711.08792}, } ","categories":"","description":"By introducing a new loss that encourages sparsity, an auto-encoder can be used to go from existing word vectors to new ones that are sparser and more interpretable, though the impact on downstream tasks is mixed.","excerpt":"By introducing a new loss that encourages sparsity, an auto-encoder …","ref":"/reading-notes/old-blog/2017-11-28_interpretableembeddings/","tags":"","title":"SPINE: SParse Interpretable Neural Embeddings (Subramanian et al., 2017)"},{"body":"We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models usually end up doing reasonably well. This paper asks an important question - are those metrics measuring generalisability effectively? In particular, if we sample our test set from a slightly different distribution of data, do models still work well?\nAs a controlled set up they form a simple dataset as follows for each sentence:\n Go through the sentence left to right For each word generate three words in the output, where the output words are randomly sampled from a small vocabulary that is unique to each input word  This is clearly learnable and it seems reasonable that a sequence-to-sequence neural model with attention should be able to learn it. Experiments show they do, getting close to 100% on a test set sampled the same way as the training set (input length 5-10, no symbol used twice). However, if the test set is slightly different, with sequences of length 11-15, then results vary from 0% to 98% depending on the random seed in training (other variations also lead to large variations). What this means is that sometimes the model is not learning to generalise. They also show that the models that do generalise can only do so in one way (e.g. remain effective when length varies, or remain effective when symbols are used more than once in the input).\nA few takeaways:\n Make sure your training and testing data are sampled from the distribution you are interested in More study of training data order and weight initialisation is needed (these are the two factors impacted by the random seed)  Incidentally, I am a co-author on an ACL paper that points out a similar issue for mapping text questions to SQL queries. If we restrict the test set to be novel queries (i.e. the model has to generalise) performance falls through the floor.\nCitation Paper\n@Article{Weber:2018:GenDeep, author = {Weber, Noah and Shekhar, Leena and Balasubramanian, Niranjan}, title = {The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models}, title: {The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models}, journal = {Workshop on New Forms of Generalization in Deep Learning and NLP (NAACL 2018)}, year = {2018}, url = {https://arxiv.org/abs/1805.01445}, } ","categories":"","description":"We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models...","excerpt":"We know that training a neural network involves optimising over a …","ref":"/reading-notes/old-blog/2018-05-08_seq2seqsensitivity/","tags":"","title":"The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models (Weber et al., 2018)"},{"body":"It turns out that if the vectors learned by word2vec are projected into a plane they all point in the same direction. Also, the context vectors (which are part of the algorithm, but not retained afterwards) point the other way. When visualising with t-SNE this effect is not visible because of the way the space is warped to optimise the t-SNE objective.\nThis is surprising, and may seem problematic since it doesn’t fit our goals for what these vectors should be capturing. However, it doesn’t seem to impact downstream tasks, for example, GloVe does not have this property, and doesn’t seem to derive a great benefit from it.\nCitation Paper\n@InProceedings{mimno-thompson:2017:EMNLP2017, author = {Mimno, David and Thompson, Laure}, title = {The strange geometry of skip-gram with negative sampling}, title: = {The strange geometry of skip-gram with negative sampling}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2873--2878}, url = {https://aclanthology.org/D17-1308} } ","categories":"","description":"Surprisingly, word2vec (negative skipgram sampling) produces vectors that point in a consistent direction, a pattern not seen in GloVe (but also one that doesn't seem to cause a problem for downstream tasks).","excerpt":"Surprisingly, word2vec (negative skipgram sampling) produces vectors …","ref":"/reading-notes/old-blog/2017-12-12_wordvectorgeometry/","tags":"","title":"The strange geometry of skip-gram with negative sampling (Mimno et al., 2017)"}]