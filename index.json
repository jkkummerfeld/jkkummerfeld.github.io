[{"authors":["admin"],"categories":null,"content":"My work: I aim to make language a complementary part of the interface for any application (e.g. from desktop spreadsheets to mobile apps). This involves solving a range of fascinating challenges through new methods in Artificial Intelligence and Crowdsourcing.\nI am currently a Postdoctoral Research Fellow at the University of Michigan, working on Natural Language Processing and Crowdsourcing. I completed my Ph.D. in the UC Berkeley NLP Group, advised by Dan Klein, and my B.Sc. in the University of Sydney Schwa Lab, advised by James Curran.\nE-mail: jkummerf@umich.edu\nCV: pdf\n","date":1617235200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617235200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.jkk.name/author/jonathan-k.-kummerfeld/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jonathan-k.-kummerfeld/","section":"authors","summary":"My work: I aim to make language a complementary part of the interface for any application (e.g. from desktop spreadsheets to mobile apps). This involves solving a range of fascinating challenges through new methods in Artificial Intelligence and Crowdsourcing.\nI am currently a Postdoctoral Research Fellow at the University of Michigan, working on Natural Language Processing and Crowdsourcing.","tags":null,"title":"Jonathan K. Kummerfeld","type":"authors"},{"authors":["Allison Lahnala","Gauri Kambhatla","Jiajun Peng","Matthew Whitehead","Gillian Minnehan","Eric Guldan","Jonathan K. Kummerfeld","Anıl Çamcı","Rada Mihalcea"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"6178ad9196462122b8e4271cccc54ed1","permalink":"https://www.jkk.name/publication/evomusart21/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/evomusart21/","section":"publication","summary":"Natural language processing methods have been applied in a variety of music studies, drawing the connection between music and language. In this paper, we expand those approaches by investigating chord embeddings, which we apply in two case studies to address two key questions: (1) what musical information do chord embeddings capture?; and (2) how might musical applications benefit from them? In our analysis, we show that they capture similarities between chords that adhere to important relationships described in music theory. In the first case study, we demonstrate that using chord embeddings in a next chord prediction task yields predictions that more closely match those by experienced musicians. In the second case study, we show the potential benefits of using the representations in tasks related to musical stylometrics.","tags":null,"title":"Chord Embeddings: Analyzing What They Capture and Their Role for Next Chord Prediction and Artist Attribute Prediction","type":"publication"},{"authors":["Seokhwan Kim","Michel Galley","Chulaka Gunasekara","Sungjin Lee","Adam Atkinson","Peng Baolin","Hannes Schulz","Jianfeng Gao","Jinchao Li","Mahmoud Adada","Minlie Huang","Luis Lastras","Jonathan K. Kummerfeld","Walter S. Lasecki","Chiori Hori","Anoop Cherian","Tim Marks","Abhinav Rastogi","Xiaoxue Zang","Srinivas Sunkara","Raghav Gupta"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"250aa92df3b461f7898f7e9598a3a4df","permalink":"https://www.jkk.name/publication/dstc8-ieee/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/dstc8-ieee/","section":"publication","summary":"This paper introduces the Eighth Dialog System Technology Challenge. In line with recent challenges, the eighth edition focuses on applying end-to-end dialog technologies in a pragmatic way for multi-domain task-completion, noetic response selection, audio visual scene-aware dialog, and schema-guided dialog state tracking tasks. This paper describes the task definition, provided datasets, baselines and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks.","tags":null,"title":"Overview of the Eighth Dialog System Technology Challenge: DSTC8","type":"publication"},{"authors":["Charles Welch","Jonathan K. Kummerfeld","Verónica Pérez-Rosas","Rada Mihalcea"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"ebf94ed7b14f8a356565db30591a3c74","permalink":"https://www.jkk.name/publication/coling20personal/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/coling20personal/","section":"publication","summary":"In this paper, we introduce personalized word embeddings, and examine their value for language modeling. We compare the performance of our proposed prediction model when using personalized versus generic word representations, and study how these representations can be leveraged for improved performance. We provide insight into what types of words can be more accurately predicted when building personalized models. Our results show that a subset of words belonging to specific psycholinguistic categories tend to vary more in their representations across users and that combining generic and personalized word embeddings yields the best performance, with a 4.7{%} relative reduction in perplexity. Additionally, we show that a language model using personalized word embeddings can be effectively used for authorship attribution.","tags":null,"title":"Exploring the Value of Personalized Word Embeddings","type":"publication"},{"authors":["Stefan Larson","Adrian Cheung","Anish Mahendran","Kevin Leach","Jonathan K. Kummerfeld"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"8b48f14fa09b996991202c30dd57ca83","permalink":"https://www.jkk.name/publication/coling20svp/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/coling20svp/","section":"publication","summary":"Slot-filling models in task-driven dialog systems rely on carefully annotated training data. However, annotations by crowd workers are often inconsistent or contain errors. Simple solutions like manually checking annotations or having multiple workers label each sample are expensive and waste effort on samples that are correct. If we can identify inconsistencies, we can focus effort where it is needed. Toward this end, we define six inconsistency types in slot-filling annotations. Using three new noisy crowd-annotated datasets, we show that a wide range of inconsistencies occur and can impact system performance if not addressed. We then introduce automatic methods of identifying inconsistencies. Experiments on our new datasets show that these methods effectively reveal inconsistencies in data, though there is further scope for improvement.","tags":null,"title":"Inconsistencies in Crowdsourced Slot-Filling Annotations: A Typology and Identification Methods","type":"publication"},{"authors":["Youxuan Jiang","Huaiyu Zhu","Jonathan K. Kummerfeld","Yunyao Li","Walter Lasecki"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"ce8a038e2e9d27323596e64ac75ba7fa","permalink":"https://www.jkk.name/publication/emnlp-findings20srl/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/emnlp-findings20srl/","section":"publication","summary":"Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95% accuracy for predicate labels and 93% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56% to 14% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.","tags":null,"title":"A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels","type":"publication"},{"authors":["Charles Welch","Jonathan K. Kummerfeld","Verónica Pérez-Rosas","Rada Mihalcea"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"3a5ffa49c2ed22d5d5e8487885189eec","permalink":"https://www.jkk.name/publication/emnlp20demographics/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/emnlp20demographics/","section":"publication","summary":"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.","tags":null,"title":"Compositional Demographic Word Embeddings","type":"publication"},{"authors":["Charles Welch","Rada Mihalcea","Jonathan K. Kummerfeld"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"2718c47cf6e9f6f31b290f88ab841e7c","permalink":"https://www.jkk.name/publication/emnlp20lm/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/emnlp20lm/","section":"publication","summary":"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.","tags":null,"title":"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation","type":"publication"},{"authors":["Stefan Larson","Anthony Zheng","Anish Mahendran","Rishi Tekriwal","Adrian Cheung","Eric Guldan","Kevin Leach","Jonathan K. Kummerfeld"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"8d0cb48f1dd67687aa81a0c278cc1df4","permalink":"https://www.jkk.name/publication/emnlp20taboo/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/emnlp20taboo/","section":"publication","summary":"Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our method, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard models. Finally, we show that our approach is complementary to recent work on improving data diversity, and training on data collected with our approach leads to more robust models.","tags":null,"title":"Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness","type":"publication"},{"authors":[],"categories":[],"content":"When we crowdsource data for tasks like SRL and sentiment analysis we only care about accuracy. For tasks where workers write new content, such as paraphrasing and creating questions, we also care about data diversity. If our data is not diverse then models trained on it will not be robust in the real world. The core idea of this paper is to encourage creativity by constraining workers.\nWe use three steps:\n Collect some data. Create a taboo list of words / phrases based on the data collected. Return to step 1, but tell workers they can\u0026rsquo;t use things in the taboo list.  We explored this idea for task-oriented dialogue. We identified taboo words using an SVM with a bag-of-words to identify common words associated with specific intents or slot values. Depending on the taboo word, we got quite different paraphrases. For example, for the sentence \u0026ldquo;What is the capital of Florida?\u0026rdquo; we collected paraphrases with various taboo words:\n   Taboo Word Paraphrases      what city is the state capital of florida   florida what is the capital of the sunshine state   capital where is the seat of government in florida   what tell me the name of florida\u0026rsquo;s capital    These examples show interesting variations, but to see if the variations are significant we tried collecting new test sets for five intent classification datasets and four slot filling datasets. With just two taboo words, a BERT based model trained on the original dataset did considerably worse on our new data. The drop varied from 2 to 33 points, with a median of 9. This indicates that we are capturing ways of expressing these intents that are not well covered by the original data.\nAddressing this issue is simple - train on data collected with our method! Interestingly, this approach is complementary to the outlier-based approach from our NAACL 2019 paper. Examples collected using one approach are hard for models trained on data from the other. Fortunately, training with data from a mix of the two leads to strong results on both.\nI\u0026rsquo;m particularly excited about this work because the general idea could be applied in so many ways:\n Change the task. Vary the type of taboo item (e.g. phrases instead of words). Try other ways of selecting taboo items. Use a different mapping from taboo items to tasks.  In fact, more specific versions of this idea have already been used. Luis von Ahn\u0026rsquo;s ESP game for image captioning used a taboo list. Each image had a list of complete labels previously assigned to the image. New labels could not match an existing label. That work predates this paper by 16 years, but hasn\u0026rsquo;t had much traction in the NLP community, possibly because of the limitation of requiring complete matches on labels (making it impractical for sentences or even phrases). I\u0026rsquo;m hopeful that our more general version will be useful in a range of crowdsourcing efforts.\nCitation  Paper\n@InProceedings{emnlp20taboo, title = {Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness}, author = {Larson, Stefan and Zheng, Anthony and Mahendran, Anish and Tekriwal, Rishi and Cheung, Adrian and Guldan, Eric and Leach, Kevin and Kummerfeld, Jonathan K.}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2020}, location = {Online}, url = {https://www.jkk.name/pub/emnlp20taboo.pdf}, }  ","date":1602516493,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602516493,"objectID":"d965f687ddf8d63d0086cf0fd5b9e55e","permalink":"https://www.jkk.name/post/2020-10-12_taboo/","publishdate":"2020-10-12T10:28:13-05:00","relpermalink":"/post/2020-10-12_taboo/","section":"post","summary":"When we crowdsource data for tasks like SRL and sentiment analysis we only care about accuracy.  For tasks where workers write new content, such as paraphrasing and creating questions, we also care about data diversity.  If our data is not diverse then models trained on it will not be robust in the real world.  The core idea of this paper is to encourage creativity by constraining workers.","tags":["crowdsourcing","dialogue"],"title":"Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness (Larson, et al., EMNLP 2020)","type":"post"},{"authors":[],"categories":[],"content":"Most work in NLP uses datasets with a diverse set of speakers. In practise, everyone speaks / writes slightly differently and our models would be better if they accounted for that. This has been the motivation for a line of work by Charlie Welch that I\u0026rsquo;ve been a collaborator on (in CICLing 2019, IEEE Intelligent Systems 2019, CoLing 2020, and this paper).\nHere, the question is how to improve language modeling for a new user of a service who voluntarily provided some demographic information, but you have no other data for. Our solution is a language model that (1) has a separate word embedding space for each individual demographic value, and (2) forms a word embedding for a given user by composing the embeddings for their demographics. In experiments on Reddit, this leads to improvements in performance for all demographic groups.\nIn the process, we also developed a way to extract demographics of Reddit users. Prior work has either inferred demographics or looked at flairs (labels in user profiles). We use self-reported information in posts, such as \u0026ldquo;I am a [blah]\u0026rdquo;. We use simple regular expressions, which are enough to get two or more demographic values for 61,000 users. There is also relatively little overlap with a flair based method (less than 0.5% of ours are in a set based on flairs).\nIt is important to note that a range of ethical issues exist around the use of demographics in machine learning. We discuss a range of issues in the paper, but I also wanted to mention a few here. First, to collect our data, we identified self-reported demographics in Reddit text. This avoids some of the problems with inferring demographics, but it does mean our sample is biased (it only contains people who wish to publicly share demographics online). Second, we must consider how our work may be used. There is a potential positive (improved performance for specific groups), but also the risk that in order to use our ideas developers require users to disclose information or try to infer it automatically. Third, there is the risk that our work is interpreted as implying that how someone speaks is a consequence of their demographics. For more detailed discussion of these and other issues see the \u0026ldquo;Limitations and Ethical Considerations\u0026rdquo; section of the paper.\nCitation  Paper\n@InProceedings{emnlp20demographics, title = {Compositional Demographic Word Embeddings}, author = {Welch, Charles and Kummerfeld, Jonathan K. and P{\\'e}rez-Rosas, Ver{\\'o}nica and Mihalcea, Rada}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2020}, location = {Online}, url = {https://arxiv.org/pdf/2010.02986.pdf}, abstract = {Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.}, }  ","date":1602379511,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602379511,"objectID":"4b75464f0e1af93881aa3c50345656eb","permalink":"https://www.jkk.name/post/2020-10-10_demographicembeddings/","publishdate":"2020-10-10T20:25:11-05:00","relpermalink":"/post/2020-10-10_demographicembeddings/","section":"post","summary":"Most work in NLP uses datasets with a diverse set of speakers. In practise, everyone speaks / writes slightly differently and our models would be better if they accounted for that. This has been the motivation for a line of work by [Charlie Welch](http://cfwelch.com/) that I've been a collaborator on (in [CICLing 2019](https://www.jkk.name/publication/cicling19personal), [IEEE Intelligent Systems 2019](https://www.jkk.name/publication/ieee19personal/), [CoLing 2020](https://www.jkk.name/publication/coling20personal/), and this paper).","tags":["paper","word-embeddings","language-model"],"title":"Compositional Demographic Word Embeddings (Welch et al., EMNLP 2020)","type":"post"},{"authors":[],"categories":[],"content":"My previous post discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions. This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.\nThe core new idea is a filtering process in which workers identify incorrect answers for a task. This is the first step of a three stage process:\n Five workers iteratively filter the options for a label (either for a predicate or argument) until there are only three. Five workers select the correct answer. If the workers disagree or any of them indicates uncertainty, ask an expert to annotate the example.  To make this work, we use an automatic system for identifying spans for predicates and arguments. This is better than going straight to the second step because it makes the set of labels less overwhelming, focusing effort on the subtle distinctions between the options.\nThis mixture of crowd and expert effort achieves high accuracy (94%) while only having 12% of examples annotated by experts. The cost is about 52 cents per label for crowd work plus the cost of the expert.\nIt\u0026rsquo;s a little tricky to compare the cost with prior work. Comparing to other work on SRL, we spend more on the crowd, but less on experts. Whether that trade-off is worth it will depend on the cost of experts. In practise, our experts are often members of the research team and so their time is a stronger constraint than the crowdsourcing budget. In that case, our approach comes out ahead as we can get more data annotated per unit of expert effort (by a factor of four). The QA-SRL work is quite a bit cheaper, at 54 cents per predicate with 2.9 roles on average (which would be ~$2 + expert effort for our approach), but the type of annotations collected are quite different, with ours providing labels from the sense inventory in Propbank.\nI see a range of interesting potential improvements for future work. First, bringing in methods of worker training in order to improve their accuracy and so reduce the need for duplicate effort. Second, combining with ideas from other work, such as having a model deciding whether examples are easy or hard and changing how they are processed accordingly, or using QA-SRL annotation to inform the process.\nCitation  Paper\n My Tweet\n@InProceedings{emnlp-findings20srl, title = {A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels}, author = {Jiang, Youxuan and Zhu, Huaiyu and Kummerfeld, Jonathan K. and Li, Yunyao and Lasecki, Walter}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, shortvenue = {Findings of EMNLP}, month = {November}, year = {2020}, location = {Online}, url = {https://www.jkk.name/pub/emnlp-findings20srl.pdf}, abstract = {Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95\\% accuracy for predicate labels and 93\\% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56\\% to 14\\% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.}, }  ","date":1601842212,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601842212,"objectID":"acce8ad11998ce6e70a527e46a61e445","permalink":"https://www.jkk.name/post/2020-10-04_crowdsrl/","publishdate":"2020-10-04T15:10:12-05:00","relpermalink":"/post/2020-10-04_crowdsrl/","section":"post","summary":"My [previous post](https://www.jkk.name/post/2020-09-25_crowdqasrl/) discussed work on crowdsourcing QA-SRL, a way of capturing semantic roles in text by asking workers to answer questions. This post covers a paper I contributed to that also considers crowdsourcing SRL, but collects the more traditional form of annotation used in resources like Propbank.","tags":["paper","annotation","crowdsourcing","srl"],"title":"A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels (Jiang, et al., Findings of EMNLP 2020)","type":"post"},{"authors":["Jonathan K. Kummerfeld"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"fa65e16957efc60154e9640333cbfcbc","permalink":"https://www.jkk.name/publication/hcomp20fair/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/hcomp20fair/","section":"publication","summary":"Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S.~federal minimum wage. Meanwhile, research on collecting high quality annotations (e.g.~for Natural Language Processing) suggests using qualifications such as a minimum number of previously completed tasks. If most requesters who pay fairly use this kind of minimum qualification, then workers may be forced to complete a substantial amount of poorly paid work for other requesters before they can earn a fair wage. This paper (1) explores current conventions for the threshold, (2) discusses possible alternatives, and (3) presents a study of correlation between approved work and work quality.","tags":null,"title":"Qualification Labour: A Fair Wage Isn't Enough if Workers Need to Do 5,000 Low Paid Tasks to Qualify for Your Task","type":"publication"},{"authors":[],"categories":[],"content":"This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 10 million+ words of text, but only 1 GPU for training?\nThe impact of tying, freezing, and pretraining It is standard practise to tie the input and output embeddings of language models (i.e., use the same weights in both places), training them together and initialising them randomly. Several papers have shown that this improves results by providing more frequent updates to the input embeddings. But if you have data available for pretraining it is less clear that this is the right approach. To explore this I\u0026rsquo;m going to use a few symbols:\nHere are the results of training an AWD-LSTM with all variations of these parameters, evaluated on the standard LM development set of the PTB (Std) and a variation that has actual words instead of unk (Rare):\nLight blue shows the standard configuration and light red shows our proposal. The table is ranked by performance on Std and has four clear sections:\n(a) Frozen random output embeddings.\n(b) Frozen pretrained output embeddings.\n(c) Frozen random input embeddings.\n(d) Various configurations.\nI was surprised by the dramatic difference between input and output embeddings here. Freezing the output embeddings, even with a good embedding space, leads to terrible performance. In contrast, freezing input embeddings is fine if they are pretrained, and has a far smaller impact when they are random.\nEvaluating with rare words, the big picture is mostly the same, but pretraining has a bigger impact. One interesting difference is that the top five models all use pretrained input embeddings, with a large gap from there to the next results. At the same time, pretraining the output embeddings seems to have only a small impact (when holding all other variables fixed). Finally, the best results freeze the input embeddings. Our explanation is that embeddings become inconsistent when they aren\u0026rsquo;t frozen. The vectors for words in the training set are moved but the ones seen only in pretraining stay where they are, leading to an inconsistent embedding space.\nThe paper then goes through a series of experiments to explore this, varying data domain, similarity of pretraining data, and more. Here I\u0026rsquo;m going to jump straight to the final results. The table below considers a dataset with 43 million in-domain tokens for pretraining and 7 million for LM training. The other models are the standard AWD-LSTM, an n-gram language model, and two version of GPT-2 (without finetuning):\nFor word level prediction perplexity is reduced by 4. However, if we train and test with BPE there is no improvement (see the SHA-RNN paper for some issues with comparing BPE and word evaluation). So if your application works with BPE this finding isn\u0026rsquo;t useful, but for word-level modeling it probably is.\nA few notes about this work:\n A natural next step would be to explore ways to train the language model with more data. Modifying the AWD-LSTM code to support training sets larger than GPU memory could render pretraining unnecessary (though at the cost of much longer training). In some experiments (not in the paper), we found that when the pretraining set and training set were the same, pretraining didn\u0026rsquo;t improve performance, but it did speed up training. Properties of evaluation datasets have shaped the direction of work on language modeling. It\u0026rsquo;s important to think beyond the hyperparameters that are easy to vary (e.g., hidden vector dimensions) when adapting a model for a new scenario. Writing robust research code is hard. We tried getting several other models to run with our variations, but going beyond reproducing results to actually modifying code proved hard. Even for the AWD-LSTM, we failed to reproduce results except when we went back to one of the earliest releases. This paper was saved by author response. The initial reviews were 3.5, 2.5, 3.5 and based on the response and reviewer discussion the 2.5 went to a 4. The response contained answers to reviewer questions, including a bunch of statistics about the data that are now in the final paper. I have always been a fan of author response. It can lead to more informed acceptance decisions and more useful feedback to authors. To achieve that, both authors and reviewers need to engage with it though. In particular, reviewers need to give something of substance to be responded to and they need to carefully read and consider the response.  Citation  Paper\n@InProceedings{emnlp20lm, title = {Improving Low Compute Language Modeling with In-Domain Embedding Initialisation}, author = {Welch, Charles and Mihalcea, Rada and Kummerfeld, Jonathan K.}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2020}, url = {https://www.jkk.name/pub/emnlp20lm.pdf}, abstract = {Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.}, }  Tables in written form Table with training variations Each section is presented separately below, with the model described using five words followed by the result on the standard data and the result on the data with rare words.\nFirst section:\n tied frozen dice frozen dice, 680, 1120 untied frozen dice frozen dice, 680, 1120 untied unfrozen dice frozen dice, 680, 431 untied unfrozen train frozen dice, 220, 372 untied frozen train frozen dice, 218, 360  Second section:\n untied frozen dice frozen train, 121, 202 untied unfrozen dice frozen train, 95.0, 170 untied unfrozen train frozen train, 91.3, 147 tied frozen train frozen train, 90.7, 136 untied frozen train frozen train, 90.7, 136  Third section:\n untied frozen dice unfrozen dice, 82.2, 143 untied frozen dice unfrozen train, 81.4, 142  Fourth section:\n untied unfrozen dice unfrozen dice, 65.3, 120 untied unfrozen dice unfrozen train, 64.1, 113 untied unfrozen train unfrozen dice, 62.5, 105 untied unfrozen train unfrozen train, 61.7, 98.5 untied frozen train unfrozen train, 61.6, 97.1 tied unfrozen dice unfrozen dice, 61.3, 112 untied frozen train unfrozen dice, 61.1, 98.1 tied unfrozen train unfrozen train, 59.8, 98.7  Final results table Models with word level evaluation, giving development results then test results:\n N-Gram, 92.3, 95.0 Baseline AWD-LSTM, 52.8, 53.5 Our approach, 49.0, 49.4  Models with BPE evaluation:\n N-Gram, 56.7, 55.3 GPT-2 (112m), 46.4, 43.8 Baseline AWD-LSTM, 37.8, 36.7 Our approach, 38.3, 37.2 GPT-2 (774m), 32.5, 33.7  Acknowledgements Dice Icon by Andrew Doane from the Noun Project. Fire and Snowflake Icons by Freepik from www.flaticon.com.\n","date":1601404704,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601404704,"objectID":"ddf8003cf66dc46c215f89b2c6994853","permalink":"https://www.jkk.name/post/2020-09-29_pretraininglm/","publishdate":"2020-09-29T13:38:24-05:00","relpermalink":"/post/2020-09-29_pretraininglm/","section":"post","summary":"This paper explores two questions. First, what is the impact of a few key design decisions for word embeddings in language models? Second, based on the first answer, how can we improve results in the situation where we have 50 million+ words of text, but only 1 GPU for training?","tags":["paper","language-model","word-embeddings"],"title":"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation (Welch, Mihalcea, and Kummerfeld, EMNLP 2020)","type":"post"},{"authors":[],"categories":[],"content":"Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments. Over the last few years, Luke Zettlemoyer\u0026rsquo;s Group has been exploring using question-answer pairs to represent this structure. This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank. However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.\nI got three main things out of this paper:\n It shifted my approach to crowdsourcing to consider workers more like traditional expert annotators. It reinforced the idea that small shifts in crowd workflows can have a major impact on annotation quality. QA-SRL can capture roles not covered by PropBank.  The work also provides a new dataset that will be useful for future work on this problem, and useful benchmarks of systems and measurements of data quality. Expanding on the three points above:\nCrowd workers: The paper argues in favour of putting more time into training workers. Most of the work I\u0026rsquo;ve seen in NLP for crowdsourcing (including my own) focuses on modifying task design or using ML post-processing to improve results. Here, they run a large-scale qualification task and filter workers based on their performance, then train those workers by paying them to read a set of instructions (23 text-dense slides) and do two small annotation rounds with feedback after each one. This increases the upfront cost, but reduces the cost of annotation by reducing the need for multiple annotations of each item. The paper doesn\u0026rsquo;t provide quite enough detail to quantify the cost. We do know that to get to 11 workers they needed to train 30 workers at a cost of 2 hours each plus 30 minutes of researcher time each. If we assume 60 workers did the preliminary round, each taking 5 minutes, and that workers cost $12 / hour ($10 to the workers, $2 to Amazon), that\u0026rsquo;s almost $800 plus 15 hours of researcher time. For a large annotation effort, the savings during annotation will make that worth it (or, as in this case, it will lead to higher quality data). I am curious which aspect was more important though - filtering the pool of workers, or training workers.\nWorkflow impact: In previous QA-SRL work, one worker wrote a question and its answers and two workers checked the question and independently added answers. Here, two workers independently write a question+answer and a third work consolidates the annotations into a final annotation. The cost for a label is about the same (54c / predicate vs. 51c / predicate), but coverage is considerably higher. The design space for crowd workflows is huge and this is another example of how important it is to explore. It\u0026rsquo;s also possible that the changes in recruitment and training were more critical than the workflow shift, but the study didn\u0026rsquo;t include evaluation with only one or the other.\nQA-SRL vs. PropBank: This may be less surprising to someone who works more on SRL, but they found their approach captured many implicit roles that PropBank does not. Specifically, of 100 annotated arguments that were not in PropBank, 68 were valid implicit arguments. I\u0026rsquo;m curious about what those implicit arguments are capturing. Maybe targeted re-annotation could be used to add them to PropBank (identifying relevant sentences by trace parsing).\nCitation  Paper\n Code\n My Tweet\n@inproceedings{roit-etal-2020-controlled, title = \u0026quot;Controlled Crowdsourcing for High-Quality {QA}-{SRL} Annotation\u0026quot;, author = \u0026quot;Roit, Paul and Klein, Ayal and Stepanov, Daniela and Mamou, Jonathan and Michael, Julian and Stanovsky, Gabriel and Zettlemoyer, Luke and Dagan, Ido\u0026quot;, booktitle = \u0026quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\u0026quot;, month = \u0026quot;jul\u0026quot;, year = \u0026quot;2020\u0026quot;, address = \u0026quot;Online\u0026quot;, publisher = \u0026quot;Association for Computational Linguistics\u0026quot;, url = \u0026quot;https://aclanthology.org/2020.acl-main.626\u0026quot;, doi = \u0026quot;10.18653/v1/2020.acl-main.626\u0026quot;, pages = \u0026quot;7008--7013\u0026quot;, abstract = \u0026quot;Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.\u0026quot;, }  ","date":1601047038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601047038,"objectID":"5e8dba4cd28619aa16ed530d5018af24","permalink":"https://www.jkk.name/post/2020-09-25_crowdqasrl/","publishdate":"2020-09-25T10:17:18-05:00","relpermalink":"/post/2020-09-25_crowdqasrl/","section":"post","summary":"Semantic Role Labeling captures the content of a sentence by labeling the word sense of the verbs and identifying their arguments.  Over the last few years, [Luke Zettlemoyer's Group](https://www.cs.washington.edu/people/faculty/lsz/) has been exploring using question-answer pairs to represent this structure.  This approach has the big advantage that it is easier to explain than the sense inventory and role types of more traditional SRL resources like PropBank.  However, even with that advantage, crowdsourcing this annotation is difficult, as this paper shows.","tags":["srl","crowdsourcing","paper","data"],"title":"Controlled Crowdsourcing for High-Quality QA-SRL Annotation (Roit, et al., ACL 2020)","type":"post"},{"authors":[],"categories":[],"content":"Training models requires massive amounts of labeled data. We usually sample data iid from the target domain (e.g. newspapers), but it seems intuitive that this means we wast effort labeling samples that are obvious or easy and so not informative during training. Active Learning follows that intuition, labeling data incrementally, selecting the next example(s) to label based on what a model considers uncertain. Lots of work has shown this can be effective for that model, but if the labeled dataset is then used to train another model will it also do well?\nFor text classification this paper finds the answer is no: training model X on iid samples is as good or better than training on samples collected while active learning with model Y. They show this through experiments with four datasets and three models, training on up to 25% of the available data. For named entity recognition the story is different in my opinion - iid is consistently slightly worse, though the gains from active learning are small in all cases (0 to 0.6 point gain for the better model, 0.4 to 1.7 for the weaker model). One caveat is that these models are not state-of-the-art. For CoNLL 2003 NER, many models score around 93, but these models are getting 70-90. On OntoNotes, the best results are close to 90, but these models get 74-85. This is still an interesting result, but I\u0026rsquo;m left with a few questions:\n This work focused on a low data scenario. What if I have a lot of data? It may be that sampling iid and active learning based samples were similar here because either way the data was capturing the core phenomena. The challenge here is that you can\u0026rsquo;t run this experiment easily with an existing dataset (unless it is truly massive). How does the sampled data differ from iid data? Is there a significant shift in the distribution of class types? What about using a hybrid approach, with some data sampled iid and other data sampled randomly?  Overall, what I take away from this work is that active learning may not be the right choice for building a small dataset in NLP. For large datasets, building models, or other tasks and domains the conclusions are less clear, though it is certainly worth being aware of the risk that a dataset made with active learning may not be equally useful to all models.\nCitation  Paper\nTwitter discussion in 2018) and 2019.\n@inproceedings{lowell-etal-2019-practical, title = \u0026quot;Practical Obstacles to Deploying Active Learning\u0026quot;, author = \u0026quot;Lowell, David and Lipton, Zachary C. and Wallace, Byron C.\u0026quot;, booktitle = \u0026quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\u0026quot;, month = \u0026quot;nov\u0026quot;, year = \u0026quot;2019\u0026quot;, address = \u0026quot;Hong Kong, China\u0026quot;, publisher = \u0026quot;Association for Computational Linguistics\u0026quot;, url = \u0026quot;https://aclanthology.org/D19-1003\u0026quot;, doi = \u0026quot;10.18653/v1/D19-1003\u0026quot;, pages = \u0026quot;21--30\u0026quot;, abstract = \u0026quot;Active learning (AL) is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. In AL, one iteratively selects training examples for annotation, often those for which the current model is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice, one does not have the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.\u0026quot;, }  ","date":1600373315,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600373315,"objectID":"44cd230f4f62460604c2b471bcf5320c","permalink":"https://www.jkk.name/post/2020-09-17_activelearningbrittle/","publishdate":"2020-09-17T15:08:35-05:00","relpermalink":"/post/2020-09-17_activelearningbrittle/","section":"post","summary":"Training models requires massive amounts of labeled data.  We usually sample data iid from the target domain (e.g. newspapers), but it seems intuitive that this means we wast effort labeling samples that are obvious or easy and so not informative during training.  Active Learning follows that intuition, labeling data incrementally, selecting the next example(s) to label based on what a model considers uncertain.  Lots of work has shown this can be effective for that model, but if the labeled dataset is then used to train another model will it also do well?","tags":["paper","annotation"],"title":"Practical Obstacles to Deploying Active Learning (Lowell, et al., EMNLP 2019)","type":"post"},{"authors":[],"categories":[],"content":"Natural language interfaces to computer systems are an exciting area with new workshops ( WNLI at ACL and IntEx-SemPar at EMNLP), a range of datasets (including my own work on text-to-SQL), and many papers. Most work focuses on either (1) commands for simple APIs, (2) generating a database query, or (3) generating general purpose code. This paper considers an interesting application: interaction with data visualisation tools.\nUsing the full flexibility of these tools is a tall order, so this work focuses on commands to modify style parameters of a figure. For that setting, the problem can be framed as task-oriented dialogue in which each style parameter (e.g. x-axis font size) is a slot that needs to be defined. Using this framing of the problem, the paper presents a new dataset of 3,200 conversations in which a person modifies the style of a plot. These were collected on Mechanical Turk by having one worker describe a target plot and another worker manipulating values for parameters to match it. There are 12 plot types with 3-13 properties, with the target plot randomly generated. Baseline approaches do fairly well, but far short of a human (either another worker or one of the authors).\nIt\u0026rsquo;s a large resource with high agreement between annotators and the paper presents detailed analysis and helpful examples. One experiment I\u0026rsquo;d be curious to see is results with a fixed number of training examples per plot type (or per slot type). Histograms and scatter plots appear particularly difficult in the breakdown of results by plot type, but they are also the types with the fewest examples (a tenth as many as the type with the most).\nI find this general topic exciting because it brings together several areas of NLP and it seems feasible to create a useful system in the near future. Hopefully there will be progress on models for this dataset and development of additional resources. In particular, there was a decision here to limit generation to slot-values, which is powerful, but does not capture the full flexibility of matplotlib (at least not without further work on representing more features this way). Arbitrary code generation would be a fantastic extension, though creating the data would require some creativity as the approach used here wouldn\u0026rsquo;t directly work.\nCitation  Paper\n@inproceedings{shao-nakashole-2020-chartdialogs, title = \u0026quot;{C}hart{D}ialogs: {P}lotting from {N}atural {L}anguage {I}nstructions\u0026quot;, author = \u0026quot;Shao, Yutong and Nakashole, Ndapa\u0026quot;, booktitle = \u0026quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\u0026quot;, month = \u0026quot;jul\u0026quot;, year = \u0026quot;2020\u0026quot;, address = \u0026quot;Online\u0026quot;, publisher = \u0026quot;Association for Computational Linguistics\u0026quot;, url = \u0026quot;https://aclanthology.org/2020.acl-main.328\u0026quot;, doi = \u0026quot;10.18653/v1/2020.acl-main.328\u0026quot;, pages = \u0026quot;3559--3574\u0026quot;, abstract = \u0026quot;This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61{\\%} plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.\u0026quot;, }  ","date":1599507694,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599507694,"objectID":"b37b9c76dd2b13c047b1a1fb6674b987","permalink":"https://www.jkk.name/post/2020-09-07_chartdialogs/","publishdate":"2020-09-07T14:41:34-05:00","relpermalink":"/post/2020-09-07_chartdialogs/","section":"post","summary":"Natural language interfaces to computer systems are an exciting area with new workshops ([WNLI](https://aclanthology.org/volumes/2020.nli-1/) at ACL and [IntEx-SemPar](https://intex-sempar.github.io/) at EMNLP), a range of datasets (including my own work on [text-to-SQL](/publication/acl18sql/)), and many papers. Most work focuses on either (1) commands for simple APIs, (2) generating a database query, or (3) generating general purpose code. This paper considers an interesting application: interaction with data visualisation tools.","tags":["paper","dialogue","data","grounded-language","nli"],"title":"ChartDialogs: Plotting from Natural Language Instructions (Shao and Nakashole, ACL 2020)","type":"post"},{"authors":[],"categories":[],"content":"It is difficult to predict how well a model will work in the real world. Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to. This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories. Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.\nThe paper organises these tests along two axes. One is the type of test:\n Invariance: Giving the same answer when changes are made that should not impact the model prediction. Directional: Giving an answer that differs in a way that matches the intended impact of a change. Minimum Function Tests: A range of other tests that consider specific cases.  The other axis is the linguistic property being varied:\n Vocabulary Change Named Entity Variation Temporal Shift Negation Semantic Role Swap Various Other Changes  For example, an invariance test on vocabulary would be that replacing words with their synonyms should not change the result.\nThe paper tests the idea on (1) sentiment analysis on SST-2, (2) identifying matching questions on QQP, and (3) machien comprehension on SQuAD. Researchers / developers using the method are more effective at finding issues than those asked to write tests without this framework to approach the problem.\nUnderstanding system errors has been an interest of mine for a long time now (back to my 2012 parsing paper) and from my experience with startups it is definitely challenging to develop effective tests for NLP models. I\u0026rsquo;m curious to see how this approach works out when used iteratively. When users modify their model or data to address the problems do they actually fix them or just overfit to the new set of tests? Another open question is how to apply these to problems with more structured output (e.g. text-to-SQL). Some would easily apply, e.g. invariance tests, while others would be more difficult.\nCitation  Paper\n@inproceedings{ribeiro-etal-2020-beyond, title = \u0026quot;Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist\u0026quot;, author = \u0026quot;Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer\u0026quot;, booktitle = \u0026quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\u0026quot;, month = \u0026quot;jul\u0026quot;, year = \u0026quot;2020\u0026quot;, address = \u0026quot;Online\u0026quot;, publisher = \u0026quot;Association for Computational Linguistics\u0026quot;, url = \u0026quot;https://aclanthology.org/2020.acl-main.442\u0026quot;, doi = \u0026quot;10.18653/v1/2020.acl-main.442\u0026quot;, pages = \u0026quot;4902--4912\u0026quot;, abstract = \u0026quot;Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.\u0026quot;, }  ","date":1599162269,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599162269,"objectID":"1aa88f22cbc5283d7051d19de01b0fbe","permalink":"https://www.jkk.name/post/2020-09-03_checklist/","publishdate":"2020-09-03T14:44:29-05:00","relpermalink":"/post/2020-09-03_checklist/","section":"post","summary":"It is difficult to predict how well a model will work in the real world. Carefully curated test sets provide some signal, but only if they are large, representative, and have not been overfit to. This paper builds on two ideas for this problem: constructing challenge datasets and breaking performance down into subcategories. Together, these become a process of designing specific tests that measure how well a model handles certain types of variation in data.","tags":["paper","analysis","data"],"title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Ribeiro, et al., ACL 2020 Best Paper)","type":"post"},{"authors":null,"categories":null,"content":"","date":1592870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592870400,"objectID":"635a6bf78ca9056413fef475342308cf","permalink":"https://www.jkk.name/data/dstc7/","publishdate":"2020-06-23T00:00:00Z","relpermalink":"/data/dstc7/","section":"data","summary":"Data from Noetic End-to-End Response Selection Challenge. Dialogue from Ubuntu tech support and Michigan course advising.","tags":["dialogue"],"title":"DSTC 7 track 1: Next Utterance Selection","type":"data"},{"authors":null,"categories":null,"content":"","date":1592870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592870400,"objectID":"12732c3912917367116aa1d1cf051620","permalink":"https://www.jkk.name/data/dstc8/","publishdate":"2020-06-23T00:00:00Z","relpermalink":"/data/dstc8/","section":"data","summary":"Data from NOESIS II: Predicting Responses, Identifying Success, and Managing Complexity in Task-Oriented Dialogue. Dialogue from Ubuntu tech support and Michigan course advising.","tags":["dialogue"],"title":"DSTC 8 track 2: Next Utterance Selection","type":"data"},{"authors":null,"categories":null,"content":"","date":1585602026,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585602026,"objectID":"3a1a901b54bd0e99eea96d22130816fe","permalink":"https://www.jkk.name/software/colab-spanbert/","publishdate":"2020-03-30T16:00:26-05:00","relpermalink":"/software/colab-spanbert/","section":"software","summary":"A notebook that (1) sets up the SpanBERT code and model, and (2) runs inference on text you provide.","tags":["colab"],"title":"Colaboratoy Notebook for Coreference Resolution with SpanBERT","type":"software"},{"authors":null,"categories":[],"content":"Keeping up with research is hard. I\u0026rsquo;ve previously made lists of papers I wanted to read, and then only gotten to a small fraction of them. Simply resolving to read more papers hasn\u0026rsquo;t worked for me.\nI\u0026rsquo;m trying out a new approach. The goals are (1) read less of more papers, and (2) read more papers that are critical to my work. Sometimes just the introduction or abstract is enough for me to get the ideas I need from the paper. I want to read the whole paper only if it is really relevant to me. The problem is that it\u0026rsquo;s easy to start reading a paper and then just keep going, and without a process it can be easy to put off starting at all. This is the process I\u0026rsquo;ve worked out (in Chrome) to do this:\n Go through the proceedings for a conference on the ACL anthology and read every title. Based on the title, decide whether to read the abstract. Based on the abstract, decide whether to read the introduction, in which case open the paper in a tab. Bookmark all tabs. Either use Shift+Command+D or Bookmarks -\u0026gt; Bookmark All Tabs. Export the folder of bookmarks to a file. To do this, go to chrome://bookmarks, select the new folder then use the menu on the far right of the blue bar to select Export Bookmarks. Run the code below, with bookmarks_DATE.html as input (note, requires PyPDF2). This produces a pdf with only the introduction of each paper (approximately). Read through the pdf this produces and flag the papers to read all of.  This will hopefully produce a list that is short enough to read all of (and maybe write blog posts about!).\n# Get the paper URLs import sys papers = {} for line in sys.stdin: if 'aclanthology.org' in line: content = line.strip() url = content.split()[1].split('\u0026quot;')[1][:-1] + \u0026quot;.pdf\u0026quot; name = content.split(\u0026quot; - ACL Anthology\u0026quot;)[0].split(\u0026quot;\u0026gt;\u0026quot;)[-1] papers[name] = url # Download the papers import io, requests PDFs = {} for name, url in papers.items(): r = requests.get(url, auth=('usrname', 'password'), verify=False,stream=True) assert 200 \u0026lt;= r.status_code \u0026lt; 400 r.raw.decode_content = True PDFs[name] = io.BytesIO(r.content) # Get the Introductions from PyPDF2 import PdfFileReader, PdfFileWriter import string pdf_writer = PdfFileWriter() for name, raw_pdf in PDFs.items(): pdf = PdfFileReader(raw_pdf) page0 = pdf.getPage(0) pdf_writer.addPage(page0) text = page0.extractText().split('\\n') done = False for part in text: # Try to find the start of section 2 if part.startswith('2') and len(part) \u0026gt; 1: if part[1] in string.ascii_letters: done = True if not done: page1 = pdf.getPage(1) start = page1.extractText().split('\\n')[0] # Try to find the start of section 2 if start.startswith('2') and len(start) \u0026gt; 1: if start[1] in string.ascii_letters: done = True if not done: pdf_writer.addPage(page1) with open('example.pdf', 'wb') as out: pdf_writer.write(out)  ","date":1580072946,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580072946,"objectID":"02e401bbb217655abff39ca0481c7fee","permalink":"https://www.jkk.name/post/paper-reading/","publishdate":"2020-01-26T16:09:06-05:00","relpermalink":"/post/paper-reading/","section":"post","summary":"Keeping up with research is hard. I\u0026rsquo;ve previously made lists of papers I wanted to read, and then only gotten to a small fraction of them. Simply resolving to read more papers hasn\u0026rsquo;t worked for me.\nI\u0026rsquo;m trying out a new approach. The goals are (1) read less of more papers, and (2) read more papers that are critical to my work.","tags":["advice"],"title":"Paper Reading","type":"post"},{"authors":["Laura Burdick","Jonathan K. Kummerfeld","Rada Mihalcea"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"638cf7eab032188b5a88d69862eb022d","permalink":"https://www.jkk.name/publication/arxiv20embeddings/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/arxiv20embeddings/","section":"publication","summary":"Word embeddings are powerful representations that form the foundation of many natural language processing architectures and tasks, both in English and in other languages. To gain further insight into word embeddings in multiple languages, we explore their stability, defined as the overlap between the nearest neighbors of a word in different embedding spaces. We discuss linguistic properties that are related to stability, drawing out insights about how morphological and other features relate to stability. This has implications for the usage of embeddings, particularly in research that uses embeddings to study language trends.","tags":null,"title":"Analyzing the Surprising Variability in Word Embedding Stability Across Languages","type":"publication"},{"authors":["Jordan S. Huffaker","Jonathan K. Kummerfeld","Walter S. Lasecki","Mark S. Ackerman"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"73cad420ce5c304e1c9f8a1c829d9321","permalink":"https://www.jkk.name/publication/chi20anchor/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/chi20anchor/","section":"publication","summary":"Detecting rhetoric that manipulates readers’ emotions requires distinguishing intrinsically emotional content (IEC; e.g., a parent losing a child) from emotionally manipulative language (EML; e.g., using fear-inducing language to spread anti-vaccine propaganda). However, this remains an open classifcation challenge for both automatic and crowdsourcing approaches. Machine Learning approaches only work in narrow domains where labeled training data is available, and non-expert annotators tend to confate IEC with EML. We introduce an approach, anchor comparison, that leverages workers’ ability to identify and remove instances of EML in text to create a paraphrased 'anchor text', which is then used as a comparison point to classify EML in the original content. We evaluate our approach with a dataset of news-style text snippets and show that precision and recall can be tuned for system builders’ needs. Our contribution is a crowdsourcing approach that enables non-expert disentanglement of social references from content.","tags":null,"title":"Crowdsourced Detection of Emotionally Manipulative Language","type":"publication"},{"authors":["Chulaka Gunasekara","Jonathan K. Kummerfeld","Luis Lastras","Walter S. Lasecki"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fb4bab325bef4820f077313684c4ed13","permalink":"https://www.jkk.name/publication/ws-aaai-dstc20task2/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/ws-aaai-dstc20task2/","section":"publication","summary":"Real-world conversation often involves more than two participants and complex conversation structures, but most datasets for dialogue research simplify the task to make it more tractable. This shared task built on prior tasks for goal-oriented dialogue, moving towards more realistic settings. Seventeen teams participated in the primary task, predicting the next utterance in a multi-party conversation, and several teams participated in supplementary tasks. All of the datasets have been publicly released, providing a standard benchmark for future work in this space.","tags":null,"title":"NOESIS II: Predicting Responses, Identifying Success, and Managing Complexity in Task-Oriented Dialogue","type":"publication"},{"authors":["Luis Fernando D'Haro","Koichiro Yoshino","Chiori Hori","Tim K. Marks","Lazaros Polymenakos","Jonathan K. Kummerfeld","Michel Galley","Xiang Gao"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"c614db7d2d4f3275f36044d66d1cca9b","permalink":"https://www.jkk.name/publication/csl20dstc/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/csl20dstc/","section":"publication","summary":"This paper provides detailed information about the seventh Dialog System Technology Challenge (DSTC7) and its three tracks aimed to explore the problem of building robust and accurate end-to-end dialog systems. In more detail, DSTC7 focuses on developing and exploring end-to-end technologies for the following three pragmatic challenges: (1) sentence selection for multiple domains, (2) generation of informational responses grounded in external knowledge, and (3) audio visual scene-aware dialog to allow conversations with users about objects and events around them. This paper summarizes the overall setup and results of DSTC7, including detailed descriptions of the different tracks, provided datasets and annotations, overview of the submitted systems and their final results. For Track 1, LSTM-based models performed best across both datasets, allowing teams to effectively handle task variants where no correct answer was present or when multiple paraphrases were included. For Track 2, RNN-based architectures augmented to incorporate facts by using two types of encoders: a dialog encoder and a fact encoder plus using attention mechanisms and a pointer-generator approach provided the best results. Finally, for Track 3, the best model used Hierarchical Attention mechanisms to combine the text and vision information obtaining a 22% better result than the baseline LSTM system for the human rating score. More than 220 participants were registered and about 40 teams participated in the final challenge. 32 scientific papers reporting the systems submitted to DSTC7, and 3 general technical papers for dialog technologies, were presented during the one-day wrap-up workshop at AAAI-19. During the workshop, we reviewed the state-of-the-art systems, shared novel approaches to the DSTC7 tasks, and discussed the future directions for the challenge (DSTC8).","tags":null,"title":"Overview of the seventh Dialog System Technology Challenge: DSTC7","type":"publication"},{"authors":["Philip Paquette","Yuchen Lu","Steven Bocco","Max O. Smith","Satya Ortiz-Gagné","Jonathan K. Kummerfeld","Joelle Pineau","Satinder Singh","Aaron Courville"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"da7cd2ac0876b6c5a24cbb4bca497580","permalink":"https://www.jkk.name/publication/neurips19diplomacy/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/neurips19diplomacy/","section":"publication","summary":"Diplomacy is a seven-player non-stochastic, non-cooperative game, where agents acquire resources through a mix of teamwork and betrayal. Reliance on trust and coordination makes Diplomacy the first non-cooperative multi-agent benchmark for complex sequential social dilemmas in a rich environment. In this work, we focus on training an agent that learns to play the No Press version of Diplomacy where there is no dedicated communication channel between players. We present DipNet, a neural-network-based policy model for No Press Diplomacy. The model was trained on a new dataset of more than 150,000 human games. Our model is trained by supervised learning (SL) from expert trajectories, which is then used to initialize a reinforcement learning (RL) agent trained through self-play. Both the SL and RL agents demonstrate state-of-the-art No Press performance by beating popular rule-based bots.","tags":null,"title":"No-Press Diplomacy: Modeling Multi-Agent Gameplay","type":"publication"},{"authors":["Seokhwan Kim","Michel Galley","Chulaka Gunasekara","Sungjin Lee","Adam Atkinson","Baolin Peng","Hannes Schulz","Jianfeng Gao","Jinchao Li","Mahmoud Adada","Minlie Huang","Luis Lastras","Jonathan K. Kummerfeld","Walter S. Lasecki","Chiori Hori","Anoop Cherian","Tim K. Marks","Abhinav Rastogi","Xiaoxue Zang","Srinivas Sunkara","Raghav Gupta"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"b92d5a90b9cdd9bd0929b34bf1115fc0","permalink":"https://www.jkk.name/publication/ws-neurips-convai19dstc/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/ws-neurips-convai19dstc/","section":"publication","summary":"This paper introduces the Eighth Dialog System Technology Challenge. In line with recent challenges, the eighth edition focuses on applying end-to-end dialog technologies in a pragmatic way for multi-domain task-completion, noetic response selection, audio visual scene-aware dialog, and schema-guided dialog state tracking tasks. This paper describes the task definition, provided datasets, and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks.","tags":null,"title":"The Eighth Dialog System Technology Challenge","type":"publication"},{"authors":["Stefan Larson","Anish Mahendran","Joseph J. Peper","Christopher Clarke","Andrew Lee","Parker Hill","Jonathan K. Kummerfeld","Kevin Leach","Michael A. Laurenzano","Lingjia Tang","Jason Mars"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"d55b8515a3bdf99a5cb801d629cbd54a","permalink":"https://www.jkk.name/publication/emnlp19data/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/emnlp19data/","section":"publication","summary":"Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scope---i.e., queries that do not fall into any of the system's supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.","tags":null,"title":"An Evaluation for Intent Classification and Out-of-Scope Prediction","type":"publication"},{"authors":["Jordan S. Huffaker","Jonathan K. Kummerfeld","Walter S. Lasecki","Mark S. Ackerman"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"fbb6270089b550bea0dfc30f85f6a8ae","permalink":"https://www.jkk.name/publication/ws-cscw19voids/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/ws-cscw19voids/","section":"publication","summary":"","tags":null,"title":"Training Data Voids: Novel Attacks Against NLP Content Moderation","type":"publication"},{"authors":null,"categories":[],"content":"Am I getting the most out of the time I put into conferences? This year NAACL and ACL ran mentoring programs to help newer members of the community and in the process of giving advice I started to question whether my own approach to conferences was effective. Most online advice is aimed at students attending for the first time. What about a more experienced researcher? I\u0026rsquo;ve fallen into certain patterns without stepping back to think about whether they are effective and what could be better.\nDuring sessions There are four main options during a session in the main conference:\n Attend a talk. Pro: Typically the most exciting work is presented in talks. Con: Most talks are poorly presented. Even in a good talk, it is easy to sit and listen but not take it in. Attend the poster session. Pro: Easy to spend more or less time on each poster. Con: Easy to start skimming titles and figures without learning much (particularly when tired). Talk to people. Pro: A great way to (1) learn about what other people are doing / thinking about right now, (2) make connections / network, (3) make specific people aware of your own work. Con: Sometimes conversations become just small talk, which is not always worthwhile (Edit: see clarification in the postscript). It can also be hard to approach new people. Relax. Pro: Being focused and engaged is tiring, a break can be rejuvenating. Con: Missing out on work being presented.  There is no perfect fixed combination of these. The strategy I want to try next time is:\n  Over breakfast, choose 2-3 talks from each session that cover must-see work for my interests. Look through posters for the topics I have an interest in and flag must-see items. If the conference provides breakfast, eat elsewhere to do this and then show up for the last 30 minutes to chat.\n  When a session starts, if I\u0026rsquo;m in an interesting conversation, keep talking. Otherwise, go to the talks selected that morning and drop by posters in between.\n  Take notes in the conference handbook on every talk I attend and every poster I do more than read the title of.\n  This minimises the number of decisions during the day and focuses on the work I am most interested in. Of the options above, taking a break isn\u0026rsquo;t included. I\u0026rsquo;m hoping this approach (and more below) will make that unnecessary in general.\nDuring breaks At the first few conferences I went to, I was constantly meeting new people because I didn\u0026rsquo;t know anyone. Now, I tend to talk to the same people at every conference. I do want to catch up with those people, but it means I\u0026rsquo;m not making the most of the diverse group the conference brings together.\nWho should I be trying to meet? As a student, I was most interested in meeting faculty working in my area. Now, I want to look for (1) the students doing work I am excited about and (2) faculty at places I intend to apply to. My reasoning on the students is that (1) they will have more to say about their work than their advisor, (2) they are the future of the field, and (3) if there is scope for collaboration then it will be easier to get their advisor on board if they are excited than vice versa.\nHow should I try to meet people? Conferences are so big these days that simply hoping to bump into someone won\u0026rsquo;t work. One solution is to contact people ahead of time and plan to meet during a specific break. The same idea can apply to lunch. Usually I have just joined lunch groups in an ad hoc way, but having a plan for the nucleus of a group and then picking up more people on the day would be more effective.\nIn the evening I always go to the conference receptions (ie. Welcome / Social) and plan to continue. The key questions in my mind are about industry hosted events and when to go to sleep.\nI usually don\u0026rsquo;t get invited to industry events these days. One thing I could do differently is be more proactive in talking to people from the companies hosting events to tell them I am interested. When I am invited, I think it is worth going as it\u0026rsquo;s another chance to meet people in a setting where it is easy to join and leave conversations (like conference breaks).\nSleep is crucial for conferences. I\u0026rsquo;ve held that opinion for a while, but on reflection I have not gone far enough. It\u0026rsquo;s easy to keep hanging out and stay up late then set an alarm to make the morning session. Almost all people need 7-8 hours of sleep, not counting the time before we fall asleep, and the specific sleep hours need to be consistent (see “Why We Sleep” by Matthew Walker). There are two options here: either plan to go back to the hotel at 10pm to be up at 7am, or plan to miss the morning session, staying out till 12am, waking up at 9am. On the topic of sleep, I\u0026rsquo;m noticing jet lag more as I get older and while the university won\u0026rsquo;t pay for lodging before the conference, I should seriously consider it anyway (but not tire myself out by doing in a million tourist activities).\nWorkshop days Previously I\u0026rsquo;ve jumped between workshops, trying to squeeze in every keynote talk from someone whose work I am interested in. In future, I plan to take go to a single workshop all day. Reflecting on the keynotes I\u0026rsquo;ve seen, most are just conference talks stitched together. I\u0026rsquo;m not learning a lot that is new. In contrast, being at a single workshop means engaging with a sub-community.\nConclusion In many ways, the plans outlined above are very similar to what I do already. I am eager to see how the changes work out and also hope that having stepped back like this I will feel less uncertain about my choices at the next conference.\nWhile putting this together I found this series of tweets from Chinmay Kulkarni interesting. Our opinions differ on certain points, so check them out: https://twitter.com/chinmay/status/988410612316286976?s=20\nPostscript: Responses After sharing this on Twitter, there was some interesting discussion. For posterity, I\u0026rsquo;m summarising that and other comments I got here:\nGeneral\n  Be willing to leave a poster, talk, or conversation. It may feel polite to stay, but time is valuable, so once something no longer seems interesting, move on to something else. This is one reason to sit on the aisle in a talk.\n  Note taking is important because there is simply too much happening to remember. One suggestion was to track who you talked to and what it was about.\n  Consider going to random things and trying to understand them. This can lead to unexpected links to your own work and may be more interesting (as everything is new).\n  I clarified my small talk point above. I definitely see it is valuable, but if every conversation is small talk then you are missing an opportunity to have a conversation you couldn\u0026rsquo;t have outside of a conference.\n  People vary in their preferences regarding staying up late or not. Some see it is valuable time to connect. Others get the same from morning activities like running groups.\n  Talks vs. Posters\n  Talks can give a lot of content in a brief period and convey the presenter\u0026rsquo;s view of the most important idea.\n  Staying for a complete talk session can expose you to work that is relevant to your interests, but you might have otherwise missed.\n  Talks are often recorded now, so you can watch them later instead (but be honest with yourself about whether you will).\n  Demos and Industry track presentations may have content not in the paper.\n  Attending a talk is a nice way to connect with someone. It means you are guaranteed to find them and there is a starting place for conversation.\n  Interactions at posters can make you feel part of the community in a way talks don\u0026rsquo;t.\n  As the community grows we may need to explore other structures. For example, at RSS, work is presented as both a four minute talk and a poster session (NAACL tried a 1-minute-madness at least once, which is a similar idea).\n  ","date":1569163775,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569163775,"objectID":"8688cfcf9442f0891ad9b6f36c8ad829","permalink":"https://www.jkk.name/post/2019-09-22_conferenceapproach/","publishdate":"2019-09-22T10:49:35-04:00","relpermalink":"/post/2019-09-22_conferenceapproach/","section":"post","summary":"Am I getting the most our of time at conferences? This post was a way for me to think through that question and come up with strategies.","tags":["advice"],"title":"Approaching Conferences","type":"post"},{"authors":null,"categories":[],"content":"Games have been a focus of AI research for decades, from Samuel\u0026rsquo;s checkers program in the 1950s, to Deep Blue playing Chess in the 1990s, and AlphaGo playing Go in the 2010s. All of those are two-player sequential games. In this paper (to appear at NeurIPS), we looked at Diplomacy, a seven player game with simultaneous turns.\nThe paper makes three main contributions:\n A neural model that plays the game. Software to play the game (determining the outcomes of player actions is a non-trivial problem). Experiments with supervised learning and reinforcement learning.  Our paper only considers the version of the game where players can not talk to each other (No Press). Engaging in conversation in the game is a fascinating challenge that will involve a lot more work.\nHow well does the bot play the game? It convincingly beats prior systems designed for the game. Playing against it, I saw an impressive improvement over the course of the project. Early on I won trivially with mostly conservative moves. Later I had to carefully consider my moves, and was unable to win as certain powers (e.g. Austria). Eventually I was unable to beat the bot without playing several times, using observations from one game to inform my strategy in subsequent games. I am not an expert player, but I doubt a human playing one power in the game with no prior knowledge can win against the bot. However, I think a single bot playing against six skilled humans would almost definitely lose (we did not test this setting).\nCitation  Paper\n@InProceedings{neurips19diplomacy, author = {Paquette, Philip and Lu, Yuchen and Bocco, Steven and Smith, Max O. and Ortiz-Gagn{\\'e}, Satya and Kummerfeld, Jonathan K. and Pineau, Joelle and Singh, Satinder and Courville, Aaron}, title = {No-Press Diplomacy: Modeling Multi-Agent Gameplay}, booktitle = {Advances in Neural Information Processing Systems 32}, year = {2019}, month = {December}, pages = {}, url = {}, arxiv = {https://arxiv.org/abs/1909.02128}, }  ","date":1568394023,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568394023,"objectID":"361ec659e785baf9085f0726e2b7ee3f","permalink":"https://www.jkk.name/post/2019-09-13_diplomacynopress/","publishdate":"2019-09-13T13:00:23-04:00","relpermalink":"/post/2019-09-13_diplomacynopress/","section":"post","summary":"Games have been a focus of AI research for decades, from Samuel's checkers program in the 1950s, to Deep Blue playing Chess in the 1990s, and AlphaGo playing Go in the 2010s. All of those are two-player...","tags":["paper","neurips"],"title":"No-Press Diplomacy: Modeling Multi-Agent Gameplay (Paquette et al., 2019)","type":"post"},{"authors":null,"categories":null,"content":"","date":1567382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567382400,"objectID":"f7e5118ea7c59e1542e73137f7305abf","permalink":"https://www.jkk.name/data/irc/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/data/irc/","section":"data","summary":"Annotation of IRC messages with reply-to structure, which disentangles simultaneous conversations. The largest such annotated resource.","tags":["discourse"],"title":"IRC Disentanglement","type":"data"},{"authors":null,"categories":null,"content":"","date":1567382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567382400,"objectID":"0c450aa3568f007f97b15bff100ea3ca","permalink":"https://www.jkk.name/software/slate/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/software/slate/","section":"software","summary":"A terminal-based text annotation tool in Python.","tags":["annotation"],"title":"SLATE: A Super-Lightweight Annotation Tool for Experts","type":"software"},{"authors":["Chulaka Gunasekara","Jonathan K. Kummerfeld","Lazaros Polymenakos","Walter S. Lasecki"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"8faed1e3b7a810362a8e604a008834e1","permalink":"https://www.jkk.name/publication/ws-acl-convai19dstc7/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/ws-acl-convai19dstc7/","section":"publication","summary":"","tags":null,"title":"DSTC7 Task 1: Noetic End-to-End Response Selection","type":"publication"},{"authors":null,"categories":[],"content":"This post is about my own paper to appear at ACL later this month. What is interesting about this paper will depend on your research interests, so that\u0026rsquo;s how I\u0026rsquo;ve broken down this blog post.\nA few key points first:\n  Data and code are available on Github. The paper is also available. The general-purpose span labeling and linking annotation tool we used is also appearing at ACL. Check out DSTC 8 Track 2, which is based on this work.  You study discourse We investigated discourse structure when multiple conversations are occurring in the same stream of communication. In our case, the stream is a technical support channel for Ubuntu on Internet Relay Chat (IRC). We annotated each message with which message(s) it was a response to. As far as we are aware, this is the first large-scale corpus with this kind of discourse structure in synchronous chat. Here is an example from the data, with annotations marked by edges and colours:\nWe don\u0026rsquo;t frame the paper as being about reply-structure though. Instead, we focus on a byproduct of these annotations - conversation disentanglement. Given our graph of reply-structure, each connected component is a single conversation (as shown by each colour in the example). The key prior work on the disentanglement problem is Elsner and Charniak (2008), who released the largest annotated resource for the task, with 2,500 messages manually separated into conversations. We annotated their data with our annotation scheme and 75,000 additional messages.\nWe built a set of simple models for predicting reply-structure and did some analysis of assumptions about discourse from prior disentanglement work, but there is certainly more scope for study here. One direction would be to develop better models for this task. Another would be to study patterns in the data to understand how people are able to follow the conversation.\nYou work on dialogue There has been a lot of work recently using the Ubuntu dataset from Lowe et al., (2015), which was produced by heuristically disentangling conversations from the same IRC channel we use. Their work opened up a fantastic research opportunity by providing 930,000 conversations for training and evaluating dialogue systems. However, they were unable to evaluate the quality of their conversations because they had no annotated data.\nUsing our data, we found that only 20% of their conversations are a true prefix of a conversation (since their next utterance classification task cuts the conversation off part-way, being a true prefix is all that matters). Many conversations are missing messages, and some have extra messages from other conversations. Unsurprisingly, our trained model does better, producing conversations that are a true prefix 81% of the time. We also noticed that their heuristic was incorrectly linking messages far apart in time. This is not tested by our evaluation set, so we constructed this figure, which shows the problem is quite common:\nThe purple results are based on the output of our model over the entire Ubuntu IRC logs. That output is the basis of DSTC 8 Track 2. Once the competition finishes (October 20th, 2019) we will release all of the conversations.\nYou am interested in studying online communities This is not my area of expertise, but our data and models could enable the exploration of interesting questions. For example:\n What is the structure of the community? By looking at who asks for help and who responds we could see patterns of behaviour. How does a community evolve over time? This data spans 15 years, during which there were many Ubuntu releases, Stackoverflow was created, other Ubuntu forums were created, etc. It seems likely that those events and more would be reflected in the data.  It would be interesting to apply the model to other communities, but that would require additional in-domain data to get good results. We have no plans to collect additional data at this stage, and for other channels there are copyright questions that might be difficult to resolve (the Ubuntu channels have an open access license).\nYou mainly care about neural network architectures We experimented with a bunch of ideas that didn\u0026rsquo;t improve performance, so our final model is very simple (a feedforward network with features representing the logs and sentences represented by averaging and max-pooling GloVe embeddings). Maybe that means there is an opportunity for you to improve on our results with a fancy model? One of our motivations for making such a large new resource was to make it possible to train sophisticated models.\nAcknowledgments This project has been going since I started at Michigan as a postdoc funded by a grant from IBM. The final paper is the result of collaboration with a large group of people from Michigan and IBM. Thank you!\nCitation  Paper\n@InProceedings{acl19disentangle, author = {Kummerfeld, Jonathan K. and Gouravajhala, Sai R. and Peper, Joseph and Athreya, Vignesh and Gunasekara, Chulaka and Ganhotra, Jatin and Patel, Siva Sankalp and Polymenakos, Lazaros and Lasecki, Walter S.}, title = {A Large-Scale Corpus for Conversation Disentanglement}, booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, location = {Florence, Italy}, month = {July}, year = {2019}, url = {https://github.com/jkkummerfeld/irc-disentanglement/raw/master/acl19irc.pdf}, arxiv = {https://arxiv.org/abs/1810.11118}, software = {https://jkk.name/irc-disentanglement}, data = {https://jkk.name/irc-disentanglement}, }  ","date":1562771946,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562771946,"objectID":"63f24556c7ae65a7d2cefbff4a0a1244","permalink":"https://www.jkk.name/post/2019-07-10_disentanglement/","publishdate":"2019-07-10T11:19:06-04:00","relpermalink":"/post/2019-07-10_disentanglement/","section":"post","summary":"This post is about my own paper to appear at ACL later this month. What is interesting about this paper will depend on your research interests, so that\u0026rsquo;s how I\u0026rsquo;ve broken down this blog post.\nA few key points first:\n  Data and code are available on Github. The paper is also available.","tags":["paper","acl","discourse","dialogue","data"],"title":"A Large-Scale Corpus for Conversation Disentanglement (Kummerfeld et al., 2019)","type":"post"},{"authors":["Jonathan K. Kummerfeld","Sai R. Gouravajhala","Joseph J. Peper","Vignesh Athreya","Chulaka Gunasekara","Jatin Ganhotra","Siva Sankalp Patel","Lazaros Polymenakos","Walter S. Lasecki"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"91eb1c0c4506db0cc5525979596b21e0","permalink":"https://www.jkk.name/publication/acl19disentangle/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/acl19disentangle/","section":"publication","summary":"Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our data is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. We use our data to re-examine prior work, in particular, finding that 89% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research.","tags":null,"title":"A Large-Scale Corpus for Conversation Disentanglement","type":"publication"},{"authors":["Jonathan K. Kummerfeld"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"312d94fae2b71b8452bc2aaa0cedcc6c","permalink":"https://www.jkk.name/publication/acl19slate/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/acl19slate/","section":"publication","summary":"Many annotation tools have been developed, covering a wide variety of tasks and providing features like user management, pre-processing, and automatic labeling. However, all of these tools use a Graphical User Interface, and often require substantial effort for installation and configuration. This paper presents a new annotation tool that is designed to fill the niche of a lightweight interface for users with a terminal-based workflow. Slate supports annotation at different scales (spans of characters, tokens, and lines, or a document) and of different types (free text, labels, and links), with easily customisable keybindings, and unicode support. In a user study comparing with other tools it was consistently the easiest to install and use. Slate fills a need not met by existing systems, and has already been used to annotate two corpora, one of which involved over 250 hours of annotation effort.","tags":null,"title":"SLATE: A Super-Lightweight Annotation Tool for Experts","type":"publication"},{"authors":["Stefan Larson","Anish Mahendran","Andrew Lee","Jonathan K. Kummerfeld","Parker Hill","Michael Laurenzano","Johann Hauswald","Lingjia Tang","Jason Mars"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"34804edc7402521fdcee10141f070b9b","permalink":"https://www.jkk.name/publication/naacl19outliers/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/naacl19outliers/","section":"publication","summary":"In a corpus of data, outliers are either errors: mistakes in the data that are counterproductive, or are unique: informative samples that improve model robustness. Identifying outliers can lead to better datasets by (1) removing noise in datasets and (2) guiding collection of additional data to fill gaps. However, the problem of detecting both outlier types has received relatively little attention in NLP, particularly for dialog systems. We introduce a simple and effective technique for detecting both erroneous and unique samples in a corpus of short texts using neural sentence embeddings combined with distance-based outlier detection. We also present a novel data collection pipeline built atop our detection technique to automatically and iteratively mine unique data samples while discarding erroneous samples. Experiments show that our outlier detection technique is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models.","tags":null,"title":"Outlier Detection for Improved Data Quality and Diversity in Dialog Systems","type":"publication"},{"authors":null,"categories":[],"content":"Crowdsourcing, collecting annotations of data from a distributed group of people online, is a major source of data for AI research. The original idea involved people doing it as volunteers (e.g. Folding@home) or as a byproduct of some other goal (e.g. reCAPTCHA), but most of the data collected in AI today is from paid workers. Recently, Hal Daumé III mentioned on Twitter that Figure Eight, a paid crowdsourcing service, had removed their free licenses for academics, and asked for alternatives (Note, Figure Eight has since been acquired by Appen). A bunch of people had suggestions which I wanted to record for my own future reference, hence this blog post.\nThese fell into a few categories:\n Crowd providers, which directly connect with workers. Crowd enhancers, which provide a layer on top of the providers that adds features (e.g. active learning, nice templates, sophisticated workflows). Annotation tools, which are designed to integrate with crowd providers (or your own internal workers). Interfaces, which make it easier to use one of the crowd providers.  I decided not to break the first two categories apart because it was sometimes unclear whether a service was using their own crowd or providing a layer over another, but I have roughly sorted them. Where possible I have included pricing, though some services did not make it easy to find. Take note of the description in each case because the data collected varies substantially. Also note that many tasks can be structured as a classification task (e.g. \u0026ldquo;Is this coreference link correct?\u0026quot;), making many of these services more flexible than the \u0026lsquo;text classification\u0026rsquo; label below may seem (though structuring your task so costs don\u0026rsquo;t explode may require some thought).\n  Mechanical Turk, a small set of templates and the option to define a web UI that does whatever you want. Cost is a 20% fee on top of whatever you choose to pay workers (though note it jumps to 40% if you have more than 10 assignments for a HIT!). Figure Eight (now part of Appen), included for completeness, did not investigate further due to the cost.  Hybrid, seems to be any task you can define in text (including with links?). 40% fee, though there is a discount of some type for academic and non-profit institutions.  Prolific, seems to be that you just provide a link to a site for annotations (originally intended for survey research). 30% fee. Last year they had a research grant program.  Gorilla, designed for social science research, but could be used for any classification or free text task. Costs $1.19 / response, though note that you construct a questionnaire with a series of questions. There are also discounts available when collecting thousands of responses.  Scale, classification tasks for 8c / annotation. There is an academic program, but details are not available online (mentioned here).  Amazon SageMaker Ground Truth, text classification for 8c / label, decreasing after 50,000 annotations + a workflow fee of 1.2c / label.  iMerit, NER, classification, and sentiment tasks. When used on the Amazon Marketplace they are 5 dollars / hour (India based workers) or 25 (US based workers).  Mechanical Turk Integration Interfaces These are interfaces for Mechanical Turk that provide an easier way to set up HITs without having to mess with Amazon\u0026rsquo;s APIs yourself. Both are free, but have slightly different features:\n  LegionTools, self-hosted or not, includes key features for real-time systems.  MTurk Manager, self-hosted, includes features for custom views of responses from workers.  Annotation User Interfaces There are many annotation tools for NLP (e.g. my own, SLATE!), but these annotation tools are designed to integrate with providers above to collect annotations.\n  Prodigy, span classification (e.g. NER), multiple choice questions (which can be used to do a wide range of tasks), and relations (see examples). Cost is whatever you pay a crowd provider + 390 for a lifetime license, or 10k for a university-wide lifetime license, though they also often give free licenses to academics. One distinctive property is that you download and run it yourself, providing complete control over your data.  LightTAG, span classification and links. Cost is 1c / annotation + the cost from a crowd provider, but there is an academic license that makes it free.  ","date":1555253375,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555253375,"objectID":"5a507276e757b0856a9c56d867eab311","permalink":"https://www.jkk.name/post/crowdsourcing-services/","publishdate":"2019-04-14T10:49:35-04:00","relpermalink":"/post/crowdsourcing-services/","section":"post","summary":"A range of services exist for collecting annotations from paid workers. This post gives an overview of a bunch of them.","tags":["crowdsourcing"],"title":"Crowdsourcing Services","type":"post"},{"authors":["Charles Welch","Verónica Pérez-Rosas","Jonathan K. Kummerfeld","Rada Mihalcea"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"be2e0bbb108b4378791ca42a044dc1db","permalink":"https://www.jkk.name/publication/cicling19personal/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/cicling19personal/","section":"publication","summary":"We examine a large dialog corpus obtained from the conversation history of a single individual with 104 conversation partners. The corpus consists of half a million instant messages, across several messaging platforms. We focus our analyses on seven speaker attributes, each of which partitions the set of speakers, namely: gender; relative age; family member; romantic partner; classmate; co-worker; and native to the same country. In addition to the content of the messages, we examine conversational aspects such as the time messages are sent, messaging frequency, psycholinguistic word categories, linguistic mirroring, and graph-based features reflecting how people in the corpus mention each other. We present two sets of experiments predicting each attribute using (1) short context windows; and (2) a larger set of messages. We find that using all features leads to gains of 9-14% over using message text only.","tags":null,"title":"Look Who's Talking: Inferring Speaker Attributes from Personal Longitudinal Dialog","type":"publication"},{"authors":["Chulaka Gunasekara","Jonathan K. Kummerfeld","Lazaros Polymenakos","Walter S. Lasecki"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c697d3910bd17f6cbfba97f9aa36ccdf","permalink":"https://www.jkk.name/publication/ws-aaai-dstc19task1/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/ws-aaai-dstc19task1/","section":"publication","summary":"Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.","tags":null,"title":"DSTC7 Task 1: Noetic End-to-End Response Selection","type":"publication"},{"authors":["Charles Welch","Verónica Pérez-Rosas","Jonathan K. Kummerfeld","Rada Mihalcea"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"025cdc9a1deab77c713eb04cfe0c77a4","permalink":"https://www.jkk.name/publication/ieee19personal/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/ieee19personal/","section":"publication","summary":"We explore the use of longitudinal dialog data for two dialog prediction tasks: next message prediction and response time prediction. We show that a neural model using personal data that leverages a combination of message content, style matching, time features, and speaker attributes leads to the best results for both tasks, with error rate reductions of up to 15% compared to a classifier that relies exclusively on message content and to a classifier that does not use personal data.","tags":null,"title":"Learning from Personal Longitudinal Dialog Data","type":"publication"},{"authors":["Koichiro Yoshino","Chiori Hori","Julien Perez","Luis Fernando D'Haro","Lazaros Polymenakos","Chulaka Gunasekara","Walter S. Lasecki","Jonathan K. Kummerfeld","Michel Galley","Chris Brockett","Jianfeng Gao","Bill Dolan","Xiang Gao","Huda Alamari","Tim K. Marks","Devi Parikh","Dhruv Batra"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"abce99c2a05dddf9e18d59f219c420a8","permalink":"https://www.jkk.name/publication/ws-neurips-convai18dstc/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/ws-neurips-convai18dstc/","section":"publication","summary":"This paper introduces the Seventh Dialog System Technology Challenges (DSTC), which use shared datasets to explore the problem of building dialog systems. Recently, end-to-end dialog modeling approaches have been applied to various dialog tasks. The seventh DSTC (DSTC7) focuses on developing technologies related to end-to-end dialog systems for (1) sentence selection, (2) sentence generation and (3) audio visual scene aware dialog. This paper summarizes the overall setup and results of DSTC7, including detailed descriptions of the different tracks and provided datasets. We also describe overall trends in the submitted systems and the key results. Each track introduced new datasets and participants achieved impressive results using state-of-the-art end-to-end technologies.","tags":null,"title":"Dialog System Technology Challenge 7","type":"publication"},{"authors":null,"categories":[],"content":"The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset). Some of these are discussed in my CoNLL Shared Task submission paper, the biggest being the choice to not annotate mentions that are not coreferent. This paper describes a new dataset that has a different set of compromises, specifically:\n A broader definition of coreference (e.g. appositives are coreferent) All mentions annotated Different annotation methods for different subsets of the data (training data is double annotated and then adjudicated, while the development and test data is triple annotated, all pairs of annotations are adjudicated, then the outcomes are merged by voting) A variety of genres, but generally simpler language  The dataset is 10x the size of OntoNotes and freely available, which is fantastic. The source text is 2/3rds the RACE dataset (English reading comprehension exams from China), and 1/3rd scraped websites. Measurements of annotator agreement suggest the annotations are not as consistent as OntoNotes, but still good enough to be a useful resource. I do disagree with one aspect of the paper\u0026rsquo;s analysis - the results show a substantial gain in performance when providing gold mentions, suggesting to me that it remains an important challenge in coreference resolution. I\u0026rsquo;m also curious whether my coreference analysis tool would find different patterns in errors on this dataset compared to OntoNotes.\nCitation  Paper\n Data\n@InProceedings{Chen:EMNLP:2018, author = {Chen, Hong and Fan, Zhenhua and Lu, Hao and Yuille, Alan and Rong, Shu}, title = {PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution}, booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, year = {2018}, publisher = {Association for Computational Linguistics}, pages = {172--181}, location = {Brussels, Belgium}, url = {https://aclanthology.org/D18-1016}, }  ","date":1541694572,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541694572,"objectID":"e9a8581c2a5084a3401ef158415d0a2b","permalink":"https://www.jkk.name/post/2018-11-08_corefdata/","publishdate":"2018-11-08T11:29:32-05:00","relpermalink":"/post/2018-11-08_corefdata/","section":"post","summary":"The OntoNotes dataset, which is the focus of almost all coreference resolution research, had several compromises in its development (as is the case for any dataset).  Some of these are discussed in...","tags":["paper","data","coreference"],"title":"PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution (Chen et al., 2018)","type":"post"},{"authors":null,"categories":[],"content":"A common argument in favour of neural networks is that they do not require \u0026lsquo;feature engineering\u0026rsquo;, manually defining functions that produce useful representations of the input data (e.g. a function that checks if a word is in a list of cities and returns 1 or 0). This paper argues that there is in fact still value in such functions.\nThe task is named entity recognition and the model is a CRF with a bidirectional LSTM using character and word embeddings. The functions in this case are (1) part of speech tags, (2) word shapes, and (3) gazetteers. Importantly, as well as receiving these as inputs, the model has to predict them as outputs (in both cases using predictions, not gold values). The improvement on the test set is substantial, ~0.8 F1. Ablation indicates that POS tags and word shape are particularly important, and having both the input and output is important. Interestingly, the shift on the development set is more marginal, ~0.3 F1, and the ablation doesn\u0026rsquo;t show as clear trends.\nOverall, my takeaway is that these kinds of features (which are not very hard to define) are worth the effort. However, there are a few more values I would have liked to see:\n Multi-task learning (they kind of get at this with one ablation, but it is on non-gold output) Cross-validation results (given the difference between dev and test) ELMo (the paper argues that it is orthogonal, which is reasonable, but I\u0026rsquo;m still curious)  Citation  Paper\n@InProceedings{Wu:2018:EMNLP, author = {Wu, Minghao and Liu, Fei and Cohn, Trevor}, title = {Evaluating the Utility of Hand-crafted Features in Sequence Labelling}, booktitle = {EMNLP}, year = {2018}, url = {https://arxiv.org/abs/1808.09075}, }  ","date":1536071843,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536071843,"objectID":"7943d6aa3522e932ec6157b86ab1b427","permalink":"https://www.jkk.name/post/2018-09-04_featureengineering/","publishdate":"2018-09-04T10:37:23-04:00","relpermalink":"/post/2018-09-04_featureengineering/","section":"post","summary":"A common argument in favour of neural networks is that they do not require 'feature engineering', manually defining functions that produce useful representations of the input data (e.g. a function...","tags":["paper"],"title":"Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu et al., 2018)","type":"post"},{"authors":null,"categories":null,"content":"","date":1534118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534118400,"objectID":"1485a7feca42163611c75ca793944bdb","permalink":"https://www.jkk.name/software/neural-tagger/","publishdate":"2018-08-13T00:00:00Z","relpermalink":"/software/neural-tagger/","section":"software","summary":"Implementations of a POS tagger in DyNet, PyTorch, and Tensorflow, visualised to show the overall picture and make comparisons easy.","tags":["neural-networks"],"title":"Neural POS tagging","type":"software"},{"authors":null,"categories":null,"content":"","date":1531353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531353600,"objectID":"a52e4d6aa9bfea31186e16afa2be7c91","permalink":"https://www.jkk.name/software/text2sql-baseline/","publishdate":"2018-07-12T00:00:00Z","relpermalink":"/software/text2sql-baseline/","section":"software","summary":"A simple LSTM-based model that uses templates and slot-filing to map questions to SQL queries.","tags":["semantics"],"title":"Text to SQL Baseline","type":"software"},{"authors":null,"categories":null,"content":"","date":1531353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531353600,"objectID":"6a9f08c0304a39f194c44af15bcfa293","permalink":"https://www.jkk.name/data/text-to-sql/","publishdate":"2018-07-12T00:00:00Z","relpermalink":"/data/text-to-sql/","section":"data","summary":"A collection of datasets containing questions in English paired with SQL queries for a provided database. Our version homogenises the style of the SQL and corrects errors in previous versions of the data.","tags":["semantics"],"title":"Text to SQL datasets","type":"data"},{"authors":["Catherine Finegan-Dollak","Jonathan K. Kummerfeld","Li Zhang","Karthik Ramanathan","Sesh Sadasivam","Rui Zhang","Dragomir Radev"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"b5c84741c02cd2f93f4b2fc296051e3f","permalink":"https://www.jkk.name/publication/acl18sql/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/acl18sql/","section":"publication","summary":"To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.","tags":null,"title":"Improving Text-to-SQL Evaluation Methodology","type":"publication"},{"authors":null,"categories":["acl","domain-adaptation","syntax"],"content":"Virtually all systems trained using data have trouble when applied to datasets that differ even slightly - even switching from Wall Street Journal text to New York Times text can hurt parsing performance slightly. Extensive work has explored how to adapt to new domains (including one of my own), but generally these approaches only made up a fraction of the gap in performance.\nThis paper shows two interesting new approaches to this issue:\n Use ELMo, a type of word representation trained on massive amounts of text. Train a span-based parser with partial annotations.  The first is straightforward, and further demonstrates the effectiveness of ELMo. To give a sense of how much this helps, the Charniak parser goes from 92 on the WSJ to 85 on the Brown corpus, while this model goes from 94 to 90. The second idea takes advantage of a recent parsing model with a simple approach:\n Independently assign a score to every span of a sentence, indicating whether it is part of the parse. Find the maximum scoring set of spans using a dynamic program.  The structure of the scoring step allows for a convenient form of partial annotations. Simply label the tricky spans in a sentence (e.g. to indicate where a prepositional phrase attaches / does not attach). During training on partially annotated sentences, only the labeled spans are used to update the model. This gives dramatic gains across multiple datasets.\nCitation  Paper\n@InProceedings{Joshi:2018:ACL, author = {Joshi, Vidur and Peters, Matthew and Hopkins, Mark}, title = {Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples}, booktitle = {ACL}, year = {2018}, url = {https://arxiv.org/abs/1805.06556}, }  ","date":1528849980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528849980,"objectID":"446ee8952e33797a5780b1b917212bdb","permalink":"https://www.jkk.name/post/2018-06-12_parseradaptation/","publishdate":"2018-06-12T20:33:00-04:00","relpermalink":"/post/2018-06-12_parseradaptation/","section":"post","summary":"Virtually all systems trained using data have trouble when applied to datasets that differ even slightly - even switching from Wall Street...","tags":["paper"],"title":"Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples (Vidur Joshi et al., 2018)","type":"post"},{"authors":["Yiping Kang","Yunqi Zhang","Jonathan K. Kummerfeld","Parker Hill","Johann Hauswald","Michael A. Laurenzano","Lingjia Tang","Jason Mars"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"1532ee9d156f57530dcb79cc707d62c6","permalink":"https://www.jkk.name/publication/naacl18data/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/naacl18data/","section":"publication","summary":"Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities. Crowdsourcing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs. Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis, which may not be indicative of impact on trained model performance. In this paper, we present a study of crowdsourcing methods for a user intent classification task in one of our deployed dialogue systems. Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs. Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems.","tags":null,"title":"Data Collection for a Production Dialogue System: A Startup Perspective","type":"publication"},{"authors":["Youxuan Jiang","Catherine Finegan-Dollak","Jonathan K. Kummerfeld","Walter Lasecki"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"849ae4eee0032e6fe87b325f559bf85e","permalink":"https://www.jkk.name/publication/naacl18summary/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/naacl18summary/","section":"publication","summary":"Most summarization research focuses on summarizing the entire given text, but in practice readers are often interested in only one aspect of the document or conversation. We propose 'targeted summarization' as an umbrella category for summarization tasks that intentionally consider only parts of the input data. This covers query-based summarization, update summarization, and a new task we propose where the goal is to summarize a particular aspect of a document. However, collecting data for this new task is hard because directly asking annotators (e.g., crowd workers) to write summaries leads to data with low accuracy when there are a large number of facts to include.  We introduce a novel crowdsourcing workflow, Pin-Refine, that allows us to collect highquality summaries for our task, a necessary step for the development of automatic systems.","tags":null,"title":"Effective Crowdsourcing for a New Type of Summarization Task","type":"publication"},{"authors":["Laura Burdick","Jonathan K. Kummerfeld","Rada Mihalcea"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"b674fbf25e9c74f352d09085da4ff38e","permalink":"https://www.jkk.name/publication/naacl18embeddings/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/naacl18embeddings/","section":"publication","summary":"Despite the recent popularity of word embedding methods, there is only a small body of work exploring the limitations of these representations. In this paper, we consider one aspect of embedding spaces, namely their stability. We show that even relatively high frequency words (100-200 occurrences) are often unstable. We provide empirical evidence for how various factors contribute to the stability of word embeddings, and we analyze the effects of stability on downstream tasks.","tags":null,"title":"Factors Influencing the Surprising Instability of Word Embeddings","type":"publication"},{"authors":null,"categories":[],"content":"We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models usually end up doing reasonably well. This paper asks an important question - are those metrics measuring generalisability effectively? In particular, if we sample our test set from a slightly different distribution of data, do models still work well?\nAs a controlled set up they form a simple dataset as follows for each sentence:\n Go through the sentence left to right For each word generate three words in the output, where the output words are randomly sampled from a small vocabulary that is unique to each input word  This is clearly learnable and it seems reasonable that a sequence-to-sequence neural model with attention should be able to learn it. Experiments show they do, getting close to 100% on a test set sampled the same way as the training set (input length 5-10, no symbol used twice). However, if the test set is slightly different, with sequences of length 11-15, then results vary from 0% to 98% depending on the random seed in training (other variations also lead to large variations). What this means is that sometimes the model is not learning to generalise. They also show that the models that do generalise can only do so in one way (e.g. remain effective when length varies, or remain effective when symbols are used more than once in the input).\nA few takeaways:\n Make sure your training and testing data are sampled from the distribution you are interested in More study of training data order and weight initialisation is needed (these are the two factors impacted by the random seed)  Incidentally, I am a co-author on an ACL paper that points out a similar issue for mapping text questions to SQL queries. If we restrict the test set to be novel queries (i.e. the model has to generalise) performance falls through the floor.\nCitation  Paper\n@Article{Weber:2018:GenDeep, author = {Weber, Noah and Shekhar, Leena and Balasubramanian, Niranjan}, title = {The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models}, journal = {Workshop on New Forms of Generalization in Deep Learning and NLP (NAACL 2018)}, year = {2018}, url = {https://arxiv.org/abs/1805.01445}, }  ","date":1525784431,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525784431,"objectID":"f0e8539ce49d8507b14610220eba0d59","permalink":"https://www.jkk.name/post/2018-05-08_seq2seqsensitivity/","publishdate":"2018-05-08T09:00:31-04:00","relpermalink":"/post/2018-05-08_seq2seqsensitivity/","section":"post","summary":"We know that training a neural network involves optimising over a non-convex space, but using standard evaluation methods we see that our models...","tags":["paper","gen-deep","analysis","neural-network"],"title":"The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models (Weber et al., 2018)","type":"post"},{"authors":["Charles Welch","Jonathan K. Kummerfeld","Song Feng","Rada Mihalcea"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"b4d04f97087d0563480c521c9f37cb18","permalink":"https://www.jkk.name/publication/lrec18amr/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/lrec18amr/","section":"publication","summary":"In this paper we explore the role played by world knowledge in semantic parsing. We look at the types of errors that currently exist in a state-of-the-art Abstract Meaning Representation (AMR) parser, and explore the problem of how to integrate world knowledge to reduce these errors. We look at three types of knowledge from (1) WordNet hypernyms and super senses, (2) Wikipedia entity links, and (3) retraining a named entity recognizer to identify concepts in AMR. The retrained entity recognizer is not perfect and cannot recognize all concepts in AMR and we examine the limitations of the named entity features using a set of oracles. The oracles show how performance increases if it can recognize different subsets of AMR concepts. These results show improvement on multiple fine-grained metrics, including a 6% increase in named entity F-score, and provide insight into the potential of world knowledge for future work in Abstract Meaning Representation parsing.","tags":null,"title":"World Knowledge for Abstract Meaning Representation Parsing","type":"publication"},{"authors":null,"categories":[],"content":"Assigning a probability distribution over the next word or character in a sequence (language modeling) is a useful component of many systems, such as speech recognition and translation. Recently neural networks have come to dominate in performance, with a range of clever innovations in network structure. This paper is not about new models, but rather explores the current evaluation and how well carefully tuned baseline models can do.\nThe key observations for me were:\n There are issues with the PTB dataset for character-level evaluation - it removes all punctuation, makes numbers \u0026lsquo;N\u0026rsquo;, and removes rare words (i.e. it is a character-level version of the token-level task). Given that the original Penn Treebank exists, I would have been interested to see a comparison with the PTB without any simplification. The other dataset, enwik8, makes sense as a testing ground for compression algorithms, but is a little odd for modeling language, since it is the first 100 million bytes of a Wikipedia XML dump. The paper does have another dataset, WikiText, which sounds good, but then there is no character-level evaluation! The LSTM is able to achieve ~SotA results for character-level modeling. The key seems to be careful design of the softmax that produces the final probability distribution: (1) rare words are clustered and represented by a single value in the distribution calculation, and (2) word vectors are shared between input and output. Dropout matters more than the network design, and multiple forms of dropout should be tuned jointly. This comes from analysis of a set of models trained with random variation in hyperparameters.  Citation  Paper\n@Article{2018arXiv180308240M, author = {{Merity}, S. and {Shirish Keskar}, N. and {Socher}, R.}, title = {An Analysis of Neural Language Modeling at Multiple Scales}, journal = {ArXiv e-prints}, year = {2018}, url = {https://arxiv.org/abs/1803.08240}, }  ","date":1523926522,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523926522,"objectID":"cca68d980242da70f03a3da4f15bc4fd","permalink":"https://www.jkk.name/post/2018-04-16_lm_analysis/","publishdate":"2018-04-16T20:55:22-04:00","relpermalink":"/post/2018-04-16_lm_analysis/","section":"post","summary":"Assigning a probability distribution over the next word or character in a sequence (language modeling) is a useful component of many systems...","tags":["paper","language-model","arxiv"],"title":"An Analysis of Neural Language Modeling at Multiple Scales (Merity et al., 2018)","type":"post"},{"authors":null,"categories":[],"content":"Being able to query a database in natural language could help make data accessible to more people. Systems that do this have to solve two challenges: (1) understanding the query and (2) expressing the response in a way the user will understand. Recently there have been papers in the NLP community on the first challenge, but this paper comes from the DB community and considers the second.\nThe approach assumes we have a syntactic parse of the query and an alignment between the parse and the SQL query it corresponds to (they rely on prior work for this query interpretation piece). Given that, the new idea in this paper is to take the database results and use the alignment to insert values for each field into the original parse, and from there into the original question. To avoid extremely long sentences (when there are multiple result rows) they define a procedure to identify ways to summarise results.\nHowever, I\u0026rsquo;m not convinced by the evaluation. The dataset they use was collected by (1) enumerating the 196 types of queries people could ask using the Microsoft Academic Search service, and (2) a person manually writing a question for each query. As a result, the questions feel very formulaic and also only cover cases that we already have a user-friendly interface for, making it unclear how well this will generalise to more natural data. Still, this work explores an interesting problem and it\u0026rsquo;s cool to see a direct use of syntactic parsing!\nCitation  Paper\n@Article{Deutch:2017, author = {Deutch, Daniel and Frost, Nave and Gilad, Amir}, title = {Provenance for Natural Language Queries}, journal = {Proceedings of the VLDB Endowment}, volume = {10}, number = {5}, month = {Jan}, year = {2017}, issn = {2150-8097}, pages = {577--588}, doi = {10.14778/3055540.3055550}, url = {http://www.vldb.org/pvldb/vol10/p577-deutch.pdf}, }  ","date":1520557908,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520557908,"objectID":"b7d5ba73050892d60b1b59c5550fb315","permalink":"https://www.jkk.name/post/2018-03-08_sql/","publishdate":"2018-03-08T20:11:48-05:00","relpermalink":"/post/2018-03-08_sql/","section":"post","summary":"Being able to query a database in natural language could help make data accessible ...","tags":["paper","vldb","sql"],"title":"Provenance for Natural Language Queries (Deutch et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Usually when we learn, we have a curriculum designed to incrementally build understanding. It seems reasonable that the same idea could be useful for machine learning, and indeed there is a large body of work on the topic. This paper explores the specific question of whether a curriculum can help develop task-specific word vectors, and whether we can determine an effective curriculum automatically.\nThey define a linear model with a range of features that characterise a paragraph of text, such as the number of distinct words, the number of prepositional phrases, and the average number of syllables per word. Paragraphs are sorted by the model and used to train word vectors with word2vec. These word vectors are then used as part of a model for a target task, giving a score that indicates the quality of the curriculum. Based on this score the weights for the model are updated, using a form of Bayesian optimisation.\nOne really nice aspect of this paper is the range of tasks considered: sentiment analysis, NER, POS tagging, and parsing. Learning a curriculum does improve performance slightly, and which features are important varies across the tasks (indicating the importance of task-specific curriculums). However, the models are somewhat restricted (as shown by the low absolute performance) because they do not change the word vectors during training. For most of this paper that\u0026rsquo;s a reasonable decision, as it allows a clearer learning signal, but it would have been interesting to also see the impact on the normal training scenario and a state-of-the-art model. In my experience (and in our soon-to-appear NAACL paper) we find that variations in word vectors can disappear during training for a downstream task.\nCitation  Paper\n@InProceedings{tsvetkov-EtAl:2016:P16-1, author = {Tsvetkov, Yulia and Faruqui, Manaal and Ling, Wang and MacWhinney, Brian and Dyer, Chris}, title = {Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning}, booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {August}, year = {2016}, address = {Berlin, Germany}, publisher = {Association for Computational Linguistics}, pages = {130--139}, url = {https://aclanthology.org/P16-1013} }  ","date":1520302198,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520302198,"objectID":"b83da31744e9b505b578925918f6f811","permalink":"https://www.jkk.name/post/2018-03-05_curriculum/","publishdate":"2018-03-05T21:09:58-05:00","relpermalink":"/post/2018-03-05_curriculum/","section":"post","summary":"Reordering training sentences for word vectors may impact their usefulness for downstream tasks.","tags":["paper","acl","word-vectors"],"title":"Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning (Tsvetkov et al., 2016)","type":"post"},{"authors":null,"categories":[],"content":"It would be convenient to have a way to represent sentences in a vector space, similar to the way vectors are frequently used to represent input words for a task. Quite a few sentence embeddings methods have been proposed, but none have really caught on. Building on prior work by the same authors, the approach here is to define a neural network that maps a sentence to a vector, then train it with a loss function that measures similarity between the vectors for paraphrases.\nThis paper scales up the approach, using millions of paraphrases, and explores a range of models. To get the paraphrases they use translation (start with a sentence, translate it to another language and back, then assume the translation is a paraphrase). For negative examples they use the sentence that the model currently thinks is most similar other than the correct one (choosing this from a large enough set is key).\nThe best model is very simple - concatenate together the average of word vectors and the average of character trigram vectors. That consistently beats prior work, including convolutional models, and LSTMs. In a way, this is nice as it is a simple way to get a sentence representation! On the other hand, this can\u0026rsquo;t possibly capture the semantics of a sentence fully since it doesn\u0026rsquo;t take word order into consideration at all.\nCitation  ArXiv Paper\n@ARTICLE{2017arXiv171105732W, author = {{Wieting}, J. and {Gimpel}, K.}, title = {Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations}, journal = {ArXiv e-prints}, archivePrefix = {arXiv}, eprint = {1711.05732}, primaryClass = {cs.CL}, year = {2017}, month = {November}, url = {https://arxiv.org/abs/1711.05732}, }  ","date":1517444736,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517444736,"objectID":"bfba51509cd56b8236ac8c2a6fc7b152","permalink":"https://www.jkk.name/post/2018-01-31_sentencerepfromparaphrases/","publishdate":"2018-01-31T19:25:36-05:00","relpermalink":"/post/2018-01-31_sentencerepfromparaphrases/","section":"post","summary":"With enough training data, the best vector representation of a sentence is to concatenate an average over word vectors and an average over character trigram vectors.","tags":["paper","arxiv","semantics"],"title":"Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (Wieting et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"There is a lot of interest in dialogue agents, but a lot of work sits at one of two extremes: either chit-chat agents that just chat, or task-oriented agents that aim to call a specific API for the user. This work is about trying to integrate a range of systems from both categories, to get something more general purpose as a result.\nThe core approach is a hybrid system that switches between different agents behind the scenes (an approach taken by a number of Alexa Prize teams). The innovation here is that crowd workers will help with the decision (both suggesting things to say and voting on which response to use), and their votes will be used to learn a model to (partially) replace the people over time.\nUnfortunately, the improvement from a learned model of votes is only small (saves only 14% of the crowd effort), and the automated responses are rarely chosen (12% of the time). That said, it seems like an interesting design with a lot of subtle decisions that require more exploration - the sets of agents (4-6 here, mostly narrow types), the voting scheme (only 1 or 2 votes needed here), choosing which agent responses to show (here, the proportion of previously accepted messages from this agent), and so on. That choice of which responses to show is particularly tricky, as with this scheme a very domain specific agent might get voted down too much initially and never be chosen when the appropriate time comes. One potentially interesting alternative would be to let the crowd workers choose which agent\u0026rsquo;s response to see, and possibly even post-edit slightly.\nNote - This post is the first of a (hopefully) regular series again. However, rather than keeping it weekday-ly, I plan to do three times a week, at least until the ACL deadline.\nCitation  Paper\n@InProceedings{blah, title = {Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time}, author = {Huang, Ting-Hao (Kenneth) and Chang, Joseph Chee and Bigham, Jeffrey P.}, booktitle = {CHI}, year = {2018}, url = {https://www.cs.cmu.edu/~tinghaoh/pdf/2018/2018_chi.pdf}, }  ","date":1517173280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517173280,"objectID":"3ef1d6fafb956ab3a909ab61e11e68e5","permalink":"https://www.jkk.name/post/2018-01-28_crowdassistant/","publishdate":"2018-01-28T16:01:20-05:00","relpermalink":"/post/2018-01-28_crowdassistant/","section":"post","summary":"For a more flexible dialogue system, use the crowd to propose and vote on responses, then introduce agents and a model for voting, gradually learning to replace the crowd.","tags":["paper","chi","crowdsourcing","dialogue"],"title":"Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time (Huang et al., 2018)","type":"post"},{"authors":null,"categories":[],"content":"To construct word vectors from multi-domain data, use a separate vector for each domain and add a loss term to encourage them to agree. Here the loss is an l2 norm, weighted by a factor that depends on the frequency of the words in the two domains. The factor is the harmonic mean of the normalised frequency in each domain (so the lower frequency dominates the factor, pulling it lower). Across a range of tasks this consistently performs better than other approaches.\nCitation  Paper\n@InProceedings{yang-lu-zheng:2017:EMNLP2017, author = {Yang, Wei and Lu, Wei and Zheng, Vincent}, title = {A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2898--2904}, url = {https://aclanthology.org/D17-1312} }  ","date":1513128340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513128340,"objectID":"de9b49faec2c89c2897aaf4b143fa06f","permalink":"https://www.jkk.name/post/2017-12-12_multidomainwordvector/","publishdate":"2017-12-12T20:25:40-05:00","relpermalink":"/post/2017-12-12_multidomainwordvector/","section":"post","summary":"To leverage out-of-domain data, learn multiple sets of word vectors but with a loss term that encourages them to be similar.","tags":["paper","emnlp","word-vectors"],"title":"A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings (Yang et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"It turns out that if the vectors learned by word2vec are projected into a plane they all point in the same direction. Also, the context vectors (which are part of the algorithm, but not retained afterwards) point the other way. When visualising with t-SNE this effect is not visible because of the way the space is warped to optimise the t-SNE objective.\nThis is surprising, and may seem problematic since it doesn\u0026rsquo;t fit our goals for what these vectors should be capturing. However, it doesn\u0026rsquo;t seem to impact downstream tasks, for example, GloVe does not have this property, and doesn\u0026rsquo;t seem to derive a great benefit from it.\nCitation  Paper\n@InProceedings{mimno-thompson:2017:EMNLP2017, author = {Mimno, David and Thompson, Laure}, title = {The strange geometry of skip-gram with negative sampling}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2873--2878}, url = {https://aclanthology.org/D17-1308} }  ","date":1513127734,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513127734,"objectID":"ca8b8aec06bc416c3c59cea31aa64e6e","permalink":"https://www.jkk.name/post/2017-12-12_wordvectorgeometry/","publishdate":"2017-12-12T20:15:34-05:00","relpermalink":"/post/2017-12-12_wordvectorgeometry/","section":"post","summary":"Surprisingly, word2vec (negative skipgram sampling) produces vectors that point in a consistent direction, a pattern not seen in GloVe (but also one that doesn't seem to cause a problem for downstream tasks).","tags":["paper","emnlp","word-vectors"],"title":"The strange geometry of skip-gram with negative sampling (Mimno et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Getting high quality annotations from crowdsourcing requires careful design. This paper looks at how one annotation a worker does can influence their next annotation, for example:\n When scoring translations, a good example may make the next one look worse in comparison For labeling tasks, we may expect a long sequence of the same label to be rare (the gambler\u0026rsquo;s fallacy)  To investigate this they fit a linear model with inputs (previous label, gold label, random noise) and see what the coefficients are. Across multiple tasks, there is a non-zero correlation with the previous label. Interestingly, there also seems to be a learning effect for good workers, where over time they become calibrated and show less sequence bias. Fortunately, there is a simple solution - for each worker, give every annotator their documents in a different random order! With that change, averaging over annotations should avoid this bias.\nCitation  Paper\n@InProceedings{mathur-baldwin-cohn:2017:EMNLP2017, author = {Mathur, Nitika and Baldwin, Timothy and Cohn, Trevor}, title = {Sequence Effects in Crowdsourced Annotations}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2860--2865}, url = {https://aclanthology.org/D17-1306} }  ","date":1512780549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512780549,"objectID":"5fc230f2b64d4af5a8ec6a8d85ec5ae6","permalink":"https://www.jkk.name/post/2017-12-08_crowdbias/","publishdate":"2017-12-08T19:49:09-05:00","relpermalink":"/post/2017-12-08_crowdbias/","section":"post","summary":"Annotator sequence bias, where the label for one item affects the label for the next, occurs across a range of datasets. Avoid it by separately randomise the order of items for each annotator.","tags":["paper","emnlp","annotation"],"title":"Sequence Effects in Crowdsourced Annotations (Mathur et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Word vectors are great for common words, but what about rare words? People can have a fairly good understanding of a word given only a few instances, but it\u0026rsquo;s fairly standard to turn all words with a frequency of less than 5 into UNK when learning word vectors.\nOne simple approach is to add up word vectors from the context of the rare word and use that as the representation. This paper proposes using a tweaked version of word2vec: keep vectors for frequent words fixed, increase the learning rate, use a fixed width context window, initialise with the additive approach, and only subsample by discarding frequent words. All of those make sense, though I am curious whether it would be better to just decrease subsampling or disable it entirely.\nThe results are mixed, with the improvement over the additive approach data dependent. That might partly reflect the tasks though - something downstream like POS tagging would have been interesting, particularly since the LSTM may already be capturing contextual information that covers what the additive approach has, but not what this adds. Ultimately this is not a solution to this problem, but it\u0026rsquo;s an idea to keep in mind.\nCitation  Paper\n@InProceedings{herbelot-baroni:2017:EMNLP2017, author = {Herbelot, Aur\\'{e}lie and Baroni, Marco}, title = {High-risk learning: acquiring new word vectors from tiny data}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {304--309}, url = {https://aclanthology.org/D17-1030} }  ","date":1512697539,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512697539,"objectID":"1ab8b3efc541235a1c9f3de9747ef6b8","permalink":"https://www.jkk.name/post/2017-12-07_rarewordvectors/","publishdate":"2017-12-07T20:45:39-05:00","relpermalink":"/post/2017-12-07_rarewordvectors/","section":"post","summary":"The simplest way to learn word vectors for rare words is to average their context. Tweaking word2vec to make greater use of the context may do slightly better, but it's unclear.","tags":["paper","emnlp","word-embeddings"],"title":"High-risk learning: acquiring new word vectors from tiny data (Herbelot et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Selectional preferences in this context are about how some verbs are more likely to take certain types of arguments (e.g. people laugh, computers do not). Many papers have added features or structures to coreference systems aiming to get at this kind of information. This paper presents another way of doing it and experiments that probe how useful it is (punchline: not very).\nTheir approach is to parse a large amount of text, producing noun-verb pairs. They learn vector representations of the relations and try to create a single space containing both entities and relations (e.g. Michigan gets a vector, as does attended@dobj). The goal is that entities end up in locations similar to the locations of relations they are selected for.\nFor results, first it seems like these vector similarities do not correlate particularly strongly with being coreferent. It could be that the feature on its own isn\u0026rsquo;t enough, or this representation might not be capturing it effectively. Adding this to the Stanford coreference system they are able to get slight gains, though the improvement might not be statistically significant.\nI\u0026rsquo;m not sure exactly how to do this, but it would be neat if a vector at some point of the model could be modified to remove any correlation with these features, and see what that does to performance. If performance remains high, then this actually is an uninformative feature, but if it drops that suggests the model is already learning it.\nCitation  Paper\n@InProceedings{heinzerling-moosavi-strube:2017:EMNLP2017, author = {Heinzerling, Benjamin and Moosavi, Nafise Sadat and Strube, Michael}, title = {Revisiting Selectional Preferences for Coreference Resolution}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {1332--1339}, url = {https://aclanthology.org/D17-1138} }  ","date":1512605120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512605120,"objectID":"8b1a0fb74986fa68aebd29533730e196","permalink":"https://www.jkk.name/post/2017-12-06_coreferencearguments/","publishdate":"2017-12-06T19:05:20-05:00","relpermalink":"/post/2017-12-06_coreferencearguments/","section":"post","summary":"It seems intuitive that a coreference system could benefit from information about what nouns a verb selects for, but experiments on explicitly adding a representation of it to a neural system does not lead to gains, implying it is already learning them or they are not useful.","tags":["paper","emnlp","coreference"],"title":"Revisiting Selectional Preferences for Coreference Resolution (Heinzerling et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"One reason learning for semantic parsing is difficult is that the datasets are generally small. Assuming some words behave similarly across domains, multi-domain parsing should improve performance by providing more data, which is essentially what this paper finds. They consider several configurations, all based on a sequence to sequence LSTM:\n Train a separate model for every domain. Use a single model. They do three subtypes here, (a) that\u0026rsquo;s it, (b) add an LSTM input at each step with the domain, (c) give the domain as a token at the start. Use a single encoder model, but a different decoder for each domain. Combine (1) and (3), have two encoders, one that is domain specific and one that is trained on all domains.  The results show that any of these does better than (1), with (2b) doing best. There also seems to be three sections: first the independent models (1), then the models with multiple decoders (3 and 4), then the variants of (2). A natural thing to try would be a version of (4) with a single decoder, in which case the thing that is shared is the output space representation (rather than the input space as the motivation for the paper frames it). From the paper it sounds like very little hyperparameter tuning was tried, which is a shame because it makes it less clear how definitive the results are.\nCitation  Paper\n@InProceedings{herzig-berant:2017:Short, author = {Herzig, Jonathan and Berant, Jonathan}, title = {Neural Semantic Parsing over Multiple Knowledge-bases}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {623--628}, url = {https://aclanthology.org/P17-2098} }  ","date":1512520113,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512520113,"objectID":"5aac6380f93a5cf07433c1b30ab7f78b","permalink":"https://www.jkk.name/post/2017-12-05_multidomainparsing/","publishdate":"2017-12-05T19:28:33-05:00","relpermalink":"/post/2017-12-05_multidomainparsing/","section":"post","summary":"Training a single parser on multiple domains can improve performance, and sharing more parameters (encoder and decoder as opposed to just one) seems to help more.","tags":["paper","acl","semantic-parsing"],"title":"Neural Semantic Parsing over Multiple Knowledge-bases (Herzig et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Interpreting the behaviour of statistical models in NLP has been hard for a long time, but it has gotten even harder with nonlinear models. The simplest method so far in NLP has been to look at the attention distributions in sequence to sequence models, but that doesn\u0026rsquo;t provide everything we need and obviously only applies when the model has attention. For looking at the dynamics of the hidden state in an LSTM the Harvard NLP group built a cool visualisation, but what about structured outputs?\nThis paper considers sequence to sequence models and determines which parts of the input were most important for determining each part of the output. The steps are:\n Use a variational autoencoder to get perturbed versions of the input Use logistic regression to get scores for every output symbol indicating how sensitive it is to variations in parts of the input Create a bipartite graph between inputs and outputs, then find high weight components in the graph  These components serve as the representation of which parts of the input determine which parts of the output. Experiments show results that match with past observations and intuitions, which is good for supporting the effectiveness of the method, but it\u0026rsquo;s a shame this didn\u0026rsquo;t uncover any exciting new patterns.\nCitation  Paper\n@InProceedings{alvarezmelis-jaakkola:2017:EMNLP2017, author = {Alvarez-Melis, David and Jaakkola, Tommi}, title = {A causal framework for explaining the predictions of black-box sequence-to-sequence models}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {412--421}, url = {https://aclanthology.org/D17-1042} }  ","date":1512506445,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512506445,"objectID":"2547dad80fdbb6dcadbfaaca7c0b9296","permalink":"https://www.jkk.name/post/2017-12-05_explainingpredictions/","publishdate":"2017-12-05T15:40:45-05:00","relpermalink":"/post/2017-12-05_explainingpredictions/","section":"post","summary":"To explain structured outputs in terms of which inputs have most impact, treat it as identifying components in a bipartite graph where weights are determined by perturbing the input and observing the impact on outputs.","tags":["paper","emnlp","analysis"],"title":"A causal framework for explaining the predictions of black-box sequence-to-sequence models (Alvarez-Melis et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"The classic NER system is a model that has a lot of curated features, like lists of people, and does inference by choosing the top scoring tag sequence for the whole sentence, using Viterbi decoding. The neural version swaps the curated features for word vectors, and viterbi inference for an LSTM (maybe with beam search). This paper makes the argument that in reality people are very good at identifying an entity in isolation, so why do global decoding for the best tag sequence?\nGiven that perspective, they make a model that scores every span of the sentence independently using a feedforward network. To get an input representing context, they use a weighted sum of word embeddings, where the weights decay exponentially further from the span (FOFE = Fixed-size Ordinally Forgetting Encoding). The authors point out that this gives a fixed length encoding that could be reversed to recover the original sequence (assuming arbitrary precision floating point numbers). Thinking about the calculation though, a word ten positions away is having its vector scaled down by a factor of a thousand, so it probably has negligible impact on the decision. They also apply this idea to the characters of the span itself in both directions.\nOne tradeoff with the independent classification idea is that it can select overlapping spans. This is a benefit in one sense, because it naturally handles nested entities (e.g. \u0026ldquo;[Member of the Order of [Australia]]\u0026quot;), but for partially overlapping spans we have to decide which to keep. Their solution is to sort by model score and keep the higher scoring option.\nThe experiments show this is comparable with previous work using LSTMs. There were a few things I found interesting in the results:\n The FOFE encoding for characters is far worse than a CNN encoding when on their own, but give similar gains when combined with word level features. Since the FOFE essentially ignores the centre of long spans, this suggests they are both learning some representation of prefixes and suffixes. They don\u0026rsquo;t try it, but this model seems very amenable to gazetteers, which may be a way to further boost performance. They have an in-house dataset of 10,000 manually labeled documents (!), but it only gives a 3% gain on the KBP evaluation.  Citation  Paper\n@InProceedings{xu-jiang-watcharawittayakul:2017:Long, author = {Xu, Mingbin and Jiang, Hui and Watcharawittayakul, Sedtawut}, title = {A Local Detection Approach for Named Entity Recognition and Mention Detection}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1237--1247}, url = {https://aclanthology.org/P17-1114} }  ","date":1512160139,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512160139,"objectID":"a078dfd54785947bb7847f7f1e01f6cf","permalink":"https://www.jkk.name/post/2017-12-01_nonsequencener/","publishdate":"2017-12-01T15:28:59-05:00","relpermalink":"/post/2017-12-01_nonsequencener/","section":"post","summary":"Effective NER can be achieved without sequence prediction using a feedforward network that labels every span with a fixed attention mechanism for getting contextual information.","tags":["paper","acl","ner"],"title":"A Local Detection Approach for Named Entity Recognition and Mention Detection (Xu et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"This paper considers the task of identifying named entities in a sentence and the relations between them. The contribution is a way of formulating the task as tagging, so a bi-directional LSTM can be applied.\nThe tags are like in NER (Begin, Inside, End, Single, Outside), but rather than Person, Location, etc, they label each entity with the relation it is participating in, and whether it is in role one or two for the relation. Applying a two layer bidirectional LSTM to this set up gets to state-of-the-art precision on news data. To get SotA F-score they modify the loss to place less weight on Outside tags, which raises recall at the cost of precision.\nOne catch with this approach is handling multiple relations of the same type. The solution here is to link pairs that are closest together (unclear what they do for nesting). That doesn\u0026rsquo;t handle overlapping relations, which the authors say is particularly common in the BioInfer data (I\u0026rsquo;m curious how much it is hurting here too). It\u0026rsquo;s unclear how this could be addressed without a radical redesign, since extending the tag scheme could lead to sparsity issues.\nI was not familiar with this data, so I looked back to the original paper the annotated test data came from: Hoffman et al., (2011). There is no dev set, only a 395 sentence test set, so the standard practise is to use random 10% samples of the test data for development. Also, if I understand it correctly, the data was annotated by manually confirming the output of systems, which means it will have recall errors. If interest in this data grows, going back and annotating more seems worthwhile.\nCitation  Paper\n@InProceedings{zheng-EtAl:2017:Long, author = {Zheng, Suncong and Wang, Feng and Bao, Hongyun and Hao, Yuexing and Zhou, Peng and Xu, Bo}, title = {Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1227--1236}, url = {https://aclanthology.org/P17-1113} }  ","date":1512090101,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512090101,"objectID":"1f6baf7c51dd522c844cb6886e49dd3d","permalink":"https://www.jkk.name/post/2017-11-30_taggingrelations/","publishdate":"2017-11-30T20:01:41-05:00","relpermalink":"/post/2017-11-30_taggingrelations/","section":"post","summary":"By encoding the relation type and role of each word in tags, an LSTM can be applied to relation extraction with great success.","tags":["paper","acl","relation-extraction"],"title":"Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme (Zheng et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Most effective summarisation systems are extractive, selecting the most important sentences in a document and sticking them together. Clearly that is not how people write summaries, but creating abstractive summaries means generating fluent language. At the same time, most datasets are based on news text, where the first few sentences are a strong baseline summary (by design, as journalists need to assume that the reader could stop at any point). This paper introduces several ideas to get state-of-the-art results on summarisation using an abstractive system.\nThere are three core new ideas, one for decoding and two for the model. The idea in decoding is a beam search in which the score is increased when adding bigrams that occur in the source but are not in the output. In the model, they propose a new form of attention based on PageRank, similar to previous methods used for ranking sentences in summarisation. For every pair of sentences plus the current decoder hidden vector, a similarity score is calculated ($h_1 M h_2$), where $M$ is a matrix of parameters. This produces a matrix of similarities, which they run PageRank on with initialisation set so that all weight starts on the decoder hidden vector. That produces a score for each input sentence, which is normalised to get attention values. The second idea is that they don\u0026rsquo;t want to attend to the same sentence multiple times, so before normalising they subtract the previous score for that sentence (with it capped at 0 to avoid negative values).\nTogether, these lead to state of the art results, beating both extractive and abstractive systems. Though in human evaluation using the first three sentences as a summary remains a very strong baseline, only slightly behind this system on informativeness and ahead on coherence and fluency. Ablation shows that the decoding idea has the biggest impact, but the graph based attention does help. Interestingly, if the score in decoding is extremely biased to focus on the bigram addition aspect performance only decreases a little. That may reflect the nature of the metric, which is based on ngram overlap.\nThere are also a bunch of little details that may be crucial, like adding markers for entities (which seems like a possible space for a more elegant solution). I\u0026rsquo;m not sure the beam search scoring idea has applications beyond summarisation, but thee modified attention might!\nCitation  Paper\n@InProceedings{tan-wan-xiao:2017:Long, author = {Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo}, title = {Abstractive Document Summarization with a Graph-Based Attentional Neural Model}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1171--1181}, url = {https://aclanthology.org/P17-1108} }  ","date":1512000845,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512000845,"objectID":"9e94897d0270664271d733c8e96f618a","permalink":"https://www.jkk.name/post/2017-11-29_abstractivesummarisation/","publishdate":"2017-11-29T19:14:05-05:00","relpermalink":"/post/2017-11-29_abstractivesummarisation/","section":"post","summary":"Neural abstractive summarisation can be dramatically improved with a beam search that favours output that matches the source document, and further improved with attention based on PageRank, with a modification to avoid attending to the same sentence more than once.","tags":["paper","acl","summarisation"],"title":"Abstractive Document Summarization with a Graph-Based Attentional Neural Model (Tan et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"The first step in almost any neural network model for language is to look up a vector for each token in the input. These vectors express relations between the words, but it is difficult to know exactly what relations. This work proposes a way to modify a vector space of words to have more interpretable dimensions.\nThe core idea is actually more general, it is a new loss that encourages sparsity in an auto-encoder. In this case the model is very simple: input a word vector, apply an affine transformation and a pointwise nonlinearity, producing a hidden vector, then apply another affine transformation to get the output. The loss is a combination of how well the input and output match (reconstruction loss), plus a function that is minimised when the average activation is below a threshold (average sparsity loss), and the new idea, a loss that is minimised at either 0 or 1 for each hidden value. To get the hidden values to be bounded between 1 and 0, the nonlinearity used is a modified ReLU that stops increasing after reaching 1. After training, the hidden values become the new word vectors.\nTo evaluate interpretability they consider the top 4 words along each dimension, add a random word, and ask a person to identify the odd word out. Using either word2vec or GloVe as the initial vectors and applying this method, the results shown a dramatic difference (~25 vs. ~70). On downstream tasks the story is more mixed. With 1,000 dimensional vectors, there is usually an improvement for GloVe, but not for word2vec, and the differences are generally small. Apparently going up to 2,000 further improves interpretability scores, but \u0026lsquo;at a severe cost\u0026rsquo; for the downstream tasks. Going the other direction, to 500, hurts interpretability, and probably doesn\u0026rsquo;t improve downstream performance (it isn\u0026rsquo;t mentioned).\nI would be curious to see if taking these new word vectors and applying them to a downstream task like parsing, but letting them change during training, would be beneficial. The general idea of a sparse auto-encoder also seems cool and may have other applications.\nCitation  ArXiv Paper\n@ARTICLE{2017arXiv171108792S, author = {{Subramanian}, A. and {Pruthi}, D. and {Jhamtani}, H. and {Berg-Kirkpatrick}, T. and {Hovy}, E.}, title = {SPINE: SParse Interpretable Neural Embeddings}, journal = {ArXiv e-prints}, archivePrefix = {arXiv}, eprint = {1711.08792}, primaryClass = {cs.CL}, year = {2017}, month = {November}, url = {https://arxiv.org/abs/1711.08792}, }  ","date":1511905869,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511905869,"objectID":"505cebbc8e9aa71e4e0f1da80d72a69b","permalink":"https://www.jkk.name/post/2017-11-28_interpretableembeddings/","publishdate":"2017-11-28T16:51:09-05:00","relpermalink":"/post/2017-11-28_interpretableembeddings/","section":"post","summary":"By introducing a new loss that encourages sparsity, an auto-encoder can be used to go from existing word vectors to new ones that are sparser and more interpretable, though the impact on downstream tasks is mixed.","tags":["paper","arxiv","word-vectors"],"title":"SPINE: SParse Interpretable Neural Embeddings (Subramanian et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"When people read a sentence they form an entire world around it, making inferences about unwritten properties based on their prior knowledge. If we want NLP systems to do the same, we need data to train and test this common sense aspect of language understanding.\nThis paper is about a new dataset of automatically generated sentence pairs with human ratings. The ratings indicate that given the first sentence, the second sentence is either very likely, likely, plausible, technically possible, or impossible. These ratings are crowdsourced, using the median of three ratings per example. The pay rates are fairly low, at $3.45 / hour (1.99c / example and 20.71 seconds / example), though it\u0026rsquo;s possible that the time is being skewed by outliers, and it\u0026rsquo;s unclear exactly how pay was determined (does this include Amazon\u0026rsquo;s cut? Why is it an average cost per example, rather than just the cost?).\nThe main contribution is the novel way of generating the sentences. For each prompt sentence, an argument is chosen, and then a hypothesis is generated in one of three ways (all trained with Gigaword). (1) A sequence-to-sequence model takes the full sentence as input and generates a sentence. (2) The same as (1), but with only the argument provided. (3) A sentence is sampled from templates generated by abstraction of sentences in the training data. Together these produce a diverse set of examples that get a range of ratings, with only \u0026lsquo;likely\u0026rsquo; being somewhat rarer. They also labeled some pairs from SNLI and COPA, to enable analysis of how this task compares.\nThey also provide a set of baselines for the new task. Using the baselines, they show that the generated sentences are somewhat more difficult than the pairs from existing datasets. The standard metrics proposed are MSE and Spearman\u0026rsquo;s Rho (both necessary because otherwise always guessing the middle would get an MSE better than any of the proposed baselines). Interestingly, regression does quite a bit better than a set of one-vs-all SVMs on MSE, and also slightly better on rho (I\u0026rsquo;m surprised because while there is an ordinal scale, it doesn\u0026rsquo;t feel like it should have a strong continuous interpretation).\nCitation  Paper\n@article{TACL1082, author = {Zhang, Sheng and Rudinger, Rachel and Duh, Kevin and Van Durme, Benjamin }, title = {Ordinal Common-sense Inference}, journal = {Transactions of the Association for Computational Linguistics}, volume = {5}, year = {2017}, keywords = {}, issn = {2307-387X}, url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1082}, pages = {379--395} }  ","date":1511799705,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511799705,"objectID":"8606e2e2893fd4349ff0332d4f6be23b","permalink":"https://www.jkk.name/post/2017-11-27_commonsense/","publishdate":"2017-11-27T11:21:45-05:00","relpermalink":"/post/2017-11-27_commonsense/","section":"post","summary":"A new task and dataset of 39k examples for common sense reasoning, with a sentence generated for each prompt and a manual label indicating their relation, from very likely to impossible.","tags":["paper","tacl"],"title":"Ordinal Common-sense Inference (Zhang et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"This work presents a system that parses sentences and identifies grammatical errors simultaneously. It\u0026rsquo;s an intuitive combination - a syntactic model should assign higher probability to a parse for a fixed version of a sentence than the one with a mistake.\nThey build on an incremental \u0026lsquo;easy-first\u0026rsquo; dependency parsing approach. Easy-First parsing starts with the set of words in the sentence and allows an edge to be created between any adjacent pair of words. Once an edge is created, the child is hidden beneath its parent, so now the parent is effectively adjacent to a word slightly further away. Then the process repeats, until there is only one word left (the root of the sentence). In a way it is like following a dynamic program, but with only a single state that ties together multiple cells.\nThe change in this paper is the addition of actions that insert a word, delete a word, or alter a word. To make it work, there are constraints to avoid cycles of repeated actions (e.g. insert-delete-insert-delete\u0026hellip;), and on the sets of allowed word substitutions. To produce additional training data, a tool is used to inject errors into grammatical text. On error detection, this approach does lead to improvements, though it changes a relatively small number of the sentences. On dependency parsing it is (unsurprisingly) worse than a baseline system on grammatical text. It does perform better on ungrammatical text, though the data is generated using the same process as the training data, creating a bias in the system\u0026rsquo;s favour.\nCitation  Paper\n@InProceedings{sakaguchi-post-vandurme:2017:Short, author = {Sakaguchi, Keisuke and Post, Matt and Van Durme, Benjamin}, title = {Error-repair Dependency Parsing for Ungrammatical Texts}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {189--195}, url = {https://aclanthology.org/P17-2030} }  ","date":1511383733,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511383733,"objectID":"8fd8a3d517f2e9150e9a874bfd71b183","permalink":"https://www.jkk.name/post/2017-11-22_errorrepairparsing/","publishdate":"2017-11-22T15:48:53-05:00","relpermalink":"/post/2017-11-22_errorrepairparsing/","section":"post","summary":"Grammatical error correction can be improved by jointly parsing the sentence being corrected.","tags":["paper","acl","error-correction"],"title":"Error-repair Dependency Parsing for Ungrammatical Texts (Sakaguchi et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Attention, a weighted average over vectors with weights determined based on context (usually decoder state), has proven effective in many NLP tasks. There are several variants, and this paper adds new types that address the question of how to apply attention to different sources at the same time, such as text and an image.\nThey consider three general versions:\n Concatenation, just do attention separately then concatenate the vectors from the input sources Flat, do the weighted average over all of the inputs Hierarchical, do attention separately, but then combine the vectors with another phase of attention  They also explore two variations that are orthogonal to the list above:\n The first step and the last step in attention both involve the input vectors being multiplied by a weight matrix. Should that matrix be shared for the two steps, or different? (the first informs the decision of what to give high weight in the average, the second determines what is being averaged over) sentinel gates, a modification to the way the inputs and context vector are combined that allow one or the other to be ignored.  They consider two tasks, (1) translation when both an image and source sentence are given, (2) post-editing a translated sentence with the original source given. The results show fairly clear trends, though the systems are not great compared to baselines (worse than a text only baseline for the first, and only slightly better than a direct MT system for the second). The trends are that hierarchical is best, the sentinel doesn\u0026rsquo;t help, and it is better to not share weights (though I wonder if that would be true when controlling for the total number of parameters).\nCitation  Paper\n@InProceedings{libovicky-helcl:2017:Short, author = {Libovick\\'{y}, Jind\\v{r}ich and Helcl, Jind\\v{r}ich}, title = {Attention Strategies for Multi-Source Sequence-to-Sequence Learning}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {196--202}, url = {https://aclanthology.org/P17-2031} }  ","date":1511300539,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511300539,"objectID":"7103ce382f622565f31a7cba82c48962","permalink":"https://www.jkk.name/post/2017-11-21_multiinputattention/","publishdate":"2017-11-21T16:42:19-05:00","relpermalink":"/post/2017-11-21_multiinputattention/","section":"post","summary":"To apply attention across multiple input sources, it is best to apply attention independently and then have a second phase of attention over the summary vectors for each source.","tags":["paper","acl","neural-network"],"title":"Attention Strategies for Multi-Source Sequence-to-Sequence Learning (Libovicky et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Like the UCCA parser, this paper explores a transition-based neural model for semantic parsing, but for Minimal Recursion Semantics instead of Universal Conceptual Cognitive Annotation. Comparing MRS and UCCA, every word gets a non-terminal symbol in MRS, plus additional non-terminals for phenomena like quantification, while UCCA only introduces them for special cases like linking to a coordination. Both have discontinuous graph structures, creating a challenge for most parsers.\nThe UCCA and MRS parsers extend the basic shift-reduce transitions in different ways. Here, crossing edges can be added with a transition that forms edges between the front of the buffer and a word anywhere in the stack, while the UCCA parser used swapping and a additional reduce actions for graph edges. The models are similar, both using a form of stack-RNN, but with different structures (partly as a result of the different transition schemes). The results in this case are not state-of-the-art, though this task has received more attention, and the data is slightly biased (the parser that does better, ACE, is based on the grammar that was used to determine which sentences to include). However, the system can also be applied to AMR, and does fairly well, better than other neural AMR parsers at the time (and more recent ideas for improvements are large orthogonal).\nCitation  Paper\n@InProceedings{buys-blunsom:2017:Long, author = {Buys, Jan and Blunsom, Phil}, title = {Robust Incremental Neural Semantic Graph Parsing}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1215--1226}, url = {https://aclanthology.org/P17-1112} }  ","date":1511190792,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511190792,"objectID":"71ab4f1e666613675c7013d14bff10c2","permalink":"https://www.jkk.name/post/2017-11-20_mrsparser/","publishdate":"2017-11-20T10:13:12-05:00","relpermalink":"/post/2017-11-20_mrsparser/","section":"post","summary":"A neural transition based parser with actions to create non-local links can perform well on Minimal Recursion Semantics parsing.","tags":["paper","acl","semantic-parsing"],"title":"Robust Incremental Neural Semantic Graph Parsing (Buys et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Discourse parsing for Rhetorical Structure Theory is difficult partly because it involves a range of relation types at different scales (within and between sentences) and partly because there is relatively little annotated data available. To deal with the limited data, this paper breaks the task into two parts: (1) identify relations, (2) assign labels. Their system is state-of-the-art, and an ablation shows that the division of tasks helps performance. They also divide up the labeling step to have different classifiers for within sentences, between sentences in the same paragraph, and between paragraphs, which also helps a little.\nI find the second improvement surprising, since an expanded feature set for a single classifier would be able to emulate their multi-classifier model, while having the advantage of sharing information between classes. The first improvement is more intuitive (a denser space makes for an easier problem), though I wonder whether this will be one point on the back-and-forth that usually occurs between sequential and joint models (with joint models usually winning in the end). This paper also continues the trend of transition-based inference applying effectively to tasks, which makes sense if our models are getting good enough that search errors are not a major issue.\nCitation  Paper\n@InProceedings{wang-li-wang:2017:Short, author = {Wang, Yizhong and Li, Sujian and Wang, Houfeng}, title = {A Two-Stage Parsing Method for Text-Level Discourse Analysis}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {184--188}, url = {https://aclanthology.org/P17-2029} }  ","date":1510962020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510962020,"objectID":"92cb889610fc6b345189d999d953830b","permalink":"https://www.jkk.name/post/2017-11-17_twostagediscourseparsing/","publishdate":"2017-11-17T18:40:20-05:00","relpermalink":"/post/2017-11-17_twostagediscourseparsing/","section":"post","summary":"Breaking discourse parsing into separate relation identification and labeling tasks can boost performance (by dealing with limited training data).","tags":["paper","acl","discourse"],"title":"A Two-Stage Parsing Method for Text-Level Discourse Analysis (Wang et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Over the last few years interest has risen in parsing structures other than projective trees (including my dissertation!). There are now a range of different datasets with annotations for syntactic and/or semantic structure that include discontinuous constituents and graphs. This paper looks at UCCA, a proposed formalism that is somewhat similar to SRL, with non-terminals included to allow for easier handling of cases like coordination.\nThe parser is a transition based, with a transition system that covers all the structural phenomena in UCCA: non-terminals, discontinuous spans, and multiple parents. The key to consistent multiple parents is distinguishing the addition of edges that are the primary parent (to prevent multiple being added). To get discontinuity, they use a swap operation. They consider a range of models, including both linear and neural network examples.\nThe dataset is relatively small, with only 4,268 training sentences, and the task is hard, so performance is relatively low (50 - 75 for primary edges, 20-50 for others). The neural model consistently beats the linear ones, particularly for the non-primary edges. Comparing to other standard parsers (retrained on this data), the ability to generate the full space of structures makes a big difference.\nIt would be interesting to see coverage of this data for one-endpoint crossing graphs. If it is high, then my own parser could be applied fairly directly!\nCitation  Paper\n@InProceedings{hershcovich-abend-rappoport:2017:Long, author = {Hershcovich, Daniel and Abend, Omri and Rappoport, Ari}, title = {A Transition-Based Directed Acyclic Graph Parser for UCCA}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1127--1138}, url = {https://aclanthology.org/P17-1104} }  ","date":1510871099,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510871099,"objectID":"41442baedb69537b7780164de88cd200","permalink":"https://www.jkk.name/post/2017-11-16_ucca/","publishdate":"2017-11-16T17:24:59-05:00","relpermalink":"/post/2017-11-16_ucca/","section":"post","summary":"Parsing performance on the semantic structures of UCCA can be boosted by using a transition system that combines ideas from discontinuous and constituent transition systems, covering the full space of structures.","tags":["paper","acl","syntax"],"title":"A Transition-Based Directed Acyclic Graph Parser for UCCA (Hershcovich et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Since word2vec was released there have been a series of X2vec papers, though none have had the success of word vectors. In this case the idea is to represent entities and chunks of text (words, sentences, paragraphs).\nEntities are represented with vectors. To get the vector for a chunk of text, they:\n Sum word vectors for the text. Rescale to be of unit length. Multiply by a weight matrix and add a bias.  Then to learn these, negative log likelihood is used, where the probability is defined as a softmax over the dot product between entity and text vectors. The data is a portion of Wikipedia annotated with entities as indicated by links (plus they say the entity the page is about is implicitly part of every sentence).\nWith these new vectors in hand, they try textual similarity, with strong results. They also build a very simple entity linking system, a feed-forward network with these representations plus a few other features, and beat all prior work. Similarly They apply the same modeling approach to Quizball QA, also with strong results.\nThe simplicity and effectiveness of the model really is impressive. Some qualitative examples are included, but hard to find trends in. It does seem like a more reasonable vector learning approach than skip-thought and other similar approaches that rely only on text context - the entities provide something different, but clearly closely related. That said, I feel like more ablation is needed to see what role each of these pieces is playing (are they learning better vectors, or using them in a way that is more effective? Or both?).\nCitation  Paper\n@article{TACL1065, author = {Yamada, Ikuya and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu }, title = {Learning Distributed Representations of Texts and Entities from Knowledge Base}, journal = {Transactions of the Association for Computational Linguistics}, volume = {5}, year = {2017}, issn = {2307-387X}, url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1065}, pages = {397--411} }  ","date":1510786887,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510786887,"objectID":"f6885ee882016f8af93335a228b74cd0","permalink":"https://www.jkk.name/post/2017-11-15_entityvectors/","publishdate":"2017-11-15T18:01:27-05:00","relpermalink":"/post/2017-11-15_entityvectors/","section":"post","summary":"Vectors for words and entities can be learned by trying to model the text written about the entities. This leads to word vectors that score well on similarity tasks and entity vectors that produce excellent results on entity linking and question answering.","tags":["paper","tacl","word-vectors"],"title":"Learning Distributed Representations of Texts and Entities from Knowledge Base (Yamada et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Shift-reduce constituency parsing incrementally builds the parse either bottom-up or top-down. The difference is whether a non-terminal is placed on the stack before or after the words that it spans. This corresponds to two forms of depth-first traversal of the tree: pre-order or post-order.\nThe idea in this paper is to do an in-order traversal, which in a binary tree means traversing the left child of a node, then the node, then its right child. In this context that means putting the non-terminal symbol on the stack after the first word it spans, but before the rest. The model follows the stack-LSTM approach of Dyer et al., with non-terminals always fed into the LSTM first during composition, regardless of where it was inserted into the stack.\nThis leads to a 0.5 F1 gain on standard parsing metrics, with no hyperparameter tuning. High-level error analysis seems to show it just does better everywhere. I wonder whether further gains could be realised with a label-sensitive ordering.\nCitation  Paper\n@article{TACL1199, author = {Liu, Jiangming and Zhang, Yue }, title = {In-Order Transition-based Constituent Parsing}, journal = {Transactions of the Association for Computational Linguistics}, volume = {5}, year = {2017}, issn = {2307-387X}, url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1199}, pages = {413--424} }  ","date":1510686639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510686639,"objectID":"0b190c49e98059cce655601557b1f2f9","permalink":"https://www.jkk.name/post/2017-11-14_inorderparsing/","publishdate":"2017-11-14T14:10:39-05:00","relpermalink":"/post/2017-11-14_inorderparsing/","section":"post","summary":"Using in-order traversal for transition based parsing (put the non-terminal on the stack after its first child but before the rest) is consistently better than pre-order / top-down or post-order / bottom-up traversal.","tags":["paper","tacl","syntax"],"title":"In-Order Transition-based Constituent Parsing (Liu et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"In reference games, two players communicate in a shared world with the goal of one learning what the other is referring to. Their small scale and clear success criteria make them a convenient testbed for dialogue agents, going back decades, with recent work focusing on neural approaches. This paper considers a simple game and constrains models in various ways to improve performance and see how their communication varies, a line of work also appearing in recent papers by Jacob Andreas ( ACL 2017, EMNLP 2017).\nThe game in this case is to find out two properties of an object, where there are three possible properties, each with four possible values. Given enough flexibility, models will explicitly encode every possible structure of the world as a separate symbol, which does not generalise well. Limiting the vocabulary to one symbol per property and one per value helps, but in this particular game there are only 3 possible questions, and over two turns of dialogue the 12 value words are sufficient to encode the space. Limiting even further, to 4 words for values and providing each turn in isolation to the answerer does lead to some compositionality, but clearly not full compositionality as they still make errors on unseen combinations of the inputs.\nIt\u0026rsquo;s a short paper, so they can only do so much, but some experiments I am curious about are:\n Decrease the questioner vocabulary to 2. This avoids the problem that the questioner can express the task in one step by saying what is not needed. It\u0026rsquo;s still doable, by defining an order for questions, e.g. ask about attribute A vs. B first, then in the second step ask about either C or the other option from the first step. This is a little weird as symbols need to mean different things at different time steps, but would be interesting. Increase the number of attributes to 4. This also avoids the task expression problem, by forcing there to be compositionality on the questioner side (watching the video of the talk, someone asked this in the question time, and they didn\u0026rsquo;t know).  Citation  Paper\n@InProceedings{kottur-EtAl:2017:EMNLP2017, author = {Kottur, Satwik and Moura, Jos\\'{e} and Lee, Stefan and Batra, Dhruv}, title = {Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2952--2957}, url = {https://aclanthology.org/D17-1320} }  ","date":1510584428,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510584428,"objectID":"3d1e861d246a1b50b574036b117fb5a0","permalink":"https://www.jkk.name/post/2017-11-13_languagegame/","publishdate":"2017-11-13T09:47:08-05:00","relpermalink":"/post/2017-11-13_languagegame/","section":"post","summary":"Constraining the language of a dialogue agent can improve performance by encouraging the use of more compositional language.","tags":["paper","emnlp","grounded-language"],"title":"Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog (Kottur et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Structured information sources have been effectively used for entity detection and typing in linear models with the information encoded as binary features. This paper looks at how to integrate vector representations of structured information into an LSTM. The solution is an additional processing step during output generation, in which the vectors for relevant entities in the structured data are combined with the standard LSTM output (note, they do not affect the cell itself, so the information is not passed on through the recurrence).\nIn this case the structured information is a set of tuples forming a graph of relations between entities, from either NELL or WordNet. The actual encoding of entities is an application of prior work; vectors representing tuples are trained with the objective that the score for any tuple is higher than made-up tuples (where the score is $v_a M_r v_b$ for entities $a$ and $b$ in relation $r$). The set of relevant entities for a particular word in the sentence is obtained by string matching, and then attention is used to combine them. There is also a kind of gating mechanism to choose how big a role the entities play in the prediction, using a combination of the input, hidden state, and cell state.\nThe results are interesting not only because this method helps, but because of how well the standard LSTM does on this task, matching or exceeding prior results. This is even more impressive given how small ACE is (if I remember correctly). The other key observations are that having a sequence level loss (using a CRF) helps, and NELL and WordNet seem to be providing different types of information (as using both leads to further improvements).\nCitation  Paper\n@InProceedings{yang-mitchell:2017:Long, author = {Yang, Bishan and Mitchell, Tom}, title = {Leveraging Knowledge Bases in LSTMs for Improving Machine Reading}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1436--1446}, url = {https://aclanthology.org/P17-1132} }  ","date":1510346235,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510346235,"objectID":"e86a4765e6d7213554b4bd65c69f0150","permalink":"https://www.jkk.name/post/2017-11-10_kginlstm/","publishdate":"2017-11-10T15:37:15-05:00","relpermalink":"/post/2017-11-10_kginlstm/","section":"post","summary":"Incorporating vector representations of entities from structured resources like NELL and WordNet into the output of an LSTM can improve entity and event extraction.","tags":["paper","acl","knowledge-graph"],"title":"Leveraging Knowledge Bases in LSTMs for Improving Machine Reading (Yang et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Another paper about a dataset of dialogues, but this time with structure. Like the paper from yesterday, the aim is a dataset of task-oriented conversations, but with more complexity than prior work. The difference is that this work includes a structured representation of the state of the conversation: frames.\nA frame is essentially a tuple describing a query, e.g. (Destination: Sydney, Origin: Ann Arbor, price: 1500 USD). There are multiple frames in a dialogue (a departure from DSTC tasks), and utterances are labeled with dialogue acts that capture modifications to the frames as well as references to them. This structure sounds fairly general, though the focus here was on vacation planning, where the user is buying a package. The setup doesn\u0026rsquo;t maximise the potential complexity though, as there are a small number of set packages available, rather than the complex tradeoffs of flight+hotel combinations that exist in practise. Looking at the example dialogues in the paper, it has complete sentences of some complexity. One thing I\u0026rsquo;m still curious about is disagreements between annotators, as for the complete task the score was 0.62 +/- 5 (with dialogue acts being trickier than slot values, and no scores for frame references on their own).\nComparing to the Stanford dataset this is smaller (11k vs. 1.4k), but has more turns per dialogue (11 vs. 15) and probably longer turns too, judging by the examples. The tasks are completely different, but both come with small tables of information that are private to the two participants and required for almost every turn in the conversation. Evaluating on both could be a great way to show the flexibility of a dialogue system, but the lack of frames for the Stanford data and the difficulty of running a human evaluation for this data limits the feasible types of multi-domain experiments.\nCitation  Paper\n@InProceedings{elasri-EtAl:2017:W17-55, author = {El Asri, Layla and Schulz, Hannes and Sharma, Shikhar and Zumer, Jeremie and Harris, Justin and Fine, Emery and Mehrotra, Rahul and Suleman, Kaheer}, title = {Frames: a corpus for adding memory to goal-oriented dialogue systems}, booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue}, month = {August}, year = {2017}, address = {Saarbrucken, Germany}, publisher = {Association for Computational Linguistics}, pages = {207--219}, url = {https://aclanthology.org/W17-5526} }  ","date":1510274828,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510274828,"objectID":"7a2975309d9bac8111296d0dc018e370","permalink":"https://www.jkk.name/post/2017-11-09_framesdataset/","publishdate":"2017-11-09T19:47:08-05:00","relpermalink":"/post/2017-11-09_framesdataset/","section":"post","summary":"A new dialogue dataset that has annotations of multiple plans (frames) and dialogue acts that indicate modifications to them.","tags":["paper","sigdial","dialogue","data"],"title":"Frames: a corpus for adding memory to goal-oriented dialogue systems (El Asri et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Task-oriented dialogue systems are often focused on a very narrow task, to the point where the state can be described completely with a tuple (e.g. preferences for a restaurant). This paper sets up a more challenging task with more complex language use, while still having a specific goal and directly relevant structured information. They collected 11,000 dialogues, where two people have private lists of friends and are trying to identify which friend they have in common. While this is a lot of data, the mechanical turk workers are clearly moving fast, with dialogues taking 1.5 minutes on average, and in 18% of cases they get the friend wrong.\nThe algorithmic contribution is that the lists of people are represented as a graph, where nodes are properties like company and hobby. The graph is used to generate vectors for each person by running a form of message passing over its structure. During generation, the LSTM uses attention over these vectors to inform the output choice.\nA few interesting things in the output:\n There are cases where the output is incorrect, as in, says a fact about the structured information / knowledge base that is false. Evaluation is tricky, and over the metrics they consider sometimes this wins, but sometimes the baseline system (rules) does better. In particular, success on bot-bot evaluation doesn\u0026rsquo;t seem to clearly transfer to bot-human experiments. The utterances are very fluent, but that may be because it\u0026rsquo;s essentially copying from the training data. It looks like there is diversity in the dataset, but a lot of utterances do fit a template of \u0026ldquo;I have X who Y\u0026rdquo;  Citation  Paper\n@InProceedings{he-EtAl:2017:Long4, author = {He, He and Balakrishnan, Anusha and Eric, Mihail and Liang, Percy}, title = {Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1766--1776}, abstract = {We study a \\emph{symmetric collaborative dialogue} setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.}, url = {https://aclanthology.org/P17-1162} }  ","date":1510184764,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510184764,"objectID":"d9166f74be54396dfe5ffdcce03eceb1","permalink":"https://www.jkk.name/post/2017-11-08_graphdialogue/","publishdate":"2017-11-08T18:46:04-05:00","relpermalink":"/post/2017-11-08_graphdialogue/","section":"post","summary":"During task-oriented dialogue generation, to take into consideration a table of information about entities, represent it as a graph, run message passing to get vector representations of each entity, and use attention.","tags":["paper","acl","dialogue"],"title":"Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings (He et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"This paper brings together work on neural dependency parsing with the idea of non-terminal spines as a way to represent constituency structure. Within the transition parsing inference process they can naturally fit the generation of a new spines by gradually building up the spine, which makes for a very elegant inference process.\nSurprisingly, it doesn\u0026rsquo;t seem to matter what head choices are used to generate the spines (they tried leftmost word, rightmost word, and two standard schemes). This contrasts with my own observations that the choice of head had a big impact (0.5 F) on accuracy. I think the incrementally-built spines are the key difference. Decisions about higher up in the spine are difficult to make when looking at a single word, but with the incremental construction there is information about a larger context.\nCitation  Paper\n@InProceedings{ballesteros-carreras:2017:IWPT, author = {Ballesteros, Miguel and Carreras, Xavier}, title = {Arc-Standard Spinal Parsing with Stack-LSTMs}, booktitle = {Proceedings of the 15th International Conference on Parsing Technologies}, month = {September}, year = {2017}, address = {Pisa, Italy}, publisher = {Association for Computational Linguistics}, pages = {115--121}, url = {https://aclanthology.org/W17-6316} }  ","date":1510105365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510105365,"objectID":"c9b74a0f634f34ea9ac23ec42d22da6c","permalink":"https://www.jkk.name/post/2017-11-07_spineparsinglstm/","publishdate":"2017-11-07T20:42:45-05:00","relpermalink":"/post/2017-11-07_spineparsinglstm/","section":"post","summary":"Stack-LSTM models for dependency parsing can be adapted to constituency parsing by considering spinal version of the parse and adding a single 'create-node' operation to the transition-based parsing scheme, giving an elegant algorithm and competitive results.","tags":["paper","iwpt","syntax"],"title":"Arc-Standard Spinal Parsing with Stack-LSTMs (Ballesteros et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"NLP tools seem like a natural fit for literary analysis, but the domain shift from news text is large enough to degrade performance to the point where tools are not useful. Here the specific question is how many characters are there in novels? NER + coreference would seem to be enough, but an off-the-shelf system fares poorly (and I doubt improvements in the last few years would change that story).\nThe solution is to craft a kind of coreference system focused on getting all of the characters, but not necessarily every mention. The most interesting new piece is how they identify rare characters: identify arguments of verbs that usually take people. With this tool in hand they analyse patterns of character use over time to test hypotheses from literary analysis.\nAnother key piece of this work was a tool to annotate a collection of books with character occurrences. CHARLES, their tool, is built on top of brat, adding features to help multiple annotators coordinate labels (specifically handling the case of new character identification, which modifies the set of linkable entities).\nFinally, they released the character lists identified for the novels considered ( here). It would be interesting to modify a coreference resolution system to process these books, taking advantage of that information!\nCitation  Paper\n Annotation Tool Paper\n@InProceedings{vala-EtAl:2015:EMNLP, author = {Vala, Hardik and Jurgens, David and Piper, Andrew and Ruths, Derek}, title = {Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts}, booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2015}, address = {Lisbon, Portugal}, publisher = {Association for Computational Linguistics}, pages = {769--774}, url = {https://aclanthology.org/D15-1088} }  ","date":1510017388,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510017388,"objectID":"75c16b745c9f89ff9fa7677be6830c6d","permalink":"https://www.jkk.name/post/2017-11-06_literarycharacters/","publishdate":"2017-11-06T20:16:28-05:00","relpermalink":"/post/2017-11-06_literarycharacters/","section":"post","summary":"With some tweaks (domain-specific heuristics), coreference systems can be used to identify the set of characters in a novel, which in turn can be used to do large scale tests of hypotheses from literary analysis.","tags":["paper","emnlp"],"title":"Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts (Vala et al., 2015)","type":"post"},{"authors":null,"categories":[],"content":"Automatically generating high quality meeting notes and summaries would be awesome, but involves solving many challenges. Here, they assume speech recognition is already done and we also know the structure over utterances indicating which previous utterance each is a response to. The task is to label each of those utterance-utterance pairs with a type (e.g. elaboration) and to select the key phrase of each utterance.\nTwo datasets are used, the AMI and ICSO meeting corpora, which have all of the required information. The new idea here is to jointly model the choice of link label and the key phrase, which is intuitive. To show the value of joint modeling they run a version of the system with the same linear model, but with independent inference, which performs quite a bit worse.\nOne neat follow up is that by combining the key phrases into a list you get a form of summary. According to automatic metrics it is quite a bit better than running the summarisation system they compare to, though it\u0026rsquo;s still a long way from a human summary.\nCitation  Paper\n@InProceedings{qin-wang-kim:2017:Long, author = {Qin, Kechen and Wang, Lu and Kim, Joseph}, title = {Joint Modeling of Content and Discourse Relations in Dialogues}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {974--984}, url = {https://aclanthology.org/P17-1090} }  ","date":1509738032,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509738032,"objectID":"45cc328b096a71991ce0bc8f205248c3","permalink":"https://www.jkk.name/post/2017-11-03_discourserelations/","publishdate":"2017-11-03T15:40:32-04:00","relpermalink":"/post/2017-11-03_discourserelations/","section":"post","summary":"Identifying the key phrases in a dialogue at the same time as identifying the type of relations between pairs of utterances leads to substantial improvements on both tasks.","tags":["paper","dialogue","acl"],"title":"Joint Modeling of Content and Discourse Relations in Dialogues (Qin et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Mixture of experts can be seen as an ensemble approach in which we assume that each of our models is effective under different circumstances and so we combine them by switching between which we use to make our decision. From this perspective the idea can be applied to any set of models, but here the idea is to train (1) the expert models, (2) our method of choosing between them, and (3) a set of common model components, all at the same time.\nThe particular set up here is that they modify a series of LSTM layers, adding a new layer in between each pair of LSTMs. The new layer has a set of small feed-forward networks (the experts) and an even simpler network that chooses which expert to use. One big benefit of this is that a lot of computation can be avoided when we know some of the small feed-forward components are going to be ignored. As a result, they can scale up to massive networks while still having reasonable runtimes.\nSome key things to make this all work:\n Enough machines to train it! Also, there is a careful mixture of data and model parallelism during training. Some noise in the expert selection process A loss that directly encourages the use of multiple experts  One thing mentioned in passing is how this relates to a form of dropout (which can be viewed as training a set of overlapping experts, kind of).\nCitation  Paper\n@inproceedings{45929, title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff}, year = {2017}, booktitle = {ICLR}, URL = {https://openreview.net/pdf?id=B1ckMDqlg}, }  ","date":1509587847,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509587847,"objectID":"82298c522ed6587605169a8134f9e9e4","permalink":"https://www.jkk.name/post/2017-11-01_mixtureofexperts/","publishdate":"2017-11-01T21:57:27-04:00","relpermalink":"/post/2017-11-01_mixtureofexperts/","section":"post","summary":"Neural networks for language can be scaled up by using a form of selective computation, where a noisy single-layer model chooses among feed-forward networks (experts) that sit between LSTM layers.","tags":["paper","neural-network"],"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Shazeer et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"For any given task, automatic systems are fast, while annotation is accurate. This work is about bridging that gap to provide a way for a team of annotators to produce real-time high quality labels. The specific application is speech transcription, in which automatic systems are not accurate, while average people are slow (experts can transcribe in real time, but are very expensive).\nThe solution is to carefully break up the task and combine annotations back together. To get it to work well there are a range of subtle design decisions:\n People hear the entire audio stream, but with their section at normal volume and the rest quieter. This allows them to focus their effort while still understanding the context. The alignment process combines annotations with guidance from a language model and a model of typos based on keyboard layout. Words are locked in shortly after being typed, to encourage workers to go on rather than revising their own errors.  Follow up work added several more ideas to improve performance:\n Time warping, slowing down to half speed for their section, then going to 1.5x for the rest. Use ASR as well, either as another worker (with very uncorrelated errors), or as a starting point for human editing (or vice versa). Use A* search rather than a greedy algorithm for the alignment.  Performance does not reach the level of a professional, but is far better than ASR. From the paper it\u0026rsquo;s tricky to see a final cost, but it is certainly far lower than the professional.\nCitation  Paper\n@inproceedings{Lasecki:2012:RCG:2380116.2380122, author = {Lasecki, Walter and Miller, Christopher and Sadilek, Adam and Abumoussa, Andrew and Borrello, Donato and Kushalnagar, Raja and Bigham, Jeffrey}, title = {Real-time Captioning by Groups of Non-experts}, booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology}, series = {UIST '12}, year = {2012}, isbn = {978-1-4503-1580-7}, location = {Cambridge, Massachusetts, USA}, pages = {23--34}, numpages = {12}, url = {http://doi.acm.org/10.1145/2380116.2380122}, doi = {10.1145/2380116.2380122}, acmid = {2380122}, publisher = {ACM}, address = {New York, NY, USA}, keywords = {captioning, crowdsourcing, deaf, hard of hearing, real-time, text alignment, transcription}, }  ","date":1509470593,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509470593,"objectID":"17982a15d515da9a2e3dab32c190562a","permalink":"https://www.jkk.name/post/2017-10-31_realtimecaptioning/","publishdate":"2017-10-31T13:23:13-04:00","relpermalink":"/post/2017-10-31_realtimecaptioning/","section":"post","summary":"By dividing a task up among multiple annotators carefully we can achieve high-quality real-time annotation of data, in this case transcription of audio.","tags":["paper","crowdsourcing"],"title":"Real-time Captioning by Groups of Non-experts (Lasecki et al., 2012)","type":"post"},{"authors":null,"categories":[],"content":"Language is bursty, with rare words occurring in clumps, the simplest example being an unusual name that occurs a lot in one news article, but not in other articles. This paper is about how to modify a neural language model to take this into consideration, by adapting the model over time.\nThe main idea is to have one model of overall word usage (global) and a separate model that shifts over time to take into consideration the current text (local). The idea of adapting is not new (as the paper makes clear), but the key here is an update rule that is a modified form of RMSprop, combining the local and global models. It also seems like performing the updates after every 5 words is important, balancing frequency with informativeness (though no ablation of frequencies is presented). Conveniently, this is orthogonal to many other ideas and can essentially be stapled on top of a range of sequential architectures, consistently leading to improvements.\nOne question left open is how this would work in generation. The paper describes how it could be applied and could provide improvements, but it also seems likely to risk the repetitive outputs seen in many dialogue systems.\nCitation  ArXiv Paper\n@ARTICLE{2017arXiv170907432K, author = {{Krause}, B. and {Kahembwe}, E. and {Murray}, I. and {Renals}, S.}, title = \u0026quot;{Dynamic Evaluation of Neural Sequence Models}\u0026quot;, journal = {ArXiv e-prints}, archivePrefix = \u0026quot;arXiv\u0026quot;, eprint = {1709.07432}, keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language}, year = 2017, month = sep, adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170907432K}, adsnote = {Provided by the SAO/NASA Astrophysics Data System}, }  ","date":1509384510,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509384510,"objectID":"3df0d80316023bb32524b6f902c5dc29","permalink":"https://www.jkk.name/post/2017-10-30_neuralsequence/","publishdate":"2017-10-30T13:28:30-04:00","relpermalink":"/post/2017-10-30_neuralsequence/","section":"post","summary":"Language model perplexity can be reduced by maintaining a separate model that is updated during application of the model, allowing adaptation to short-term patterns in the text.","tags":["paper","arxiv","neural-network","language-model"],"title":"Dynamic Evaluation of Neural Sequence Models (Krause et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Update After prior work came to light that uses the same non-linearity, this paper was updated to focus more on the search process used to investigate the space. One new takeaway for me was how diverse the effective activation functions were (see figures in the new version of the paper).\nOriginal Post Non-linear functions are the key to the representation power of neural networks. Many different ones have been proposed, though it is difficult to make theoretical claims of their properties and so the choice of which to use is generally empirical. This paper proposes a new non-linearity, $\\text{swish}(x) = x \\cdot \\text{sigmoid}(x)$.\nInterestingly, it was chosen by a combination of exhaustive search and search with reinforcement learning across a range of functions, evaluating on CIFAR-10 with a small model. ReLU variants were consistently second-best to swish variants, and generally the more complicated functions performed worse. They do mention two functions that performed well, but didn\u0026rsquo;t generalise: $\\text{cos}(x) - x$ and $\\text{max}(x, \\text{tanh}(x))$, which look like this:\nIn a range of experiments in vision and machine translation swish does at least as well or slightly better than the alternatives. It also seems more robust to network depth and to work across different network structures. As for why it works so well, there are two main ideas: (1) it adds smoothness to the ReLU, (2) it has some sensitivity to negative inputs. Both of these seem particularly important at the start of training.\nCitation  ArXiv Paper\n@ARTICLE{2017arXiv171005941R, author = {{Ramachandran}, P. and {Zoph}, B. and {Le}, Q.~V.}, title = \u0026quot;{Swish: a Self-Gated Activation Function}\u0026quot;, journal = {ArXiv e-prints}, archivePrefix = \u0026quot;arXiv\u0026quot;, eprint = {1710.05941}, keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning}, year = 2017, month = oct, adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171005941R}, adsnote = {Provided by the SAO/NASA Astrophysics Data System} }  ","date":1509117765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509117765,"objectID":"3132165cf872819f385bc96a60aa4f41","permalink":"https://www.jkk.name/post/2017-10-27_swishactivation/","publishdate":"2017-10-27T11:22:45-04:00","relpermalink":"/post/2017-10-27_swishactivation/","section":"post","summary":"Switching from the ReLU non-linearity, $\\text{max}(0, x)$, to Swish, $x \\cdot \\text{sigmoid}(x)$, consistently improves performance in neural networks across both vision and machine translation tasks.","tags":["paper","neural-network","arxiv"],"title":"Searching for Activation Functions (Ramachandran et al., 2017)","type":"post"},{"authors":null,"categories":[],"content":"Word2Vec and other approaches provide a single vector representing a word\u0026rsquo;s meaning, giving words spatially defined relationships capturing relatedness. A natural extension is to consider regions in that space and allow some words to take up larger or smaller regions. Another natural idea is to allow a single word to have multiple representations, to capture the different senses. This paper considers both of those ideas, using multiple gaussian distributions per word.\nUsing gaussians has the nice property that there is a closed form for calculating the amount of overlap between them, which is used as a measure of similarity. Following ideas from word2vec, during learning the aim is to increase similarity between words that occur together and decrease it between random pairs that do not occur together. Once the word representations are learned, KL divergence is used for similarity, along with the standard approaches that only look at the gaussian centres.\nIn practise, two spherical distributions per word is sufficient. Performance is better than word2vec and several other approaches for multi-sense word embeddings. There was one puzzling line about the model suffering larger variance problems, but it was not quantified.\nIt would be very interesting to inject some knowledge, such as from WordNet, to guide the number of gaussians per word, rather than giving them all N. The paper also doesn\u0026rsquo;t get into details about the learned space, for example, are the two senses often far apart or close together? (in the latter case it is learning a slightly non-linear spatial representation).\nCitation  Paper\n@InProceedings{athiwaratkun-wilson:2017:Long, author = {Athiwaratkun, Ben and Wilson, Andrew}, title = {Multimodal Word Distributions}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1645--1656}, url = {https://aclanthology.org/P17-1151} }  ","date":1509065232,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509065232,"objectID":"f1b9124292409985e03c40549f74a10b","permalink":"https://www.jkk.name/post/2017-10-26_multimodalwordembeddings/","publishdate":"2017-10-26T20:47:12-04:00","relpermalink":"/post/2017-10-26_multimodalwordembeddings/","section":"post","summary":"By switching from representing words as points in a vector space to multiple gaussian regions we can get a better model, scoring higher on multiple word similarity metrics than a range of techniques.","tags":["paper","word-vectors"],"title":"Multimodal Word Distributions (Athiwaratkun and Wilson, 2017)","type":"post"},{"authors":null,"categories":[],"content":"This paper is a follow-up to yesterday\u0026rsquo;s, where the approach is implemented and evaluated on English and Chinese, with very strong results. The novel contribution is the idea of introducing alternating steps in the dynamic program to do unary steps (not a novel idea in general, but novel in its application to the dynamic programming version of shift-reduce parsing).\nWhat I found interesting here were the clear benefits of the dynamic program (DP) version. One way of viewing this is that the DP gives a more intelligent type of beam, avoiding the issue where the beam is filled with minor variations on a theme. Results are given for various beam sizes in both approaches, but it would be interesting to see a graph where the x-axis is number of items built. I suspect in that situation, the gap would be smaller. On speed, there is the nice theoretical bound of $O(n)$ for this approach, but that obscures a grammar constant related to the item structure.\nCitation  Paper\n@InProceedings{mi-huang:2015:NAACL-HLT, author = {Mi, Haitao and Huang, Liang}, title = {Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice}, booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, month = {May--June}, year = {2015}, address = {Denver, Colorado}, publisher = {Association for Computational Linguistics}, pages = {1030--1035}, url = {https://aclanthology.org/N15-1108} }  ","date":1508957053,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508957053,"objectID":"a489809963506f2fa6d85c0d101fbb4f","permalink":"https://www.jkk.name/post/2017-10-25_shiftreducedp/","publishdate":"2017-10-25T14:44:13-04:00","relpermalink":"/post/2017-10-25_shiftreducedp/","section":"post","summary":"An implementation of the transition-parsing as a dynamic program idea, leading to fast parsing and strong performance.","tags":["paper","syntax"],"title":"Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice (Mi and Huang, 2015)","type":"post"},{"authors":null,"categories":[],"content":"This paper from 2011 explores the relationship between transition based parsing and dynamic programming based parsing. They show how to convert common dependency parsing systems (Arc-Standard and Arc-Eager) into dynamic programs, and how doing the reverse on a dynamic program gives the Arc-Hybrid approach (which has since been used in many places, and is now joined by additional systems like Arc-Swift).\nThe benefit of this transformation is that we can find exact answers without massive beams. The drawback is that the feature set is restricted. This paper is theoretical, so it doesn\u0026rsquo;t give a direct measure of this tradeoff, though follow up work shows that avoiding search errors is indeed beneficial.\nWith all of the positive results using neural networks for multi-task learning, one thought this work leads to is whether we could treat different inference methods as different tasks. In other words, have a single model encoding the input, then have multiple inference algorithms with different extensions of that model, all trained simultaneously. The variation in available context for the different algorithms may force generality in the core representation shared across them.\nCitation  Paper\n@InProceedings{kuhlmann-gomezrodriguez-satta:2011:ACL-HLT2011, author = {Kuhlmann, Marco and G\\'{o}mez-Rodr\\'{i}guez, Carlos and Satta, Giorgio}, title = {Dynamic Programming Algorithms for Transition-Based Dependency Parsers}, booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, month = {June}, year = {2011}, address = {Portland, Oregon, USA}, publisher = {Association for Computational Linguistics}, pages = {673--682}, url = {https://aclanthology.org/P11-1068} }  ","date":1508864764,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508864764,"objectID":"82c8c4d24c3e12a495745b42a4c34571","permalink":"https://www.jkk.name/post/2017-10-24_dynamictransition/","publishdate":"2017-10-24T13:06:04-04:00","relpermalink":"/post/2017-10-24_dynamictransition/","section":"post","summary":"Transition based algorithms can be transformed into dynamic programs by defining sequences of actions that correspond to the same overall transformation.","tags":["paper","syntax"],"title":"Dynamic Programming Algorithms for Transition-Based Dependency Parsers (Kuhlmann et al., ACL 2011)","type":"post"},{"authors":null,"categories":[],"content":"This paper is an extension of the original AlphaGo work on using reinforcement learning to build a Go-player. Interestingly, the changes have simplified the overall model, as well as enabling it to do even better than the previous model, but now without any supervised training.\nOne key change is that there is a single core neural network learning to represent the game state. On top of that there are either a set of layers that produce an evaluation of the quality of a position, or there are a set of layers that place a distribution over moves. This ties in nicely to a lot of work happening at the moment on multi-task learning in NLP and elsewhere.\nGetting into the details, they use monte-carlo tree search to choose actions during training, then update the model to better match the outcomes observed. Starting from a completely random initialisation, the argument for why this works is that at every point in self-play the MCTS informed outcomes are just slightly better than the current model. That edge is enough to provide a useful signal, without being such a drastic shift because in self-play the two sides are closely matched. Interestingly, while the unsupervised model is worse at predicting what expert human players will do in a game, it is still better at predicting which player will win.\nCitation  Paper\n@Article{AlphaGoZero, author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis}, title = {Mastering the game of Go without human knowledge}, journal = {Nature}, year = {2017}, volume = {550}, issue = {7676}, pages = {354-359}, publisher = {Macmillan Publishers Limited, part of Springer Nature}, doi = {10.1038/nature24270}, url = {http://www.nature.com/nature/journal/v550/n7676/abs/nature24270.html#supplementary-information}, }  ","date":1508807577,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508807577,"objectID":"dff037bad9edf2bcf813daf71de48c69","permalink":"https://www.jkk.name/post/2017-10-23_alphagozero/","publishdate":"2017-10-23T21:12:57-04:00","relpermalink":"/post/2017-10-23_alphagozero/","section":"post","summary":"By using a single core model to build a game state representation, which then gives input to both state evaluation and move choice, DeepMind are able to apply reinforcement learning with self-play with no supervision and achieve state-of-the-art performance.","tags":["paper","rl"],"title":"Mastering the game of Go without human knowledge (Silver et al., Nature 2017)","type":"post"},{"authors":null,"categories":[],"content":"Recurrent neural networks like LSTMs and GRUs have limited scope for parallelisation because each step depends on the one before it. This architecture also means that many steps of computation separate two words that are far apart, making it difficult to capture long-distance relations. A range of approaches have been used to try to address these issues, such as convolutional structures and other forms of recurrence (e.g. QRNNs). The idea in this work is to use attention, applied multiple times, to get a network that is fast while still capturing positional information.\nTo explain the structure I put together the figure below, which captures the network structure with a few simplifications:\nThere are a few ideas being brought together here:\n Positional encoding, which is a vector of the same length as the word representation, but that depends only on the position in the input. Here they use $f(pos, dim) = sin(pos / 10000^{2 dim / d_w})$ for even dimensions and the cosine equivalent for odd dimensions (where $d_w$ is the number of dimensions. Multi-head attention, where rather than running attention once on the full vector, multiple linear transforms are applied to get smaller vectors. Scaled dot product attention, the equation is shown in the figure, the key new idea is to rescale by the square root of the dimensionality so that larger vectors don\u0026rsquo;t produce excessively sharp distributions after the softmax is applied. The more general form of this described in the paper has keys ($K$), queries ($Q$) and values ($V$), but the network uses the same vector for the key and value. I show the query with a solid line and the values/keys with dotted lines. The matrix $V$ is formed by using the $v$ vectors as rows, while $Q$ is formed by duplicating $q$ in every row. Note, on the left hand side attention is over all input words, while on the right it is only over the words produced so far (ie. content to the left). Layer normalisation, a way to rescale weights to keep vector outputs in a nice range, from Ba, Kiros and Hinton (ArXiv 2016). Other details, (1) When the outputs are words, the vectors used to initially represent each input word are also used to represent the outputs and in the final linear transformation (though with some rescaling). (2) They use a formula I haven\u0026rsquo;t seen before to adjust the learning rate during training, (3) dropout in several places and label smoothing are used for regularization.  Simplifications in the figure:\n For multi-head attention I only show two transforms, while in practise they used 8. The shaded regions are duplicated 6 times to create a stack, with the output of one region acting as the input to the next copy of it. The links from left to right are always from the top of the input stack. The musical repeat signs indicate that the structure is essentially the same. On the output side this isn\u0026rsquo;t quite true since the attention boxes only take inputs to their left (since output to the right doesn\u0026rsquo;t exist when they are being calculated).  In terms of experiments, it works at least as well if not better than prior approaches, and is a lot faster for machine translation (no speed numbers are given for parsing). There is also some nice analysis of what it ends out using the attention mechanism to focus on for each word. It seems like it can provide a way to effectively disambiguate the sense of a word based on its context.\nCitation  ArXiv Paper\nGoogle also has some blog posts up about the paper and about the library they released.\n@article{arxiv:1706.03762, author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia}, title = {Attention Is All You Need}, journal = {ArXiv}, year = {2017}, url = {http://arxiv.org/abs/1706.03762}, }  ","date":1508527523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508527523,"objectID":"2b9b283418cae4523036b4fbbddad6b9","permalink":"https://www.jkk.name/post/2017-10-20_onlyattention/","publishdate":"2017-10-20T15:25:23-04:00","relpermalink":"/post/2017-10-20_onlyattention/","section":"post","summary":"To get context-dependence without recurrence we can use a network that applies attention multiple times over both input and output (as it is generated).","tags":["paper","arxiv","neural-network"],"title":"Attention Is All You Need (Vaswani et al., ArXiv 2017)","type":"post"},{"authors":null,"categories":[],"content":"The standard way to get high quality annotations is to get labels from multiple people and take a majority vote. Getting multiple annotations costs more, and the quality of annotators can vary considerably (with spamming at one extreme). One way to avoid the quality issue is to restrict who can do the task (must have done X previous tasks with an accept rate of Y), but that limits the pool of available workers. Another approach is to try to estimate the quality of annotator work using a statistical model.\nHere a generative model is used, with the following structure:\n $T_i$, the true label, sampled with a uniform prior over labels $S_{ij}$, a binary variable indicating if the person is spamming or not, sampled as a Bernoulli variable with a Beta prior $A_{ij}$, the annotator\u0026rsquo;s decision, if they are spamming it is sampled from a multinomial with parameters specific to them (with a Dirichlet prior), otherwise it is the true label  $A$ is observed, but $T$ and $S$ are not, so they use expectation maximization to get both model parameters and variable values. To deal with nonconvexity they use 100 random restarts, deciding which is best based on how well the model describes the data. Note - this model (and the code) was the basis of the error detection paper I wrote about recently.\nFor predicting annotator quality the model is consistently effective across three datasets, though the Beta and Dirichlet priors are key for one (where annotator agreement was high on average). For determining the correct answer it is slightly better than majority vote, though the gains are small. The real advantage comes in deciding whether to discard data, where the choice of what to discard can be guided by the estimate of quality (this is what the error detection paper was doing). A range of synthetic experiments also show positive results, though their design shares the assumptions about behaviour that are baked into the model.\nI found a few results particularly interesting:\n As the number of annotators is decreased, the benefit of this approach over majority vote grows to be quite substantial (the main experiments are for data with 10 annotators). If you do use majority vote, use an odd number of annotators. Switching to an even number mainly seems to create ties. The right number is also very data dependent. Providing gold information as supervision within EM doesn\u0026rsquo;t help much unless it is quite substantial (20%+ of the data)  Citation  Paper\n@InProceedings{hovy-EtAl:2013:NAACL-HLT, author = {Hovy, Dirk and Berg-Kirkpatrick, Taylor and Vaswani, Ashish and Hovy, Eduard}, title = {Learning Whom to Trust with MACE}, booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, month = {June}, year = {2013}, address = {Atlanta, Georgia}, publisher = {Association for Computational Linguistics}, pages = {1120--1130}, url = {https://aclanthology.org/N13-1132} }  ","date":1508447330,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508447330,"objectID":"a99f46ce4a91be4ec4a7698aeb64a580","permalink":"https://www.jkk.name/post/2017-10-19_mace/","publishdate":"2017-10-19T17:08:50-04:00","relpermalink":"/post/2017-10-19_mace/","section":"post","summary":"By using a generative model to explain worker annotations, we can more effectively predict the correct label, and which workers are spamming.","tags":["paper","naacl","crowdsourcing"],"title":"Learning Whom to Trust with MACE (Hovy et al., NAACL 2013)","type":"post"},{"authors":null,"categories":[],"content":"This is another paper concerned with the challenge of sparsity in AMR parsing, specifically that there are an enormous number of output symbols in the parse trees and most are seen infrequently. The system they develop is based on the encoder-decoder with attention approach, which has previously done poorly for AMR, partially because of sparsity.\nTheir solution is to merge certain types of symbols into groups (dates, named entities, rare verbs, constants, etc) and have a standard way to map from the surface form to the output symbol. This is an alternative to the approach from the paper I wrote about last week. They also introduce a completely separate idea, which is a different way to take an AMR graph and turn it into a linear sequence. This change is necessary to make the output follow the form their model generates - a sequence (though there has been work on tree based LSTMs on the output side, so AMR could be directly generated, and I believe there has been some work on applying that to AMR).\nTogether these changes do substantially improve performance over previous encoder-decoder based work for AMR. However, there is still a substantial gap between the system and state-of-the-art, presumably because of the additional resources that other systems indirectly use by running external systems for NER, dependency parsing, etc. Given the recent success of multi-task learning with neural nets, it would be interesting to see if those resources could be used here to further boost performance. It may also be productive to combine these ideas with the graph abstraction ideas from AMR alignment paper.\nCitation  Paper\n@InProceedings{peng-EtAl:2017:EACLlong1, author = {Peng, Xiaochang and Wang, Chuan and Gildea, Daniel and Xue, Nianwen}, title = {Addressing the Data Sparsity Issue in Neural AMR Parsing}, booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers}, month = {April}, year = {2017}, address = {Valencia, Spain}, publisher = {Association for Computational Linguistics}, pages = {366--375}, url = {https://aclanthology.org/E17-1035} }  ","date":1508376665,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508376665,"objectID":"94dbdfa6deed871d205e03b9c1844e11","permalink":"https://www.jkk.name/post/2017-10-18_neuralamr/","publishdate":"2017-10-18T21:31:05-04:00","relpermalink":"/post/2017-10-18_neuralamr/","section":"post","summary":"Another paper looking at the issue of output symbol sparsity in AMR parsing, though here the solution is to group the consistent but rare symbols (rather than graph fragments like the paper last week). This drastically increases neural model performance, but does not reach the level of hybrid systems.","tags":["paper","eacl","amr"],"title":"Addressing the Data Sparsity Issue in Neural AMR Parsing (Peng et al., EACL 2017)","type":"post"},{"authors":null,"categories":[],"content":"Several NLP tasks aim to identify information regarding entities, such as when two sections of text are referring to the same thing, or which thing out of a large set (e.g. things in Wikipedia) a piece of text is about. This paper focuses on a subset of entity linking, trying to determine which entity out of a set of candidates is the correct one (in a way a kind of reranker for entity linking).\nThe task is based on a really cool dataset from Google+UMass, which collected text that was hyperlinked to wikipedia articles. The idea is that the text (mention) is probably a reference to the thing the article describes, so it is an easy way to get entity linked data for free. Here, the data is filtered to mentions that aren\u0026rsquo;t too rare (more than 10 occurrences) and where the mention isn\u0026rsquo;t used to refer to too many different entities (the two most common entities account for over 10% of occurrences). Then, the set of things that this mention is used to refer to somewhere are treated as a list of candidates, and the task is to choose which one is correct in a given context.\nThe model is of the common style at the moment:\n The context is processed using a recurrent neural network to produce a set of vectors Attention is used to produce vectors that combine the context with a candidate entity A feedforward neural network produces a score that is maxed over to get a final decision  On the wikilinks based dataset this performs quite a bit better than other models, but it is behind on the smaller manually curated datasets used elsewhere (YAGO and PPRforNED, which link entities in the CoNLL 2003 shared task). Interestingly, augmenting the training data for YAGO with data from wikilinks does improve performance. For future users of the wikilinks data there is also some nice analysis at the end of remaining challenges, which are spit between mistakes in the data (unsurprising given the approximate collection process), answers that are too general or specific, tricky cases, and the long tail (which would be even longer without the filtering used in these experiments).\nCitation  Paper\n@InProceedings{eshel-EtAl:2017:CoNLL, author = {Eshel, Yotam and Cohen, Noam and Radinsky, Kira and Markovitch, Shaul and Yamada, Ikuya and Levy, Omer}, title = {Named Entity Disambiguation for Noisy Text}, booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)}, month = {August}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {58--68}, url = {https://aclanthology.org/K17-1008} }  ","date":1508286838,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508286838,"objectID":"aeb823d0605434a7d517dcbd27215eec","permalink":"https://www.jkk.name/post/2017-10-17_nedisambiguation/","publishdate":"2017-10-17T20:33:58-04:00","relpermalink":"/post/2017-10-17_nedisambiguation/","section":"post","summary":"The WikiLinks dataset of text mentions that are hyperlinked to wikipedia articles provides a nice testing space for named entity disambiguation, and a neural network using attention over local context does reasonably well.","tags":["paper","conll","entity-linking"],"title":"Named Entity Disambiguation for Noisy Text (Eshel et al., CoNLL 2017)","type":"post"},{"authors":null,"categories":[],"content":"Attention - a weighted average over a set of vectors representing context - has consistently produced positive results. Here we see an example of how it can be applied in the case of modeling a threaded discussion.\nAttention is applied in two ways. First, over a fixed set of vectors. This is intended to provide a mechanism to choose between several different sub-models contained within a single model. Put differently, the vectors provide a set of latent representations that capture each of the different types of posts in the subreddit. Second, attention over the current utterance is used in the process of predicting responses (at training time only). This provides an additional source of input to the model, by forcing it to explain the response utterances using the same representations as a source of information.\nThe application is a new task, using values assigned to posts = upvotes - downvotes (i.e. Reddit karma). Predicting the specific value is hard, so the task is split into 7 binary decisions about whether a post has a score higher or lower than some value. On this task the new approach provides consistent gains, though overall performance remains low (53 - 56%). Confusingly though, one of the figures (number 4) seems to suggest that it was a single multi-way decision, not a set of binary decisions. I\u0026rsquo;m also curious about the data, in particular what the distribution of scores is. The paper mentions it is Zipfian, but surely it would be something double-sided with a massive peak at 0 and a rapid drop in either direction?\nOverall, this is further evidence of the versatility of the idea of attention!\nCitation  Paper\n@InProceedings{cheng-fang-ostendorf:2017:EMNLP2017, author = {Cheng, Hao and Fang, Hao and Ostendorf, Mari}, title = {A Factored Neural Network Model for Characterizing Online Discussions in Vector Space}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {2286--2296}, url = {https://aclanthology.org/D17-1242} }  ","date":1508201707,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508201707,"objectID":"ecef32f2b0cff5f80e009fc3ae5a4cb1","permalink":"https://www.jkk.name/post/2017-10-16_forumrnn/","publishdate":"2017-10-16T20:55:07-04:00","relpermalink":"/post/2017-10-16_forumrnn/","section":"post","summary":"A proposal for how to improve vector representations of sentences by using attention over (1) fixed vectors, and (2) a context sentence.","tags":["paper","emnlp","word-vectors"],"title":"A Factored Neural Network Model for Characterizing Online Discussions in Vector Space (Cheng et al., EMNLP 2017)","type":"post"},{"authors":null,"categories":[],"content":"Active learning doesn\u0026rsquo;t seem to get much attention in NLP, probably because of fear that developing data based on the errors of one model will introduce a particular sampling bias. This paper is a nice example of a problem it can be applied to that doesn\u0026rsquo;t raise that issue: detecting all the errors in a system\u0026rsquo;s output.\nThe scenario is that you have a bunch of models for doing a task (e.g. POS tagging) and a new dataset with no labeled data, which you would like to label. Having a person label the data would take a long time and doesn\u0026rsquo;t take advantage of these systems. At the same time, we can\u0026rsquo;t just run the systems and use their output because they aren\u0026rsquo;t perfect, particularly out of domain. We could run the systems and check their output, which could be faster than annotating directly, but would still take a long time. If we don\u0026rsquo;t mind having some errors, we can check just some output, but how do we decide what to check?\nThis paper applies the generative model from MACE to build a generative model of system outputs. The model is:\n For each example, sample the true label with a uniform prior Then, for each classifier, sample from a Bernoulli distribution to decide if they are good or not A good classifier returns the true label, a not good classifier samples from a multinomial over the options  Since we don\u0026rsquo;t know the parameters of the model, or the true labels, use expectation maximisation to learn.\nThis work takes that model, trains it and uses it to identify the sample that is most uncertain. A person annotates it, the correct label replaces one of the system predictions, and EM is run again. This is repeated until either there appear to be no more errors, or annotators run out of time.\nHow well does it work? The main metric is precision: how many of the instances asked for annotation actually have errors. For POS tagging on WSJ text, the taggers initially get 2.5% of words wrong. To get that down to 1.1% the precision is 33%, and to get it to 0.65% precision is 17.6%. On an out of domain dataset, the error rate is 10% initially, and is down to 5% with a precision of 50%. Put differently, in a dataset of 25,000 tokens, with 2,500 errors, after checking 2,500 tokens, there are only 1,250 errors (another 2,500 checks brings it down to 730). It also works well for NER, and consistently does better than the alternative they compare to (consider the taggers a committee and find the examples with highest entropy, i.e. greatest disagreement).\nThis seems like a natural fit for prodigy and something that could be broadly useful.\nCitation  Paper\n@InProceedings{rehbein-ruppenhofer:2017:Long, author = {Rehbein, Ines and Ruppenhofer, Josef}, title = {Detecting annotation noise in automatically labelled data}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1160--1170}, url = {https://aclanthology.org/P17-1107} }  ","date":1507915939,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507915939,"objectID":"9de8fcb58feacf69021fc9292b860e2c","permalink":"https://www.jkk.name/post/2017-10-13_errordetection/","publishdate":"2017-10-13T13:32:19-04:00","relpermalink":"/post/2017-10-13_errordetection/","section":"post","summary":"When labeling a dataset automatically there are going to be errors, but we can use a generative model and active learning to guide effort to checking the examples most likely to be incorrect.","tags":["paper","acl","annotation"],"title":"Detecting annotation noise in automatically labelled data (Rehbein and Ruppenhofer, ACL 2017)","type":"post"},{"authors":null,"categories":[],"content":"Abstract Meaning Representation (AMR) structures represent sentence meaning with labeled nodes (concepts) that are related to the words in the sentence, but not explicitly linked to them. This is a problem for most parsing algorithms, which need a way to efficiently decompose the structure in order to learn how to generate it. In dependency parsing there are no abstract nodes to generate, in constituency parsing there is a very small set of node types, and for CCG, TAG, etc the labels come from a constrained space. The solution for many AMR parsers is to have a process for generating the concepts as a first step towards parsing, and to automatically align the training data to guide this concept generation stage.\nThe first idea in this paper is about the set of AMR concepts. Some concepts are easy to link, as the concept clearly maps to a single word in the sentence. Around a quarter of concepts have a more complex relation, where a set of concepts link to a set of words, for example, named entities. The idea for these is to identify common subgraphs by abstracting some lexical items. For example, a teacher and a worker both get mapped to a person concept that is the ARG0 of the lexical item (teach, or work in this case). This can allow for the generation of entirely novel concepts (e.g. \u0026ldquo;concept\u0026rdquo;-er), giving a 0.6 boost to recall for CAMR simply by making these additional concepts available. Using a bidirectional LSTM with a character CNN to generate features on likely concepts, there is a gain of 1.0 F1 for the parser.\nThe second idea is to improve the alignments used to train concept generation by taking into consideration the graph structure. To use an aligner developed for machine translation the graph needs to be turned into a linear sequence, but that can lead to strange jumps. The idea here is to take that into consideration by modifying the calculation of the cost of distortion (i.e. jumping) to be reshaped based on the graph structure. For optimal alignment quality they consider aligning in either direction, directly changing the distance metric in the English-AMR direction, and just rescaling it to be less sensitive when appropriate for AMR-English. This is definitely higher precision than prior approaches, but lower recall. It\u0026rsquo;s hard to tell whether this helps, since the evaluation doesn\u0026rsquo;t separate it out from the first idea (results in section 5.3 are not on the same dataset as 5.1).\nGiven how separate this is from CAMR, it would be interesting to see if it helps other systems similarly. With concept identification at 83 F there is still plenty of scope for improvement, though there is no analysis of which types of concepts remain the most problematic.\nCitation  Paper\n@InProceedings{wang-xue:2017:EMNLP2017, author = {Wang, Chuan and Xue, Nianwen}, title = {Getting the Most out of AMR Parsing}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {1268--1279}, url = {https://aclanthology.org/D17-1130}, }  ","date":1507852354,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507852354,"objectID":"348b1d8db9bccb6fe3c6a8c939e5881e","permalink":"https://www.jkk.name/post/2017-10-12_amralignment/","publishdate":"2017-10-12T19:52:34-04:00","relpermalink":"/post/2017-10-12_amralignment/","section":"post","summary":"Two ideas for improving AMR parsing: (1) take graph distance into consideration when generating alignments, (2) during parsing, for concept generation, generate individual concepts in some cases and frequently occurring subgraphs in other cases.","tags":["paper","emnlp","amr"],"title":"Getting the Most out of AMR Parsing (Wang and Xue, EMNLP 2017)","type":"post"},{"authors":null,"categories":[],"content":"This paper is a detailed analysis of a surprisingly effective simple idea: train a machine translation system with sentence pairs from multiple languages, adjusting the input to have an extra token at the end that says what the target language is. To deal with class imbalance, data is oversampled to have all language pairs be equally represented (though even without that, it works fairly well).\nThe biggest advantage of this approach is that a single model can handle translation between many pairs, rather than needing $O(n^2)$ models for $n$ languages. The performance is slightly lower on average, but the single model can manage with far fewer parameters. In one example, twelve models are combined into a single model with as many parameters as one of the twelve, and the results are lower by just 0.76 BLEU on average. Another advantage of the model is the ability to handle code-switched language, though they didn\u0026rsquo;t have evaluation datasets to get an quantitative measure of accuracy.\nHaving this model also opens up the possibility of translating between pairs of languages with no parallel training data (A -\u0026gt; B). As long as there is data (A -\u0026gt; C) and (D -\u0026gt; B), sentences from A can be fed in with B as the target language. For closely related languages this works very well, and in particular, better than going via another language such that there is data for the two language pairs. For example, going from Portuguese to Spanish with the multilingual model scores 24.75, whereas going via English scores 21.62 and a model with explicit training data gets 31.50. Going between less related languages is less successful, with direct Spanish to Japanese scoring 9.14, and going via English scoring 18.00. One thing I wish the paper had is more exploration of this result - what does it get right when scoring 9.14? For the time being at least, going via a third language still seems necessary, and presumably the best language to use is whichever one the performance is highest on.\nCitation  Paper\n ArXiv version which appears to be the same aside from one extra figure of the model architecture.\nAs an aside, it is interesting to see the timeline for this paper:\n November 2016, Submission to ArXiv and in the TACL submission batch March 2017, TACL revision batch October 2017, TACL published  @article{TACL1081, author = {Johnson, Melvin and Schuster, Mike and Le, Quoc and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viégas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey}, title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation}, journal = {Transactions of the Association for Computational Linguistics}, volume = {5}, year = {2017}, issn = {2307-387X}, url = {https://www.transacl.org/ojs/index.php/tacl/article/view/1081}, pages = {339--351} }  ","date":1507757344,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507757344,"objectID":"61ce10e2ad520b0f41990806e78a79bb","permalink":"https://www.jkk.name/post/2017-10-11_multimt/","publishdate":"2017-10-11T17:29:04-04:00","relpermalink":"/post/2017-10-11_multimt/","section":"post","summary":"A translation model trained on sentence pairs from a mixture of languages can do very well across all of the languages, and even generalise somewhat to new pairs of the languages. That's useful as one model can do the work of $O(n^2)$ models, and with a fraction of the parameters.","tags":["paper","tacl","mt"],"title":"Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al., TACL 2017)","type":"post"},{"authors":null,"categories":[],"content":"Semantic parsing datasets generally consist of (question, answer) pairs, where each pair is completely independent of the rest (one exception is ATIS, which has multi-turn conversations, though most work doesn\u0026rsquo;t use them). In reality, we often ask a series of simple questions that together form a complex one, for example \u0026ldquo;What flights are available from Detroit to Sydney? And how much is the price if I don\u0026rsquo;t want to leave before 8am?\u0026rdquo; This work explores these kinds of sequential questions with a new dataset and algorithm.\nThe dataset was formed by asking crowd workers to rephrase questions from the WikiTableQuestions dataset into sequences of shorter questions. This naturally constrains the types of questions (in particular, they reference a single table only), but covers a range of domains. With 6,066 question sequences, and on average 2.9 questions / sequence, it\u0026rsquo;s a large dataset by semantic parsing standards. However, there are no logical forms, only the row, column, or cell(s) that contain the answer.\nTo solve the problem, they treat it as choosing a sequence of actions, where each action generate a part of the execution instructions. The model follows the recent approach of considering the contents of the database as part of the calculation (e.g. by taking the dot product of the vector for a cell and the vector for the question).\nThe system has consistently better performance than other QA systems on the new dataset (though no results are shown for the WikiTableQuestions dataset). At only 12.8% of sequences completely correct, there is plenty of scope for improvement. Based on the description of the operators there are definitely additional abilities that would be useful, so this model has potential to improve. That said, it seems difficult to generalise the model to handle more complicated databases with multiple interconnected tables.\nCitation  Paper\n@InProceedings{iyyer-yih-chang:2017:Long, author = {Iyyer, Mohit and Yih, Wen-tau and Chang, Ming-Wei}, title = {Search-based Neural Structured Learning for Sequential Question Answering}, booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = {July}, year = {2017}, address = {Vancouver, Canada}, publisher = {Association for Computational Linguistics}, pages = {1821--1831}, url = {https://aclanthology.org/P17-1167} }  ","date":1507657416,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507657416,"objectID":"74b8e57a4a46d8a3fbfcdee740009841","permalink":"https://www.jkk.name/post/2017-10-10_seqqa/","publishdate":"2017-10-10T13:43:36-04:00","relpermalink":"/post/2017-10-10_seqqa/","section":"post","summary":"A new dataset containing multi-turn questions about a table, and a model that generates a kind of logical form, but scores actions based on the content of the table.","tags":["paper","acl","semantic-parsing"],"title":"Search-based Neural Structured Learning for Sequential Question Answering (Iyyer et al., ACL 2017)","type":"post"},{"authors":null,"categories":[],"content":"Semantic parsing datasets are small because they are expensive to produce (logical forms don\u0026rsquo;t occur naturally and writing them down takes time). The idea here is to do semi-supervised learning by implementing both a parser and a generator, which are trained together as a form of autoencoder where the intermediate representation is natural language.\nThe architecture has four LSTMs:\n Bidirectional LSTM over a logical form. One directional LSTM attending to the first LSTM\u0026rsquo;s hidden states, generating a sentence. Bidirectional LSTM over the sentence generated by the second LSTM. One directional LSTM attending to the third LSTM\u0026rsquo;s hidden states, generating a logical form.  Usually a component like the second LSTM would choose the max word at each position (or use beam search), but here they want this whole thing to be differentiable, so the distribution over words is used. At evaluation time only the second half (3+4) is used, with the test sentence as input.\nWith this structure, a loss function is defined that compares the input to (1) and the output of (4), which in both cases is a logical form. As a result, they don\u0026rsquo;t need (logical form, sentence) pairs to train, they can use automatically generated logical forms. Of course, with only logical forms it would do something random with the intermediate representation, so some supervised examples are also needed (in which case the two halves are trained independently).\nThe results are not state-of-the-art, but good on all three tasks (Geoquery, NLmaps, SAIL), and on two they show am improvement over training (3+4) with only supervised data. Varying the amount of training data gives a less clear picture. On Geoquery with 5-25% of the data, this approach clearly helps, particularly if the queries are real rather than generated (which is a realistic scenario), but then there is no improvement for 50% or 75%, and at 100% the improvement is small. On NLmaps there was no generator, and the differences at different data %s seem like noise. SAIL has the most clear benefit, though it\u0026rsquo;s a particularly small dataset, consisting of paths in just four maps.\nThis is a cool idea that seems effective in certain situations. The generator is key, and it\u0026rsquo;s possible that performance on GeoQuery would be higher with a more sophisticated one (e.g. a tree structured generator, rather than the ngram model used here). One idea mentioned in the conclusion is to try reversing the setup (3-4-1-2) and training with natural language examples that have no logical form. How to tradeoff the different data scenarios seems like an interesting challenge!\nCitation  Paper\n@InProceedings{kovcisky-EtAl:2016:EMNLP2016, author = {Ko\\v{c}isk\\'{y}, Tom\\'{a}\\v{s} and Melis, G\\'{a}bor and Grefenstette, Edward and Dyer, Chris and Ling, Wang and Blunsom, Phil and Hermann, Karl Moritz}, title = {Semantic Parsing with Semi-Supervised Sequential Autoencoders}, booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, month = {November}, year = {2016}, address = {Austin, Texas}, publisher = {Association for Computational Linguistics}, pages = {1078--1087}, url = {https://aclanthology.org/D16-1116} }  ","date":1507573884,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507573884,"objectID":"91e679b5813d09bb923a3ea62273d606","permalink":"https://www.jkk.name/post/2017-10-09_parsing-autoencoder/","publishdate":"2017-10-09T14:31:24-04:00","relpermalink":"/post/2017-10-09_parsing-autoencoder/","section":"post","summary":"By training a parser and language generation system together, we can use semantic parses without associated sentences for training (the sentence becomes a latent representation that is being learnt).","tags":["paper","emnlp","semantic-parsing"],"title":" Semantic Parsing with Semi-Supervised Sequential Autoencoders (Kocisky et al., EMNLP 2016)","type":"post"},{"authors":null,"categories":[],"content":"Humor is an incredibly difficult problem, as this paper makes clear in its background section. Most work has considered very specific types of jokes (e.g. \u0026ldquo;that\u0026rsquo;s what she said\u0026rdquo;, or pairs of words that sound similar to form riddles). This work contributes (1) a new task, (2) an evaluation method, and (3) an example system.\nThe task is Mad Libs, where a story has some words removed and people choose new words to make the story funny. If you are familiar with the normal version, one key difference is that here people have access to the complete story when they are choosing their words. A set of 40 \u0026lsquo;stories\u0026rsquo; were written based on Simple Wikipedia articles, and workers on Mechanical Turk wrote words to fill them, with filtering based on judging by other workers.\nThe evaluation method involved recruiting a set of judges on Mechanical Turk and asking a series of questions to measure humour for a given response. As well as judging the overall story, they were asked to select which words contributed the most. By aggregating these selections as votes, each word was scored as funny or not.\nThe system is a linear classifier with a range of features, including scores from a language model. On its own, it performs very poorly, but using it as a filter to restrict the space of words a person can choose from actually leads to better performance than people on their own. Of course, it\u0026rsquo;s difficult to analyse the source of improvement; The authors theorise that it is because it prevents people from selecting words that only they would see is funny. Another interpretation is that the constraint gives them a smaller space to think about and so they can find more interesting plays on words.\nFinally, as a non-expert in this area, this paper had some nice discussion of the tradeoffs between different ways of generating humour (incongruous vs. coherent content strategies).\nCitation  Paper\n@InProceedings{hossain-EtAl:2017:EMNLP2017, author = {Hossain, Nabil and Krumm, John and Vanderwende, Lucy and Horvitz, Eric and Kautz, Henry}, title = {Filling the Blanks (hint: plural noun) for Mad Libs Humor}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher = {Association for Computational Linguistics}, pages = {649--658}, url = {https://aclanthology.org/D17-1068}, }  ","date":1507311103,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507311103,"objectID":"8677120b0604dbfda4fea3d37859ab13","permalink":"https://www.jkk.name/post/2017-10-06-madlibs/","publishdate":"2017-10-06T13:31:43-04:00","relpermalink":"/post/2017-10-06-madlibs/","section":"post","summary":"A new task and associated evaluation method plus system for Mad Libs - filling in missing words in a story in a funny way. While the system does poorly, using it as a first pass with human rerankers produces funnier stories than people alone.","tags":["paper","emnlp","humour"],"title":"Filling the Blanks (hint: plural noun) for Mad Libs Humor (Hossain et al., EMNLP 2017)","type":"post"},{"authors":null,"categories":[],"content":"This paper proposes two techniques for speeding up neural network execution on GPUs:\n Reduce computation when doing matrix-multiply by removing rows. Reduce communication on the GPU by halving the number of bits used to represent numbers.  Either of these gives a speed up of ~1.5x and together they give ~2x, across a range of different computer vision tasks+models.\nCore ideas in detail The first idea, reducing work by eliminating parts of the computation, has been considered before. In the past, however, the focus was on saving memory in models, and so the most common strategy was to move to a sparse matrix where weights close to zero are dropped. Here the focus is on speed and they show that while the sparse approach saves memory it can end up being slower because of hardware behaviour. Instead, they eliminate entire rows of the matrix, which means there is less computation, but it remains dense (and therefore fast). Rows are identified by measuring correlation between outputs and greedily eliminating rows that correlate highly with the rest of the output.\nThe natural question to ask is whether this hurts performance. First, they do two things to avoid problems, (1) a scale factor is used to make sure the outputs are of the same range that they would have been with the full matrix, and (2) they restart training to fine-tune the network once pruning is set up. With high enough pruning accuracy does fall, but speed ups can be gained before that is a problem (the exact point depends on the task).\nThe second idea relates to numerical representation, and is motivated by measurements of where the bottlenecks are in communication. Many AI researchers have tried switching to 16 bit representations to save space and time, but here they develop a different floating point encoding that gives more bits to the exponent, and fewer to the mantissa.\nThoughts  It would be interesting to see the interaction of this work with the investigation of networks without non-linear functions that can still learn non-linear behaviour because of numerical approximations. In the context of language, the weight reduction approach would be interesting to analyse. Specifically, what do we lose in our word vectors depending on the task? I\u0026rsquo;ve always had some interest in making things faster. It would be interesting to know where the remaining bottlenecks are (after applying these changes).  Citation @InProceedings{Hill:MICRO:2017, author = {Hill, Parker and Jain, Animesh and Hill1, Mason and Zamirai, Babak and Hsu, Chang-Hong and Laurenzano, Michael A. and Mahlke, Scott and Tang, Lingjia and Mars, Jason}, title = {DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission}, booktitle = {The 50th Annual IEEE/ACM International Symposium on Microarchitecture}, year = {2017}, }  ","date":1507179600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507179600,"objectID":"0edc3dc8ca41e9e720fb645585dc99c5","permalink":"https://www.jkk.name/post/2017-10-05-deftnn/","publishdate":"2017-10-18T00:00:00-05:00","relpermalink":"/post/2017-10-05-deftnn/","section":"post","summary":"GPU processing can be sped up ~2x by removing low impact rows from weight matrices, and switching to a specialised floating point representation.","tags":["paper","neural-network","efficiency"],"title":"DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission (Hill et al., MICRO 2017)","type":"post"},{"authors":null,"categories":[],"content":"How much vegetarian pizza should I order? This question frequently comes up in the world of free food at university events. In my experience (as someone who does not eat meat pizzas), often not enough is ordered. Let\u0026rsquo;s try to come up with a model to tell us how much to order. Set it up like this:\n There are $N$ people. There are $P$ pizzas. The fraction of people who are vegetarian is $V$. Assume everyone eats the same amount of pizza, and all the pizza is eaten (ie. each person eats $\\frac{P}{N}$). Assume people randomly sample from the available pizzas, subject to the constraint that some eat only vegetarian pizzas.  Now, let the fraction of vegetarian pizzas we get be $k$, and we can write down the number of vegetarian pizzas in two ways:\n How many we order: $P * k$ How many are eaten: (pizzas eaten by vegetarians) + (vegetarian pizzas eaten by others) = $\\frac{P}{N} * (N * V) + \\frac{P}{N} * (N * (1 - V)) * k$  Since all the pizza we order is eaten, these are equal. $N$ and $P$ are both positive numbers, so we can safely cancel the $N$s and divide through by $P$, giving:\n$V + (1 - V) * k = k$\nTo satisfy this equation, $k = 1$. Therefore all the pizza should be vegetarian :)\nOf course, these assumptions aren\u0026rsquo;t quite right (for example, not everyone samples randomly from the available pizza), so here are some more useful suggestions too:\n Do order more than the proportion of vegetarians. Place the vegetarian pizza at the end of the line of pizzas, or in a separate location with clear signage discouraging non-vegetarians from eating it. Order a diverse set of popular meat pizzas (people tend to want variety, so this encourages them to try more meat pizzas).  For other peoples\u0026rsquo; thoughts on this question see Serious Eats and Quora\n","date":1506488400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506488400,"objectID":"546737dbab8a1b60a1b3d1b586086d12","permalink":"https://www.jkk.name/post/pizza/","publishdate":"2017-09-27T00:00:00-05:00","relpermalink":"/post/pizza/","section":"post","summary":"A simple model to help determine how much of each pizza type to order for an event.","tags":["misc"],"title":"Ordering Pizza for an Event with Vegetarians","type":"post"},{"authors":["Greg Durrett","Jonathan K. Kummerfeld","Taylor Berg-Kirkpatrick","Rebecca S. Portnoff","Sadia Afroz","Damon McCoy","Kirill Levchenko","Vern Paxson"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"9ab5882d1a9beb2f058b577bb554c6c1","permalink":"https://www.jkk.name/publication/emnlp17forums/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/emnlp17forums/","section":"publication","summary":"One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects.  We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums.  Each of these forums constitutes its own 'fine-grained domain' in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.","tags":null,"title":"Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"8b3facf4e8cd06efeebbc16095eb2e58","permalink":"https://www.jkk.name/data/cybercrime-forums/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/data/cybercrime-forums/","section":"data","summary":"Forum posts with annotations of products.","tags":["information-extraction"],"title":"IE/NER from Cybercriminal Forums","type":"data"},{"authors":null,"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"70dd44fe4f8d7e8db5ef878916a54851","permalink":"https://www.jkk.name/data/paraphrasing-sample/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/data/paraphrasing-sample/","section":"data","summary":"Paraphrases collected while conducting experiments on factors influencing crowd performance.","tags":["misc"],"title":"Crowdsourced Paraphrases","type":"data"},{"authors":["Youxuan Jiang","Jonathan K. Kummerfeld","Walter S. Lasecki"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"3c562415de2a33ae0aeaa962f7be986a","permalink":"https://www.jkk.name/publication/acl17paraphrase/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/acl17paraphrase/","section":"publication","summary":"Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.","tags":null,"title":"Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection","type":"publication"},{"authors":["Rebecca S. Portnoff","Sadia Afroz","Greg Durrett","Jonathan K. Kummerfeld","Taylor Berg-Kirkpatrick","Damon McCoy","Kirill Levchenko","Vern Paxson"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"1afed92c7e60c8feab549d7518c89baf","permalink":"https://www.jkk.name/publication/www17forums/","publishdate":"2017-04-01T00:00:00Z","relpermalink":"/publication/www17forums/","section":"publication","summary":"Underground forums are widely used by criminals to buy and sell a host of stolen items, datasets, resources, and criminal services.  These forums contain important resources for understanding cybercrime.  However, the number of forums, their size, and the domain expertise required to understand the markets makes manual exploration of these forums unscalable. In this work, we propose an automated, top-down approach for analyzing underground forums.  Our approach uses natural language processing and machine learning to automatically generate high-level information about underground forums, first identifying posts related to transactions, and then extracting products and prices. We also demonstrate, via a pair of case studies, how an analyst can use these automated approaches to investigate other categories of products and transactions. We use eight distinct forums to assess our tools: Antichat, Blackhat World, Carders, Darkode, Hack Forums, Hell, L33tCrew and Nulled. Our automated approach is fast and accurate, achieving over 80% accuracy in detecting post category, product, and prices.","tags":null,"title":"Tools for Automated Analysis of Cybercriminal Markets","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"5184474e63f0cfe63bc054d5bb1ab334","permalink":"https://www.jkk.name/software/1ec-parsing/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/software/1ec-parsing/","section":"software","summary":"A range of tools related to one-endpoint crossing graphs - parsing, format conversion, and evaluation.","tags":["syntax"],"title":"One-Endpoint Crossing Graph Parser","type":"software"},{"authors":["Jonathan K. Kummerfeld","Dan Klein"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3e7ac5a07450493ea8d763a9440f33fa","permalink":"https://www.jkk.name/publication/tacl17parsing/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/tacl17parsing/","section":"publication","summary":"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.  We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.","tags":null,"title":"Parsing with Traces: An O($n^4$) Algorithm and a Structural Representation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"d5d94b05ee946afb5d69b35c74d20777","permalink":"https://www.jkk.name/data/shp-ptb/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/data/shp-ptb/","section":"data","summary":"Code to convert the standard Penn Treebank into a version where each word is assigned a spine of non-terminals, and arcs to indicate attachments from one spine to another.","tags":["syntax"],"title":"Spine and Arc version of the Penn Treebank","type":"data"},{"authors":["Jonathan K. Kummerfeld"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"b0479196f2b46e660e83a32f161c0e12","permalink":"https://www.jkk.name/publication/thesis16parsing/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/thesis16parsing/","section":"publication","summary":"Representation of syntactic structure is a core area of research in Computational Linguistics, disambiguating distinctions in meaning that are crucial for correct interpretation of language. Development of algorithms and statistical models over the past three decades has led to systems that are accurate enough to be deployed in industry, playing a key role in products such as Google Search and Apple Siri. However, syntactic parsers today are usually constrained to tree representations of language, and performance is interpreted through a single metric that conveys no linguistic information regarding remaining errors.\n\nIn this dissertation, we present new algorithms for error analysis and parsing. The heart of our approach to error analysis is the use of structural transformations to identify more meaningful classes of errors, and to enable comparisons across formalisms. For parsing, we combine a novel dynamic program with careful choices in syntactic representation to create an efficient parser that produces graph structured output. Together, these developments allowed us to evaluate the outstanding challenges in parsing and to address a key weakness in current work.\n\nFirst, we present a search algorithm that, given two structures, finds a sequence of modifications leading from one structure to the other. We applied this algorithm to syntactic error analysis, where one structure is the output of a parser, the other is the correct parse, and each modification corresponds to fixing one error. We constructed a tool based on the algorithm and analyzed variations in behavior between parsers, types of text, and languages. Our observations shine light on several assumptions about syntactic errors, showing some to be true and others to be false. For example, prepositional phrase attachment errors are indeed a major issue, while coordination scope errors do not hurt performance as much as expected.\n\nNext, we describe an algorithm that builds a parse in one syntactic representation to match a parse in another representation. Specifically, we build phrase structure parses from Combinatory Categorial Grammar derivations. Our approach follows the philosophy of CCG, defining specific phrase structures for each lexical category and generic rules for combinatory steps. The new parse is built by following the CCG derivation bottom-up, gradually building the corresponding phrase structure parse. This produced significantly more accurate parses than past work, and enabled us to compare performance of several parsers across formalisms.\n\nFinally, we address a weakness we observed in phrase structure parsers: the exclusion of syntactic trace structures for computational convenience. We present an efficient dynamic programming algorithm that constructs the graph structure that has the highest score under an edge-factored scoring function. We define a parse representation compatible with the algorithm, and show how certain linguistic distinctions dramatically impact coverage. We also show various ways to modify the algorithm to improve performance by exploiting properties of observed linguistic structure. This approach to syntactic parsing is the first to cover virtually all structure encoded in the Penn Treebank.","tags":null,"title":"Algorithms for Identifying Syntactic Errors and Parsing with Graph Structured Output","type":"publication"},{"authors":["Jonathan K. Kummerfeld","Taylor Berg-Kirkpatrick","Dan Klein"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"d7ccf2ad270feb23d5b008b903581eba","permalink":"https://www.jkk.name/publication/emnlp15learn/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/publication/emnlp15learn/","section":"publication","summary":"Despite the convexity of structured max-margin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal  optimization methods are often more robust and progress faster than dual methods. This advantage  is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.","tags":null,"title":"An Empirical Analysis of Optimization for Max-Margin NLP","type":"publication"},{"authors":null,"categories":null,"content":"","date":1380585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1380585600,"objectID":"6e7e5361f3abf9073b6b6df16f3eaefa","permalink":"https://www.jkk.name/software/coreference-analysis/","publishdate":"2013-10-01T00:00:00Z","relpermalink":"/software/coreference-analysis/","section":"software","summary":"A tool for classifying errors in coreference resolution.","tags":["coreference"],"title":"Coreference Error Analysis","type":"software"},{"authors":["Jonathan K. Kummerfeld","Dan Klein"],"categories":null,"content":"","date":1380585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1380585600,"objectID":"f147f3aaaaf0ccf643b6df12507cbb33","permalink":"https://www.jkk.name/publication/emnlp13analysis/","publishdate":"2013-10-01T00:00:00Z","relpermalink":"/publication/emnlp13analysis/","section":"publication","summary":"Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.","tags":null,"title":"Error-Driven Analysis of Challenges in Coreference Resolution","type":"publication"},{"authors":["Jonathan K. Kummerfeld","Daniel Tse","James R. Curran","Dan Klein"],"categories":null,"content":"","date":1375315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375315200,"objectID":"7580470ff414597037f5b0a1850321cd","permalink":"https://www.jkk.name/publication/acl13analysis/","publishdate":"2013-08-01T00:00:00Z","relpermalink":"/publication/acl13analysis/","section":"publication","summary":"Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.","tags":null,"title":"An Empirical Examination of Challenges in Chinese Parsing","type":"publication"},{"authors":["Vanessa A. Moss","Naomi M. McClure-Griffiths","Tara Murphy","D. J. Pisano","Jonathan K. Kummerfeld","James R. Curran"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"84f4d26c722c0d8d055dda9e9361d467","permalink":"https://www.jkk.name/publication/astro13clouds/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/astro13clouds/","section":"publication","summary":"We present a catalogue of high-velocity clouds (HVCs) from the Galactic All Sky Survey (GASS) of southern-sky neutral hydrogen, which has 57 mK sensitivity and 1 km/s velocity resolution and was obtained with the Parkes Telescope. Our catalogue has been derived from the stray-radiation corrected second release of GASS. We describe the data and our method of identifying HVCs and analyse the overall properties of the GASS population. We catalogue a total of 1693 HVCs at declinations ","tags":null,"title":"High-velocity Clouds in the Galactic All Sky Survey. I. Catalog","type":"publication"},{"authors":null,"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341100800,"objectID":"2107f17aa8b3b0c34b9aaa4cba222c73","permalink":"https://www.jkk.name/software/ccg2pst/","publishdate":"2012-07-01T00:00:00Z","relpermalink":"/software/ccg2pst/","section":"software","summary":"A tool for converting CCG derivations into PTB-style phrase structure trees.","tags":["syntax"],"title":"CCG to PST","type":"software"},{"authors":null,"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341100800,"objectID":"db318df8d62cdc5f58e636c6e0c7238f","permalink":"https://www.jkk.name/software/parsing-analysis/","publishdate":"2012-07-01T00:00:00Z","relpermalink":"/software/parsing-analysis/","section":"software","summary":"A tool for classifying mistakes in the output of parsers.","tags":["syntax"],"title":"Parse Error Analysis","type":"software"},{"authors":["Jonathan K. Kummerfeld","David Hall","James R. Curran","Dan Klein"],"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341100800,"objectID":"f0893dc2efb2bc1cf42cdbdb144f5a96","permalink":"https://www.jkk.name/publication/emnlp12analysis/","publishdate":"2012-07-01T00:00:00Z","relpermalink":"/publication/emnlp12analysis/","section":"publication","summary":"Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors.  We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.","tags":null,"title":"Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output","type":"publication"},{"authors":["Jonathan K. Kummerfeld","Dan Klein","James R. Curran"],"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341100800,"objectID":"2b29e16c3be2d6120cdf59ef4a76114a","permalink":"https://www.jkk.name/publication/acl12conversion/","publishdate":"2012-07-01T00:00:00Z","relpermalink":"/publication/acl12conversion/","section":"publication","summary":"We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.","tags":null,"title":"Robust Conversion of CCG Derivations to Phrase Structure Trees","type":"publication"},{"authors":["Jonathan K. Kummerfeld","Mohit Bansal","David Burkett","Dan Klein"],"categories":null,"content":"","date":1306886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1306886400,"objectID":"121f8c81eb8d0dc5b1deffa1056dd0e9","permalink":"https://www.jkk.name/publication/conll11coreference/","publishdate":"2011-06-01T00:00:00Z","relpermalink":"/publication/conll11coreference/","section":"publication","summary":"Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.","tags":null,"title":"Mention Detection: Heuristics for the OntoNotes annotations","type":"publication"},{"authors":["Raphael Candelier","Asaph Widmer-Cooper","Jonathan K. Kummerfeld","Olivier Dauchot","Giulio Biroli","Peter Harrowell","David R. Reichman"],"categories":null,"content":"","date":1283299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1283299200,"objectID":"2694f5617e87d10c765e2b2be2fc640a","permalink":"https://www.jkk.name/publication/prl10chemistry/","publishdate":"2010-09-01T00:00:00Z","relpermalink":"/publication/prl10chemistry/","section":"publication","summary":"We identify the pattern of microscopic dynamical relaxation for a two-dimensional glass-forming liquid. On short time scales, bursts of irreversible particle motion, called cage jumps, aggregate into clusters. On larger time scales, clusters aggregate both spatially and temporally into avalanches. This propagation of mobility takes place along the soft regions of the systems, which have been identified by computing isoconfigurational Debye-Waller maps. Our results characterize the way in which dynamical heterogeneity evolves in moderately supercooled liquids and reveal that it is astonishingly similar to the one found for dense glassy granular media.","tags":null,"title":"Spatiotemporal Hierarchy of Relaxation Events, Dynamical Heterogeneities, and Structural Reorganization in a Supercooled Liquid","type":"publication"},{"authors":["Matthew Honnibal","Jonathan K. Kummerfeld","James R. Curran"],"categories":null,"content":"","date":1280620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1280620800,"objectID":"b1a1ff367daa615794fab1b2bc4bc216","permalink":"https://www.jkk.name/publication/coling10morph/","publishdate":"2010-08-01T00:00:00Z","relpermalink":"/publication/coling10morph/","section":"publication","summary":"Because English is a low morphology language, current statistical parsers tend to ignore morphology and accept some level of redundancy. This paper investigates how costly such redundancy is for a lexicalised grammar such as CCG.\\n\\nWe use morphological analysis to split verb inflectional suffixes into separate tokens, so that they can receive their own lexical categories. We find that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can benefit from morphological analysis, so long as the analysis is correct.","tags":null,"title":"Morphological Analysis Can Improve a CCG Parser for English","type":"publication"},{"authors":null,"categories":null,"content":"","date":1277942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1277942400,"objectID":"d9efff416f83cae79cdd8a9b5258c7c9","permalink":"https://www.jkk.name/data/ccg-model/","publishdate":"2010-07-01T00:00:00Z","relpermalink":"/data/ccg-model/","section":"data","summary":"A model for the C\u0026C supertagger that gives the same results with smaller beam sizes, enabling faster parsing.","tags":["syntax"],"title":"Adaptive CCG Supertagging Model","type":"data"},{"authors":["Jonathan K. Kummerfeld","Jessika Roesner","Tim Dawborn","James Haggerty","James R. Curran","Stephen Clark"],"categories":null,"content":"","date":1277942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1277942400,"objectID":"2103704b8e5b1e8ff6ec9ac853061429","permalink":"https://www.jkk.name/publication/acl10adapt/","publishdate":"2010-07-01T00:00:00Z","relpermalink":"/publication/acl10adapt/","section":"publication","summary":"We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.","tags":null,"title":"Faster Parsing by Supertagger Adaptation","type":"publication"},{"authors":["Jonathan K. Kummerfeld","Jessika Roesner","James R. Curran"],"categories":null,"content":"","date":1259625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1259625600,"objectID":"d4bf0d66a9171cb6946e041b045a66c8","permalink":"https://www.jkk.name/publication/alta09tagging/","publishdate":"2009-12-01T00:00:00Z","relpermalink":"/publication/alta09tagging/","section":"publication","summary":"Parsers are often the bottleneck for data acquisition, processing text too slowly to be widely applied. One way to improve the efficiency of parsers is to construct more confident statistical models. More training data would enable the use of more sophisticated features and also provide more evidence for current features, but gold standard annotated data is limited and expensive to produce.\\n\\nWe demonstrate faster methods for training a supertagger using hundreds of millions of automatically annotated words, constructing statistical models that further constrain the number of derivations the parser must consider. By introducing new features and using an automatically annotated corpus we are able to double parsing speed on Wikipedia and the Wall Street Journal, and gain accuracy slightly when parsing Section 00 of the Wall Street Journal.","tags":null,"title":"Faster parsing and supertagging model estimation","type":"publication"},{"authors":["Jonathan K. Kummerfeld"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"09e320ea1992d9429094556af5746520","permalink":"https://www.jkk.name/publication/thesis09adapt/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/thesis09adapt/","section":"publication","summary":"Statistical parsers are crucial for tackling the grand challenges of Natural Language Processing. The most effective approaches to these tasks are data driven, but parsers are too slow to be effectively used on large data sets. State-of-the-art parsers generally cannot process more than one sentence a second, and the fastest cannot process more than fifty sentences a second. The situation is even worse when they are applied outside of the domain of their training data. The fastest systems have two components, a parser, which has time complexity O(n3) and a supertagger, which has linear time complexity. By shifting work from the parser to the supertagger we dramatically improve speed.\\n\\nThis work demonstrates several major novel ideas that improve parsing efficiency. The core idea is that the tags chosen by the parser are gold standard data for its supertagger. This leads to the second surprising conceptual development, that decreasing tagging accuracy can improve parsing performance. To demonstrate these ideas required extensive development of the C\u0026C supertagger, including imple- mentation of more efficient estimation algorithms and parallelisation of the training process. This was particularly challenging as the C\u0026C supertagger is a state-of-the-art high performance system designed with a focus on speed rather than flexibility.\\n\\nI was able to significantly improve performance on the standard evaluation corpus by using the parser to generate extremely large new resources for supertagger training. I have also shown that these methods provide significant benefits on another domain, Wikipedia text, without the cost of generating human annotated data sets. These parsing performance gains occur while supertagging accuracy decreases.\\n\\nDespite extensive use of supertaggers to improve parsing efficiency there has been no comprehensive study of the interaction between a supertagger and a parser. I present the first systematic exploration of the relationship, show the potential benefits of understanding it, and demonstrate a novel algorithm for optimising the parameters that define it.\\n\\nI have constructed models that process newspaper text 86% faster than previously, and Wikipedia text 30% faster, without any loss in accuracy and without the aid of extra gold standard resources in either domain. This work will lead directly to improvements in a range of Natural Language Processing tasks by enabling the use of far more parsed data.","tags":null,"title":"Adaptive Supertagging for Faster Parsing","type":"publication"},{"authors":["Stephen Clark","Ann Copestake","James R. Curran","Yue Zhang","Aurelie Herbelot","James Haggerty","Byung-Gyu Ahn","Curt Van Wyk","Jessika Roesner","Jonathan K. Kummerfeld","Tim Dawborn"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"e0518b2dba68382b4edb9fb8c4808dbf","permalink":"https://www.jkk.name/publication/report09jhu/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/report09jhu/","section":"publication","summary":"Scalable syntactic processing will underpin the sophisticated language technology needed for next generation information access. Companies are already using nlp tools to create web-scale question answering and 'semantic search' engines. Massive amounts of parsed web data will also allow the automatic creation of semantic knowledge resources on an unprecedented scale. The web is a challenging arena for syntactic parsing, because of its scale and variety of styles, genres, and domains.\\n\\nThe goals of our workshop were to scale and adapt an existing wide-coverage parser to Wikipedia text; improve the efficiency of the parser through various methods of chart pruning; use self-training to improve the efficiency and accuracy of the parser; use the parsed wiki data for an innovative form of bootstrapping to make the parser both more efficient and more accurate; and finally use the parsed web data for improved disambiguation of coordination structures, using a variety of syntactic and semantic knowledge sources.\\n\\nThe focus of the research was the C\u0026C parser (Clark and Curran, 2007c), a state-of-the-art statistical parser based on Combinatory Categorial Grammar (ccg). The parser has been evaluated on a number of standard test sets achieving state-of-the-art accuracies. It has also recently been adapted successfully to the biomedical domain (Rimell and Clark, 2009). The parser is surprisingly efficient, given its detailed output, processing tens of sentences per second. For web-scale text processing, we aimed to make the parser an order of magnitude faster still. The C\u0026C parser is one of only very few parsers currently available which has the potential to produce detailed, accurate analyses at the scale we were considering.","tags":null,"title":"Large-Scale Syntactic Processing: Parsing the Web","type":"publication"},{"authors":["Jonathan K. Kummerfeld","James R. Curran"],"categories":null,"content":"","date":1228089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1228089600,"objectID":"dccc454322e17f6a7db47f86188147b9","permalink":"https://www.jkk.name/publication/alta08vpc/","publishdate":"2008-12-01T00:00:00Z","relpermalink":"/publication/alta08vpc/","section":"publication","summary":"Manually maintaining comprehensive databases of multi-word expressions, for example Verb-Particle Constructions (VPCs), is infeasible. We describe a new classifier for potential VPCs, which uses information in the Google Web1T corpus to perform a simple linguistic constituency test. Specifically, we consider the fronting test, comparing the frequencies of the two possible orderings of the given verb and particle. Using only a small set of queries for each verb-particle pair, the system was able to achieve an F-score of 78.4% in our evaluation while processing thousands of queries a second.","tags":null,"title":"Classification of Verb Particle Constructions with the Google Web1T Corpus","type":"publication"},{"authors":["Jonathan K. Kummerfeld","Toby S Hudson","Peter Harrowell"],"categories":null,"content":"","date":1217548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1217548800,"objectID":"98cc6a20cd47be98468117f98ccfd201","permalink":"https://www.jkk.name/publication/chem08packing/","publishdate":"2008-08-01T00:00:00Z","relpermalink":"/publication/chem08packing/","section":"publication","summary":"This paper considers the homogeneous packing of binary hard spheres in an equimolar stoichiometry, and postulates the densest packing at each sphere size ratio. Monte Carlo simulated annealing optimizations are seeded with all known atomic inorganic crystal structures, and the search is performed within the degrees of freedom associated with each homogeneous AB structure type. Structures isopointal to the FeB structure type are found to have the highest packing fraction at all sphere size ratios. The optimized structures match or improve on the best previously demonstrated packings of this type, and show that compound structures can pack more densely than segregated close-packed structures at all radius ratios less than 0.62.","tags":null,"title":"The densest packing of AB binary hard-sphere homogeneous compounds across all size ratios","type":"publication"}]